{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d1f430d-b4c3-4319-b010-9e2c8e144a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f48631a-d20d-41eb-ae40-e60689e8a78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://api.stackexchange.com/2.3/questions\"\n",
    "params = {\n",
    "    \"order\": \"desc\",\n",
    "    \"sort\": \"creation\",\n",
    "    \"tagged\": \"machine_learning\",\n",
    "    \"site\": \"stackoverflow\",\n",
    "    \"filter\": \"withbody\",\n",
    "    \"pagesize\": 100,\n",
    "    \"page\": 25}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf594925-634e-482b-8828-6b812f889ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'items': [{'tags': ['pandas', 'tensorflow', 'machine-learning', 'keras'],\n",
       "   'owner': {'account_id': 335028,\n",
       "    'reputation': 1035,\n",
       "    'user_id': 661859,\n",
       "    'user_type': 'registered',\n",
       "    'accept_rate': 75,\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/50d463f2dfeefb3218b62edfdcb2c0c8?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'Nitul',\n",
       "    'link': 'https://stackoverflow.com/users/661859/nitul'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 43,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701328929,\n",
       "   'creation_date': 1701262288,\n",
       "   'last_edit_date': 1701328929,\n",
       "   'question_id': 77571304,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77571304/what-is-the-difference-between-sequential-and-concatenate-of-keras-while-creatin',\n",
       "   'title': 'What is the difference between Sequential and Concatenate of keras while creating input for model?',\n",
       "   'body': '<p>What is the difference between Sequential and Concatenate of keras while creating input for model?</p>\\n<p>I have seen two way of creating layers with Sequential where we just define the shape of the input, but the another way is creating  inputs with Concatenate.</p>\\n<p>What is the difference between these two approach ?</p>\\n<p>Below are approaches:</p>\\n<p><strong>Approach 1:</strong></p>\\n<pre><code>model = tf.keras.models.Sequential()\\nmodel.add(tf.keras.layers.Dense(units=1, input_shape=(2,), kernel_regularizer=\\'l1\\', activation=tf.sigmoid))\\n\\n\\nmodel.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=my_learning_rate),\\n                loss=tf.keras.losses.BinaryCrossentropy(),\\n                metrics=METRICS)\\n</code></pre>\\n<p>Approach 2:\\nLink : <a href=\"https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/binary_classification.ipynb?utm_source=mlcc&amp;utm_campaign=colab-external&amp;utm_medium=referral&amp;utm_content=binary_classification_tf2-colab&amp;hl=en#scrollTo=3tmmZIDw4JEC\" rel=\"nofollow noreferrer\">colab</a>.</p>\\n<pre><code>  my_inputs = {\\n    # Features used to train the model on.\\n    \\'median_income\\': tf.keras.Input(shape=(1,)),\\n    \\'total_rooms\\': tf.keras.Input(shape=(1,))\\n  }\\n# Use a Concatenate layer to concatenate the input layers into a single tensor.\\n  # as input for the Dense layer. Ex: [input_1[0][0], input_2[0][0]]\\n  concatenated_inputs = tf.keras.layers.Concatenate()(my_inputs.values())\\n  dense = layers.Dense(units=1, name=\\'dense_layer\\', activation=tf.sigmoid)\\n  dense_output = dense(concatenated_inputs)\\n  &quot;&quot;&quot;Create and compile a simple classification model.&quot;&quot;&quot;\\n  my_outputs = {\\n    \\'dense\\': dense_output,\\n  }\\n  model = tf.keras.Model(inputs=my_inputs, outputs=my_outputs)\\n\\n  # Call the compile method to construct the layers into a model that\\n  # TensorFlow can execute.  Notice that we\\'re using a different loss\\n  # function for classification than for regression.\\n  model.compile(optimizer=tf.keras.optimizers.experimental.RMSprop(learning_rate=my_learning_rate),\\n                loss=tf.keras.losses.BinaryCrossentropy(),\\n                metrics=METRICS)\\n</code></pre>\\n'},\n",
       "  {'tags': ['python',\n",
       "    'pandas',\n",
       "    'machine-learning',\n",
       "    'scikit-learn',\n",
       "    'pipeline'],\n",
       "   'owner': {'account_id': 30005114,\n",
       "    'reputation': 1,\n",
       "    'user_id': 22994216,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/ACg8ocKoKQFsyO7cGhkfFX-Krnxps2QSaAevGDiYmR59uLjbeQ=k-s256',\n",
       "    'display_name': 'Aditya Shandilya',\n",
       "    'link': 'https://stackoverflow.com/users/22994216/aditya-shandilya'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 76,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701357398,\n",
       "   'creation_date': 1701259008,\n",
       "   'last_edit_date': 1701357398,\n",
       "   'question_id': 77570948,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77570948/including-multiple-dataset-transformers-in-custom-transformer',\n",
       "   'title': 'Including multiple dataset transformers in custom transformer',\n",
       "   'body': \"<p>Here is my custom transformer, meant to transform the subject dataframe of encoding and scaling:</p>\\n<pre><code>class DfGrooming(BaseEstimator, TransformerMixin):\\n    def __init__(self):\\n        self.encodable_columns = ['Education','EmploymentType','MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner']\\n        self.scalable_columns = ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'InterestRate', 'LoanTerm']\\n        self.encoder = LabelEncoder()\\n        self.scaler = MinMaxScaler(feature_range=(0,5))\\n        self.X_encoded = pd.DataFrame()\\n        self.X_scaled = pd.DataFrame()\\n    \\ndef fit(self, X, y=None):\\n    self.encoder.fit(X[self.encodable_columns])\\n    self.scaler.fit(X[self.scalable_columns])\\n    return self\\n\\ndef transform(self, X, y=None):\\n    self.X_encoded = self.encoder.transform(X[self.encodable_columns])\\n    print(self.X_encoded.shape)\\n    X.drop(columns=self.encodable_columns, axis=1, inplace=True)\\n    X = pd.concat([X, self.X_encoded], axis=1)\\n    print(X.shape)\\n    self.X_scaled = X.filter(self.scalable_columns, axis=1)\\n    self.X_scaled = pd.DataFrame(scaler.transform(self.X_scaled))\\n    self.X_scaled.columns = self.scalable_columns\\n    X[self.scalable_columns] = self.X_scaled[self.scalable_columns]\\n    X.drop(['LoanID'], axis=1, inplace=True)\\n    print(X.shape)\\n    \\n    return X\\n</code></pre>\\n<p>But after running the pipeline:</p>\\n<pre><code>pipeline = Pipeline([('preparer', DfGrooming())])\\nt = pipeline.fit_transform(train_df)\\nt.head()\\n</code></pre>\\n<p>I get the following error:</p>\\n<pre><code>ValueError: bad input shape (178742, 7)\\n</code></pre>\\n<p>I would like to know what is actually happening and if I am missing anything in this implementation of transformers. Also please suggest better ways to implement this procedure. Thank you</p>\\n<p>I tried to include 2 transformers in one custom transformer.\\nI was expecting to combine 2 steps (and possibly 4 - removing and adding the encoded and scaled columns into the main dataframe)</p>\\n<p>I have a working algorithm where I semi-automatically transform the Validation and Test sets with functions, but wanted to try pipelines</p>\\n\"},\n",
       "  {'tags': ['machine-learning', 'large-language-model', 'llama-cpp-python'],\n",
       "   'owner': {'account_id': 13480794,\n",
       "    'reputation': 48,\n",
       "    'user_id': 9726166,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/2d6c683e736f5334cfbdaf54b99ee2a1?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'Muhammad Burhan',\n",
       "    'link': 'https://stackoverflow.com/users/9726166/muhammad-burhan'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 343,\n",
       "   'answer_count': 0,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1701258961,\n",
       "   'creation_date': 1701258961,\n",
       "   'question_id': 77570944,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77570944/llama-2-q4-quantized-models-response-time-on-different-cpus',\n",
       "   'title': 'Llama-2, Q4-Quantized model&#39;s response time on different CPUs',\n",
       "   'body': '<p>I am running quantized llama-2 model from <a href=\"https://huggingface.co/TheBloke/Llama-2-13B-GGUF\" rel=\"nofollow noreferrer\">here</a>. I am using 2 different machines.</p>\\n<ol>\\n<li>11th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz   2.80 GHz\\n16.0 GB (15.8 GB usable)</li>\\n</ol>\\n<p>Inference time on this machine is pretty good. I get my desired response in 3-4 minutes</p>\\n<ol start=\"2\">\\n<li>Intel(R) Xeon(R) CPU E5-2660 0 @ 2.20GHz   2.20 GHz  (2 processors)\\n224 GB</li>\\n</ol>\\n<p>Inference time on this machine is very long. It takes around half an hour to give unsatisfactory response. It even has an Nvidia 2080-Ti GPU as well. (But not using it to load model\\'s weights.</p>\\n<p>Why is this behavior? How does the CPU affects the performance?</p>\\n<p>I am using llama_cpp python package to load the model.</p>\\n'},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'linear-regression',\n",
       "    'gradient-descent'],\n",
       "   'owner': {'account_id': 29733952,\n",
       "    'reputation': 23,\n",
       "    'user_id': 22805791,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/ACg8ocLzlYSRK_HRHCbIy63u6ggHcVln4pMAyrs8hdWa15mS=k-s256',\n",
       "    'display_name': 'Sagnik Taraphdar',\n",
       "    'link': 'https://stackoverflow.com/users/22805791/sagnik-taraphdar'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 113,\n",
       "   'accepted_answer_id': 77570643,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701299067,\n",
       "   'creation_date': 1701248098,\n",
       "   'last_edit_date': 1701299067,\n",
       "   'question_id': 77569740,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77569740/why-is-my-implementation-of-linear-regression-not-working',\n",
       "   'title': 'Why is my implementation of linear regression not working?',\n",
       "   'body': '<p>I am trying to implement linear regression from scratch in python.</p>\\n<p>For reference, here are the mathematical formulae I have used: <a href=\"https://i.sstatic.net/7zzc1.png\" rel=\"nofollow noreferrer\">Equations</a></p>\\n<p>This is what I tried:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>class LinearRegression:\\n    \\n    def __init__(\\n    self, \\n    features: np.ndarray[np.float64],\\n    targets: np.ndarray[np.float64],\\n    ) -&gt; None:\\n        self.features = np.concatenate((np.ones((features.shape[0], 1)), features), axis=1)\\n        self.targets = targets\\n        self.params = np.random.randn(features.shape[1] + 1)\\n        self.num_samples = features.shape[0]\\n        self.num_feats = features.shape[1]\\n        self.costs = []\\n    \\n    def hypothesis(self) -&gt; np.ndarray[np.float64]:\\n        return np.dot(self.features, self.params)\\n    \\n    def cost_function(self) -&gt; np.float64:\\n        pred_vals = self.hypothesis()\\n        return (1 / (2 * self.num_samples)) * np.dot((pred_vals - self.targets).T, pred_vals - self.targets)\\n    \\n    def update(self, alpha: np.float64) -&gt; None:\\n        self.params = self.params - (alpha / self.num_samples) * (self.features.T @ (self.hypothesis() - self.targets))\\n    \\n    def gradientDescent(self, alpha: np.float64, threshold: np.float64, max_iter: int) -&gt; None:\\n        converged = False\\n        counter = 0\\n        while not converged:\\n            counter += 1\\n            curr_cost = self.cost_function()\\n            self.costs.append(curr_cost)\\n            self.update(alpha)\\n            new_cost = self.cost_function()\\n            if abs(new_cost - curr_cost) &lt; threshold:\\n                converged = True\\n            if counter &gt; max_iter:\\n                converged = True\\n</code></pre>\\n<p>I used the class like this:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>regr = LinearRegression(features=np.linspace(0, 1000, 200, dtype=np.float64).reshape((20, 10)), targets=np.linspace(0, 200, 20, dtype=np.float64))\\nregr.gradientDescent(0.1, 1e-3, 1e+3)\\nregr.cost_function()\\n</code></pre>\\n<p>However, I am getting the following errors:</p>\\n<pre><code>RuntimeWarning: overflow encountered in scalar power\\n  return (1 / (2 * self.num_samples)) * (la.norm(self.hypothesis() - self.targets) ** 4)\\n\\nRuntimeWarning: invalid value encountered in scalar subtract\\n  if abs(new_cost - curr_cost) &lt; threshold:\\n\\nRuntimeWarning: overflow encountered in matmul\\n  self.params = self.params - (alpha / self.num_samples) * (self.features.T @ (self.hypothesis() - self.targets))\\n</code></pre>\\n<p>What is going wrong exactly?</p>\\n'},\n",
       "  {'tags': ['python',\n",
       "    'tensorflow',\n",
       "    'machine-learning',\n",
       "    'keras',\n",
       "    'deep-learning'],\n",
       "   'owner': {'account_id': 18801513,\n",
       "    'reputation': 51,\n",
       "    'user_id': 13711721,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/f7ae41fe88116cb0b9f38aaea1fb4082?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'prynet',\n",
       "    'link': 'https://stackoverflow.com/users/13711721/prynet'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 158,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701427187,\n",
       "   'creation_date': 1701228576,\n",
       "   'last_edit_date': 1701288112,\n",
       "   'question_id': 77568420,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77568420/transfer-learning-using-vgg16-mnist-digits',\n",
       "   'title': 'Transfer learning using VGG16 MNIST Digits',\n",
       "   'body': \"<p>I am trying to perform transfer learning on the MNIST digits. I am interested in obtaining the logits and using it for gradient based attacks. But for some reason, the kernel keeps dying even though my computer is an apple m2max computer with GPU enabled. I also tried colab with GPU the same issue. The dataset is not too learng and I am reusing imagenet weights. How can I resolve this issue?</p>\\n<pre><code>class VGG16TransferLearning(tf.keras.Model):\\n  def __init__(self, base_model, models):\\n    super(VGG16TransferLearning, self).__init__()\\n    #base model\\n    self.base_model = base_model\\n\\n   # other layers\\n   self.flatten = tf.keras.layers.Flatten()\\n   self.dense1 = tf.keras.layers.Dense(512, activation='relu')\\n   self.dense2 = tf.keras.layers.Dense(512, activation='relu')\\n   self.dense3 = tf.keras.layers.Dense(10)\\n   self.layers_list = [self.flatten, self.dense1, self.dense2, self.dense3]\\n  \\n  #instantiate the base model with other layers\\n  self.model = models.Sequential(\\n    [self.base_model, *self.layers_list]\\n   )\\n\\ndef call(self, *args, **kwargs):\\n  activation_list = []\\n  out = args[0]\\n  \\n  for layer in self.model.layers:\\n    out = layer(out)\\n    activation_list.append(out)\\n  if kwargs['training']:\\n   return out\\n  else:\\n   prob = tf.nn.softmax(out)\\n   return out, prob\\n</code></pre>\\n<p>and here is the instantiation of the class above:</p>\\n<pre><code>base_model = VGG16(weights=&quot;imagenet&quot;, include_top=False, input_shape=x_train[0].shape)\\n</code></pre>\\n<p>base_model.trainable = False</p>\\n<p>my input shape is (75,75,3)</p>\\n<p>here is the compile and fit methods</p>\\n<pre><code>from tensorflow.keras import layers, models\\n</code></pre>\\n<p>model = VGG16TransferLearning(base_model, models)</p>\\n<pre><code>model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\\n          optimizer=tf.keras.optimizers.legacy.Adam(),\\n          metrics=['accuracy'])\\n\\nmodel.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\\n</code></pre>\\n<p>This is the error I get each time i call the fit method:</p>\\n<pre><code>Kernel Restarting\\nThe kernel for Untitled.ipynb appears to have died. It will restart automatically\\n</code></pre>\\n\"},\n",
       "  {'tags': ['machine-learning', 'nlp', 'spacy'],\n",
       "   'owner': {'account_id': 16176433,\n",
       "    'reputation': 679,\n",
       "    'user_id': 11678970,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/5479cda8c3b46e48a7facb7cc310a3de?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'Ankit',\n",
       "    'link': 'https://stackoverflow.com/users/11678970/ankit'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 631,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701220801,\n",
       "   'creation_date': 1701220801,\n",
       "   'question_id': 77568059,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77568059/gpe-using-spacy-identifies-locations-which-are-not-geographical-locations',\n",
       "   'title': 'GPE using SpaCy identifies locations which are not geographical locations',\n",
       "   'body': \"<p>I am using spacy library in my NLP code to identify the prominent geographical locations mentioned in the text.</p>\\n<p>But Spacy is identifying words which are not geographical locations along with the geographical locations such as: &quot;BA297&quot;, &quot;Legal&quot;, &quot;Nutrigain&quot;, &quot;Arham&quot;, &quot;Stephen&quot; and many more.</p>\\n<p>Is there any solution or better library?</p>\\n<p>Here is the code I am using to identify the geo locations:</p>\\n<pre><code># Load spaCy English model\\nnlp = spacy.load(&quot;en_core_web_sm&quot;)\\n\\n# Perform named entity recognition (NER) using spaCy\\nentities = []\\nfor word in df['words']:\\n    doc = nlp(word)\\n    for ent in doc.ents:\\n        entities.append((ent.text, ent.label_))\\n\\n# Extract GPE entities\\ngpe_entities = [entity[0] for entity in entities if entity[1] == 'GPE']\\n</code></pre>\\n\"},\n",
       "  {'tags': ['r', 'machine-learning', 'dplyr', 'nlp', 'tidytext'],\n",
       "   'owner': {'account_id': 25224785,\n",
       "    'reputation': 295,\n",
       "    'user_id': 19060810,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/d5ee1386d4db2d27d966ff05beece82d?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'nesta1990',\n",
       "    'link': 'https://stackoverflow.com/users/19060810/nesta1990'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 65,\n",
       "   'accepted_answer_id': 77564182,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701177379,\n",
       "   'creation_date': 1701170019,\n",
       "   'question_id': 77563423,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77563423/modifying-the-sentiment-of-certain-words-in-tidytext-get-sentiments',\n",
       "   'title': 'Modifying the sentiment of certain words in tidytext get_sentiments()',\n",
       "   'body': '<p>I am trying to modify the sentiment of a few specific words in my df to make them more suitable for my context, where they were used with a negative connotation but have been classified as having a positive sentiment. The words are &quot;talent&quot; and &quot;prefer&quot;.</p>\\n<p>Here is my code:</p>\\n<pre><code>#Loading packages\\nlibrary(dplyr)\\nlibrary(ggplot2)\\nrequire(readxl)\\nlibrary(tidytext)\\nrequire(writexl)\\n</code></pre>\\n<p>data example:</p>\\n<pre><code>dput(sentiment_words[1:20,c(7,8,9)])\\n</code></pre>\\n<p>data output:</p>\\n<pre><code>structure(list(word = c(&quot;talent&quot;, &quot;prefer&quot;, &quot;lies&quot;, &quot;hard&quot;, &quot;worsen&quot;, \\n&quot;addicts&quot;, &quot;obnoxious&quot;, &quot;unbearable&quot;, &quot;sickening&quot;, &quot;irritating&quot;, \\n&quot;weird&quot;, &quot;inconsiderate&quot;, &quot;weird&quot;, &quot;overwhelming&quot;, &quot;issue&quot;, &quot;complaints&quot;, \\n&quot;confined&quot;, &quot;love&quot;, &quot;confined&quot;, &quot;idiots&quot;), sentiment = c(&quot;positive&quot;, \\n&quot;positive&quot;, &quot;negative&quot;, &quot;negative&quot;, &quot;negative&quot;, &quot;negative&quot;, &quot;negative&quot;, \\n&quot;negative&quot;, &quot;negative&quot;, &quot;negative&quot;, &quot;negative&quot;, &quot;negative&quot;, &quot;negative&quot;, \\n&quot;negative&quot;, &quot;negative&quot;, &quot;negative&quot;, &quot;negative&quot;, &quot;positive&quot;, &quot;negative&quot;, \\n&quot;negative&quot;), count = c(79L, 3L, 53L, 316L, 2L, 2L, 3L, 2L, 2L, \\n7L, 24L, 2L, 24L, 2L, 198L, 21L, 4L, 52L, 4L, 19L)), class = c(&quot;grouped_df&quot;, \\n&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;), row.names = c(NA, -20L), groups = structure(list(\\n    word = c(&quot;addicts&quot;, &quot;complaints&quot;, &quot;confined&quot;, &quot;ftw&quot;, &quot;hard&quot;, \\n    &quot;idiots&quot;, &quot;inconsiderate&quot;, &quot;irritating&quot;, &quot;issue&quot;, &quot;lies&quot;, \\n    &quot;lost&quot;, &quot;love&quot;, &quot;obnoxious&quot;, &quot;overwhelming&quot;, &quot;sickening&quot;, \\n    &quot;unbearable&quot;, &quot;weird&quot;, &quot;worsen&quot;), .rows = structure(list(\\n        6L, 16L, c(17L, 19L), 2L, 4L, 20L, 12L, 10L, 15L, 3L, \\n        1L, 18L, 7L, 14L, 9L, 8L, c(11L, 13L), 5L), ptype = integer(0), class = c(&quot;vctrs_list_of&quot;, \\n    &quot;vctrs_vctr&quot;, &quot;list&quot;))), class = c(&quot;tbl_df&quot;, &quot;tbl&quot;, &quot;data.frame&quot;\\n), row.names = c(NA, -18L), .drop = TRUE))\\n</code></pre>\\n<pre><code> ###### Sentiment Analysis by Word ######\\n## Using &quot;TIDYTEXT&quot; sentiment dictionary\\nsentiment_words &lt;- df |&gt;\\n  tidytext::unnest_tokens(output=&quot;word&quot;, input=&quot;post&quot;) |&gt;\\n  dplyr::anti_join(tidytext::stop_words)|&gt;\\n  dplyr::inner_join(tidytext::get_sentiments(&quot;bing&quot;))\\n\\nsentiment_words %&gt;%\\n  count(word, sort = TRUE)\\n</code></pre>\\n<pre><code># Check the Most common positive and negative words\\nsentiment_words &lt;-\\nsentiment_words %&gt;% group_by(word) %&gt;% mutate(count = n())\\n \\nbing_word_counts &lt;- sentiment_words %&gt;%\\n  dplyr::inner_join(tidytext::get_sentiments(&quot;bing&quot;) %&gt;%\\n  count(word, sentiment, sort = TRUE))\\n</code></pre>\\n'},\n",
       "  {'tags': ['python', 'machine-learning', 'shap', 'xai'],\n",
       "   'owner': {'account_id': 4628889,\n",
       "    'reputation': 2361,\n",
       "    'user_id': 3751931,\n",
       "    'user_type': 'registered',\n",
       "    'accept_rate': 89,\n",
       "    'profile_image': 'https://i.sstatic.net/4ktXu.jpg?s=256',\n",
       "    'display_name': 'shamalaia',\n",
       "    'link': 'https://stackoverflow.com/users/3751931/shamalaia'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 74,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701176827,\n",
       "   'creation_date': 1701168962,\n",
       "   'last_edit_date': 1701176827,\n",
       "   'question_id': 77563309,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77563309/shap-force-plots-for-multiclass-problems',\n",
       "   'title': 'shap force plots for multiclass problems',\n",
       "   'body': '<p>I am trying to get to show the force plots for a given test example to all show in the same plot in the case of a multiclass classification problem.</p>\\n<p>My best attempt:</p>\\n<pre><code>explainer = shap.TreeExplainer(model)\\n\\nshap_test = explainer.shap_values(x_test)\\n\\nfig, axs = plt.subplots(6, 1, figsize=(10, 5 * 6))\\n\\nfor i in range(6):\\n    axs[i].set_title(f&quot;Waterfall Plot - Class {i}&quot;)\\n\\n    decision_plot = shap.force_plot(\\n        explainer.expected_value[i], shap_test[0][i], x_test.columns, show=False\\n    )\\nplt.show()\\n</code></pre>\\n<p>Unfortunately this still miserably fails:\\n<a href=\"https://i.sstatic.net/Jd3cM.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/Jd3cM.png\" alt=\"enter image description here\" /></a></p>\\n<p>Does anyone have a solution for this?</p>\\n'},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'linear-regression',\n",
       "    'gradient-descent',\n",
       "    'stochastic-gradient'],\n",
       "   'owner': {'account_id': 25208201,\n",
       "    'reputation': 25,\n",
       "    'user_id': 20390909,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/8dd769f74b44d0f4031cefcb3d7962bc?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'Merora',\n",
       "    'link': 'https://stackoverflow.com/users/20390909/merora'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 161,\n",
       "   'accepted_answer_id': 77564268,\n",
       "   'answer_count': 1,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1701178100,\n",
       "   'creation_date': 1701125206,\n",
       "   'last_edit_date': 1701172587,\n",
       "   'question_id': 77560377,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77560377/unexpected-output-with-stochastic-gradient-descent-algorithm-for-linear-regressi',\n",
       "   'title': 'unexpected output with stochastic gradient descent algorithm for linear regression',\n",
       "   'body': '<p>I had an unexpected output while implementing SGD algorithm for my ML homework.</p>\\n<p>This is part of my training data which normally has 320 rows:\\n<a href=\"https://i.sstatic.net/k8hOV.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/k8hOV.png\" alt=\"enter image description here\" /></a></p>\\n<p>my dataset: <a href=\"https://github.com/Jangrae/csv/blob/master/carseats.csv\" rel=\"nofollow noreferrer\">https://github.com/Jangrae/csv/blob/master/carseats.csv</a></p>\\n<p>I first did some data preprocessing:</p>\\n<pre><code>import pandas as pd\\nfrom sklearn.preprocessing import StandardScaler\\nimport numpy as np\\n\\ntrain_data = pd.read_csv(\\'carseats_train.csv\\')\\ntrain_data.replace({\\'Yes\\': 1, \\'No\\': 0}, inplace=True)\\nonehot_tr = pd.get_dummies(train_data[\\'ShelveLoc\\'], dtype=int, prefix_sep=\\'_\\', prefix=\\'ShelveLoc\\')\\ntrain_data = train_data.drop(\\'ShelveLoc\\', axis=1)\\ntrain_data = train_data.join(onehot_tr)\\n\\n\\ntrain_data_Y = train_data.iloc[:, 0]\\ntrain_data_X = train_data.drop(\\'Sales\\', axis=1)\\n\\n</code></pre>\\n<p>Then implemented the algorithm like this:</p>\\n<pre><code>learning_rate = 0.01\\nepoch_num = 50\\ninitial_w = 0.1\\nintercept = 0.1\\nw_matrix = np.ones((12, 1)) * initial_w\\n\\nfor e in range(epoch_num):\\n    for i in range(len(train_data_X)):\\n\\n        x_i = train_data_X.iloc[i].to_numpy()\\n        y_i = train_data_Y.iloc[i]\\n        \\n        y_estimated = np.dot(x_i, w_matrix) + intercept\\n        \\n        grad_w = x_i.reshape(-1, 1) * (y_i - y_estimated)\\n    \\n        grad_intercept = (y_i - y_estimated)\\n        \\n       \\n        w_matrix = w_matrix - 2 * learning_rate * grad_w\\n        intercept = intercept - 2 * learning_rate * grad_intercept\\n        \\n        \\n\\nprint(&quot;Final weights:\\\\n&quot;, w_matrix)\\nprint(&quot;Final intercept:&quot;, intercept)\\n</code></pre>\\n<p>But the output was</p>\\n<pre><code>Final weights:\\n [[nan]\\n [nan]\\n [nan]\\n [nan]\\n [nan]\\n [nan]\\n [nan]\\n [nan]\\n [nan]\\n [nan]\\n [nan]\\n [nan]]\\nFinal intercept: [nan]\\n</code></pre>\\n<p>I run it with various learning rates and I also tried convergence threshold but still got the same result.. I couldn\\'t find out why my code gives me nans..</p>\\n<p>Can anybody see the issue?</p>\\n'},\n",
       "  {'tags': ['r', 'machine-learning', 'neural-network'],\n",
       "   'owner': {'account_id': 23158829,\n",
       "    'reputation': 45,\n",
       "    'user_id': 17259742,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/df8896e6785e111921b36cadfe7d14ff?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'Lous',\n",
       "    'link': 'https://stackoverflow.com/users/17259742/lous'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 77,\n",
       "   'answer_count': 1,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1701164484,\n",
       "   'creation_date': 1701121963,\n",
       "   'last_edit_date': 1701128789,\n",
       "   'question_id': 77560144,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77560144/neuralnet-unexpected-prediction',\n",
       "   'title': 'Neuralnet Unexpected prediction',\n",
       "   'body': \"<p>I am trying to understand how the package neuralnet works.</p>\\n<p>I am using the mnist dataset, which contains 60.000 rows that correspond to different pictures and 785 columns that represent each pixel of the pictures (except for the first\\ncolumn that corresponds to the label of the picture).</p>\\n<pre><code>initial_data &lt;- read.csv(file = 'train.csv', header = TRUE)\\n</code></pre>\\n<p>The data looks like this:</p>\\n<pre><code>    label pixel1 pixel2 pixel3 pixel4 pixel5 pixel6 ...\\n1     5      0      0      3      0      1      0   ...\\n2     3      0      0      0      7      0      0   ...\\netc\\n</code></pre>\\n<p>First I remove the pixels that have a variance equal to 0. Since they are not informative to assess which is the number written in the picture.</p>\\n<pre><code>filtered_data &lt;- initial_data %&gt;%\\n  select_if(function(col) var(col) != 0)\\n\\n# Show dimension of the new filtered data\\ndim(filtered_data)\\n</code></pre>\\n<p>Then I standardize the data to ensure that each feature contributes equally\\nto the model, and the algorithm is not influenced by features with larger scales.</p>\\n<pre><code>filtered_data &lt;- as.data.frame(scale(filtered_data[-1]))\\n</code></pre>\\n<p>Now I make the data partition (80% training and 20% test).</p>\\n<pre><code>filtered_data$label &lt;- initial_data$label\\nfiltered_data &lt;- filtered_data %&gt;% select(label, everything())\\nindex &lt;- createDataPartition(filtered_data$label, p = 0.8, list = FALSE)\\n\\n# Create training and validation sets\\ntraining_data &lt;- filtered_data[index, ]\\nvalidation_data &lt;- filtered_data[-index, ]\\n\\n# Separate by predictors and labels\\ntraining_data_X &lt;- training_data[-1]\\ntraining_data_Y &lt;- training_data[1]\\nvalidation_data_X &lt;- validation_data[-1]\\nvalidation_data_Y &lt;- validation_data[1]\\n</code></pre>\\n<p>Now I generate a very simple neural network and make the predictions</p>\\n<pre><code>input_variables &lt;- paste(names(training_data_X), collapse = &quot; + &quot;)\\noutput_variable &lt;- names(training_data_Y)[1]  \\ncontent_formula &lt;- paste(output_variable, &quot;~&quot;, input_variables)\\n\\nsimple_nn_model &lt;- neuralnet(content_formula, data = training_data, hidden = 1, \\n                             act.fct = &quot;logistic&quot;, linear.output = FALSE)\\n\\npredictions_simple_model &lt;- predict(simple_nn_model, newdata = validation_data_X)\\n</code></pre>\\n<p><strong>Issue:</strong> I would expect that the object predictions_simple_model contains 10 columns (each column represents one number between 0 and 9) and their values should range from 0 to 1 (depending on the prediction made by the model). But, instead, I obtain a single column and all of their values are equal to 1.</p>\\n<pre><code>&gt; predictions_simple_model\\n           [,1]\\n137   1.0000000\\n171   1.0000000\\n213   1.0000000\\n225   1.0000000\\n236   1.0000000\\n420   1.0000000\\n576   1.0000000\\n615   1.0000000\\n899   1.0000000\\netc\\n</code></pre>\\n<p>How to solve this issue?</p>\\n\"},\n",
       "  {'tags': ['python', 'machine-learning', 'scikit-learn', 'cross-validation'],\n",
       "   'owner': {'account_id': 26230119,\n",
       "    'reputation': 9,\n",
       "    'user_id': 19906210,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/fa34a419333c7bc3460bfde853839d16?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'cigsesgi',\n",
       "    'link': 'https://stackoverflow.com/users/19906210/cigsesgi'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 100,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701992374,\n",
       "   'creation_date': 1701072538,\n",
       "   'last_edit_date': 1701085166,\n",
       "   'question_id': 77555347,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77555347/different-cross-validation-techniques-yielding-identical-evaluation-metrics',\n",
       "   'title': 'Different Cross-Validation Techniques Yielding Identical Evaluation Metrics',\n",
       "   'body': '<p>I implement three ML algorithms (K-Nearest Neighbor, Decision Trees, and Random Forest) and use four different cross-validation techniques (Hold-Out Method, Leave-One-Out Method, K-Fold Cross-Validation, Stratified K-Fold Cross-Validation) for each algorithm. The goal is to evaluate performance metrics and compare the techniques and algorithms. My code runs, but the evaluation metric values for different techniques are the same. Is it normal for these values to be identical, or might I be doing something wrong?</p>\\n<p>Here\\'s part of my code:</p>\\n<pre class=\"lang-py prettyprint-override\"><code># Initialize classifiers\\nknn = KNeighborsClassifier(n_neighbors=5, metric=\\'minkowski\\', p=2)\\ndtree = DecisionTreeClassifier(random_state=42)\\nrf = RandomForestClassifier(n_estimators=20, criterion=\\'entropy\\', random_state=0)\\n\\nclassifiers = {\\'KNN\\': knn, \\'Decision Tree\\': dtree, \\'Random Forest\\': rf}\\n\\n# Define cross-validation methods\\nloo = LeaveOneOut()\\nkf = KFold(10)\\nskf = StratifiedKFold(n_splits=5)\\n\\ncv_methods = {\\'Hold-Out Method\\': (X_train, X_test, y_train, y_test),\\n              \\'Leave-One-Out Method\\': loo,\\n              \\'K-Fold Cross-Validation\\': kf,\\n              \\'Stratified K-Fold Cross-Validation\\': skf}\\n\\n# Perform classification and evaluation for each classifier and cross-validation method\\nfor clf_name, clf in classifiers.items():\\n    print(f&quot;Classifier: {clf_name}&quot;)\\n    for cv_name, cv_method in cv_methods.items():\\n        if cv_name == \\'Hold-Out Method\\':\\n            X_train_cv, X_test_cv, y_train_cv, y_test_cv = cv_method\\n            clf.fit(X_train_cv, y_train_cv)\\n            y_pred = clf.predict(X_test_cv)\\n        else:\\n            scores = cross_val_score(clf, X, y, cv=cv_method, scoring=\\'accuracy\\')\\n            \\n\\n        # Calculate evaluation metrics\\n        accuracy = accuracy_score(y_test_cv, y_pred)\\n        precision = precision_score(y_test_cv, y_pred, average=\\'weighted\\')\\n        recall = recall_score(y_test_cv, y_pred, average=\\'weighted\\')\\n        f1 = f1_score(y_test_cv, y_pred, average=\\'weighted\\')\\n        confusion = confusion_matrix(y_test_cv, y_pred)\\n</code></pre>\\n<p>And here are the outputs, which are identical for each classifier and cross-validation method:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>Classifier: KNN\\nHold-Out Method Metrics for KNN:\\nAccuracy: 0.864620939\\nPrecision: 0.8661\\nRecall: 0.8646\\nF1 Score: 0.8652\\nConfusion Matrix:\\n[[326  41]\\n [ 34 153]]\\n\\nLeave-One-Out Method Metrics for KNN:\\nAccuracy: 0.864620939\\nPrecision: 0.8661\\nRecall: 0.8646\\nF1 Score: 0.8652\\nConfusion Matrix:\\n[[326  41]\\n [ 34 153]]\\n\\nK-Fold Cross-Validation Metrics for KNN:\\nAccuracy: 0.864620939\\nPrecision: 0.8661\\nRecall: 0.8646\\nF1 Score: 0.8652\\nConfusion Matrix:\\n[[326  41]\\n [ 34 153]]\\n\\nStratified K-Fold Cross-Validation Metrics for KNN:\\nAccuracy: 0.864620939\\nPrecision: 0.8661\\nRecall: 0.8646\\nF1 Score: 0.8652\\nConfusion Matrix:\\n[[326  41]\\n [ 34 153]]\\n\\nClassifier: Decision Tree\\nHold-Out Method Metrics for Decision Tree:\\nAccuracy: 0.980144404\\nPrecision: 0.9801\\nRecall: 0.9801\\nF1 Score: 0.9801\\nConfusion Matrix:\\n[[363   4]\\n [  7 180]]\\n\\nLeave-One-Out Method Metrics for Decision Tree:\\nAccuracy: 0.980144404\\nPrecision: 0.9801\\nRecall: 0.9801\\nF1 Score: 0.9801\\nConfusion Matrix:\\n[[363   4]\\n [  7 180]]\\n\\nK-Fold Cross-Validation Metrics for Decision Tree:\\nAccuracy: 0.980144404\\nPrecision: 0.9801\\nRecall: 0.9801\\nF1 Score: 0.9801\\nConfusion Matrix:\\n[[363   4]\\n [  7 180]]\\n\\nStratified K-Fold Cross-Validation Metrics for Decision Tree:\\nAccuracy: 0.980144404\\nPrecision: 0.9801\\nRecall: 0.9801\\nF1 Score: 0.9801\\nConfusion Matrix:\\n[[363   4]\\n [  7 180]]\\n\\nClassifier: Random Forest\\nHold-Out Method Metrics for Random Forest:\\nAccuracy: 0.981949458\\nPrecision: 0.9820\\nRecall: 0.9819\\nF1 Score: 0.9819\\nConfusion Matrix:\\n[[364   3]\\n [  7 180]]\\n\\nLeave-One-Out Method Metrics for Random Forest:\\nAccuracy: 0.981949458\\nPrecision: 0.9820\\nRecall: 0.9819\\nF1 Score: 0.9819\\nConfusion Matrix:\\n[[364   3]\\n [  7 180]]\\n\\nK-Fold Cross-Validation Metrics for Random Forest:\\nAccuracy: 0.981949458\\nPrecision: 0.9820\\nRecall: 0.9819\\nF1 Score: 0.9819\\nConfusion Matrix:\\n[[364   3]\\n [  7 180]]\\n\\nStratified K-Fold Cross-Validation Metrics for Random Forest:\\nAccuracy: 0.981949458\\nPrecision: 0.9820\\nRecall: 0.9819\\nF1 Score: 0.9819\\nConfusion Matrix:\\n[[364   3]\\n [  7 180]]\\n</code></pre>\\n<p>Why might this be happening?</p>\\n'},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'jupyter-notebook',\n",
       "    'google-colaboratory',\n",
       "    'train-test-split'],\n",
       "   'owner': {'account_id': 30001797,\n",
       "    'reputation': 11,\n",
       "    'user_id': 22991625,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/ACg8ocLm-kIpCA37dqHU4es90AFsTlKW_4bSLmqhfC7CI8Jpa1o=k-s256',\n",
       "    'display_name': 'user22991625',\n",
       "    'link': 'https://stackoverflow.com/users/22991625/user22991625'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 121,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701377150,\n",
       "   'creation_date': 1701072319,\n",
       "   'last_edit_date': 1701091422,\n",
       "   'question_id': 77555330,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77555330/can-we-use-train-test-split-in-one-single-colabor-jupyter-notebook-twice',\n",
       "   'title': 'Can we use &#39;train_test_split&#39; in one single colab(or jupyter notebook) twice?',\n",
       "   'body': '<p>I have to perform classification and regression using decision tree Machine Learning algorithm. Now I have already done the regression part of the code. If I proceed with classification task on this, i should do <em>train_test_split</em> on the preprocessed dataset. <em>In the code i have to define X and y variables, then do X_train, X_test, y_train and y_test part</em>. The same variables are repeating in both regression and classification. By taking the same variables from classification, will it consider the previous values from regression as they are given first or does it take the newly given values ?</p>\\n<p>I want the answer clearly whether we can use <em>train_test_split</em> function more than once in either a single colab or jupyter notebook.</p>\\n'},\n",
       "  {'tags': ['python',\n",
       "    'tensorflow',\n",
       "    'machine-learning',\n",
       "    'huggingface-transformers',\n",
       "    'bert-language-model'],\n",
       "   'owner': {'account_id': 17110334,\n",
       "    'reputation': 49,\n",
       "    'user_id': 12381322,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a-/AAuE7mCjA7YkxlslccgEL21_94wBjFeM9i0Mlven8KPp=k-s256',\n",
       "    'display_name': 'krishna kaushik',\n",
       "    'link': 'https://stackoverflow.com/users/12381322/krishna-kaushik'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 233,\n",
       "   'accepted_answer_id': 77555970,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701078960,\n",
       "   'creation_date': 1701068640,\n",
       "   'question_id': 77555030,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77555030/how-to-resolve-bert-hf-model-valueerror-too-many-values-to-unpack-expected-2',\n",
       "   'title': 'How to resolve BERT HF Model ValueError: too many values to unpack (expected 2)?',\n",
       "   'body': \"<p>I have a dummy dataset of two text columns and labels as below.</p>\\n<pre><code>import tensorflow as tf\\nfrom transformers import BertTokenizer, TFAutoModelForSequenceClassification\\nimport numpy as np\\nfrom datasets import Dataset, DatasetDict\\n\\n# Create a synthetic dataset with two text columns and a label column (0 or 1)\\ndata_size = 1000\\ntext_column1 = [&quot;This is sentence {}.&quot;.format(i) for i in range(data_size)]\\ntext_column2 = [&quot;Another sentence {} for tokenization.&quot;.format(i) for i in range(data_size)]\\nlabels = np.random.choice([0, 1], size=data_size)\\n</code></pre>\\n<p>I am using the HF bert model for classification.(TFAutoModelForSequenceClassification)</p>\\n<pre><code># Load BERT tokenizer and model\\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\\nmodel2 = TFAutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;)\\n</code></pre>\\n<p>When using the below code for preparing the dataset and model training, the execution is successful.</p>\\n<pre><code>def tokenize_dataset(df):\\n    # Keys of the returned dictionary will be added to the dataset as columns\\n    return tokenizer(df['text_column1'], df['text_column2'])\\n# Convert to a DataFrame\\nimport pandas as pd\\ndf = pd.DataFrame({'text_column1': text_column1, 'text_column2': text_column2, 'label': labels})\\ndf =  Dataset.from_pandas(df).map(tokenize_dataset) \\ntf_train = model2.prepare_tf_dataset(df, batch_size=4, shuffle=True, tokenizer=tokenizer)\\nmodel2.compile(optimizer=Adam(3e-5))  # No loss argument!\\nmodel2.fit(tf_train)\\n</code></pre>\\n<p>The above code works successfully.</p>\\n<p>However when I use padding, truncation and max_length in the tokenizer, i.e as below</p>\\n<pre><code>def tokenize_dataset(df):\\n    # Keys of the returned dictionary will be added to the dataset as columns\\n    return tokenizer(df['text_column1'], df['text_column2'], padding=True,truncation=True,max_length=30, return_tensors=&quot;tf&quot;)\\n# Convert to a DataFrame\\nimport pandas as pd\\ndf = pd.DataFrame({'text_column1': text_column1, 'text_column2': text_column2, 'label': labels})\\ndf =  Dataset.from_pandas(df).map(tokenize_dataset) \\ntf_train = model2.prepare_tf_dataset(df, batch_size=4, shuffle=True, tokenizer=tokenizer)\\nmodel2.compile(optimizer=Adam(3e-5))  # No loss argument!\\nmodel2.fit(tf_train)\\n</code></pre>\\n<p>This code gave the following error:</p>\\n<pre><code>ValueError: Exception encountered when calling layer 'bert' (type TFBertMainLayer).\\n        \\n   \\n\\n in user code:\\n    \\n        File &quot;/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py&quot;, line 1557, in run_call_with_unpacked_inputs  *\\n            return func(self, **unpacked_inputs)\\n        File &quot;/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_tf_bert.py&quot;, line 766, in call  *\\n            batch_size, seq_length = input_shape\\n    \\n        ValueError: too many values to unpack (expected 2)\\n</code></pre>\\n<p>I am not able to understand why that will happen. If it happens then, why is it so and how to resolve the error?</p>\\n\"},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'keras',\n",
       "    'scikit-learn',\n",
       "    'deep-learning'],\n",
       "   'owner': {'account_id': 9922956,\n",
       "    'reputation': 1084,\n",
       "    'user_id': 9213600,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/de6eab16b0d8821174f5bbb0dafc4bc0?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'Ali H. Kudeir',\n",
       "    'link': 'https://stackoverflow.com/users/9213600/ali-h-kudeir'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 137,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701047996,\n",
       "   'creation_date': 1701040173,\n",
       "   'last_edit_date': 1701047996,\n",
       "   'question_id': 77553816,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77553816/keras-rbf-radial-basis-functions-layer-for-image-classification-is-not-working',\n",
       "   'title': 'Keras RBF (radial basis functions) layer for image classification is not working',\n",
       "   'body': \"<p>I want to build an image classifier using an RBF layer, Keras, and the fashionMNIST dataset. The model compiles correctly, the only issue I have is that the model is not learning anything, basically classifies any image to some label it comes up with ending up with 10% accuracy all the time (please note that the fashionMNIST is a balanced dataset of 10 classes).</p>\\n<p>Here is the implementation</p>\\n<pre><code>from keras.layers import Layer\\nfrom keras import backend as K\\nfrom keras.utils import to_categorical\\nfrom keras.initializers import RandomUniform, Initializer, Constant\\nimport numpy as np\\nfrom keras.datasets import mnist\\nfrom keras.layers import Dense, Flatten\\nfrom keras.models import Sequential\\nfrom keras.losses import categorical_crossentropy\\nfrom keras.optimizers import Adam\\nimport matplotlib.pyplot as plt\\nfrom sklearn.metrics import mean_squared_error\\n\\nclass InitCentersRandom(Initializer):\\n    def __init__(self, X):\\n        self.X = X\\n\\n    def __call__(self, shape, dtype=None):\\n        assert shape[1] == self.X.shape[1]\\n        idx = np.random.randint(self.X.shape[0], size=shape[0])\\n        return self.X[idx, :]\\n\\nclass RBFLayer(Layer):\\n    def __init__(self, output_dim, initializer=None, betas=1.0, **kwargs):\\n        self.output_dim = output_dim\\n        self.init_betas = betas\\n        if not initializer:\\n            self.initializer = RandomUniform(0.0, 1.0)\\n        else:\\n            self.initializer = initializer\\n        super(RBFLayer, self).__init__(**kwargs)\\n\\n    def build(self, input_shape):\\n        self.centers = self.add_weight(name='centers',\\n                                      shape=(self.output_dim, input_shape[1]),\\n                                      initializer=self.initializer,\\n                                      trainable=True)\\n        self.betas = self.add_weight(name='betas',\\n                                    shape=(self.output_dim,),\\n                                    initializer=Constant(value=self.init_betas),\\n                                    trainable=True)\\n        super(RBFLayer, self).build(input_shape)\\n\\n    def call(self, x):\\n        C = K.expand_dims(self.centers)\\n        H = K.transpose(C - K.transpose(x))\\n        return K.exp(-self.betas * K.sum(H**2, axis=1))\\n\\n    def compute_output_shape(self, input_shape):\\n        return (input_shape[0], self.output_dim)\\n\\n    def get_config(self):\\n        config = {\\n            'output_dim': self.output_dim\\n        }\\n        base_config = super(RBFLayer, self).get_config()\\n        return dict(list(base_config.items()) + list(config.items()))\\n\\n# Load and preprocess the data\\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\\nX = x_train.astype('float32') / 255.0\\nX = X.reshape((len(X), -1))\\ny = to_categorical(y_train, num_classes=10)\\n\\n# Create the RBFLayer model\\nrbflayer = RBFLayer(20,\\n                initializer=InitCentersRandom(X),\\n                betas=2.0,\\n                input_shape=X.shape[1:])\\nmodel = Sequential()\\nmodel.add(rbflayer)\\nmodel.add(Dense(10, activation='softmax'))\\nmodel.summary()\\n\\n# Compile and train the model\\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\\nmodel.fit(X, y, batch_size=32, epochs=2, verbose=True)\\n</code></pre>\\n<p>And for testing the model I am using:</p>\\n<pre><code># Assuming you have a trained model 'model' and test data 'x_test' and 'y_test'\\n\\n# Preprocess the test data\\nx_test = x_test.astype('float32') / 255.0\\nx_test = x_test.reshape((len(x_test), -1))\\ny_test_categorical = to_categorical(y_test, num_classes=10)\\n\\n# Evaluate the model on the test data\\nloss = model.evaluate(x_test, y_test_categorical, verbose=0)\\n\\n# Make predictions on the test data\\ny_pred = model.predict(x_test)\\ny_pred_classes = np.argmax(y_pred, axis=1)\\n\\n# Calculate accuracy\\ncorrect_predictions = np.sum(y_pred_classes == y_test)\\ntotal_samples = len(y_test)\\naccuracy = correct_predictions / total_samples\\n\\nprint(f'Test Loss: {loss:.4f}')\\nprint(f'Test Accuracy: {accuracy * 100:.2f}%')\\n\\n</code></pre>\\n<p>I attempted different hyperparams, RBF kernals, initial weights, etc, but the model always ends up giving ~10% accuracy.</p>\\n\"},\n",
       "  {'tags': ['python', 'machine-learning', 'keras', 'lstm'],\n",
       "   'owner': {'account_id': 29997113,\n",
       "    'reputation': 11,\n",
       "    'user_id': 22988160,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/ACg8ocJQqOeBjMWsspU6UjdqzzkGDT16oXpdHRyf6AOd-TikVbU=k-s256',\n",
       "    'display_name': 'Swapnil kumar Sahu',\n",
       "    'link': 'https://stackoverflow.com/users/22988160/swapnil-kumar-sahu'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 305,\n",
       "   'answer_count': 1,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1701017552,\n",
       "   'creation_date': 1701013286,\n",
       "   'last_edit_date': 1701017552,\n",
       "   'question_id': 77552353,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77552353/module-keras-api-v2-keras-layers-has-no-attribute-lstmstatetuple',\n",
       "   'title': 'module &#39;keras.api._v2.keras.layers&#39; has no attribute &#39;LSTMStateTuple&#39;',\n",
       "   'body': \"<p>While running a LSTM model, I'm stuck at this code -</p>\\n<pre><code># Create cell state and hidden state variables to maintain the state of the LSTM\\nc, h = [], []\\ninitial_state = []\\n\\nfor li in range(n_layers):\\n    c.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\\n    h.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\\n    initial_state.append(tf.keras.layers.LSTMStateTuple(c[li], h[li]))\\n\\n# Do several tensor transformations, because the function dynamic_rnn requires the output to be of\\n# a specific format. Read more at: https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\\n\\nall_inputs = tf.concat([tf.expand_dims(t, 0) for t in train_inputs], axis=0)\\n\\n# Create LSTM layer\\nlstm_layer = tf.keras.layers.LSTM(num_nodes[-1], return_sequences=True, return_state=True, dropout=dropout)\\n\\n# Pass inputs and initial state to the LSTM layer\\nall_lstm_outputs, final_state, _ = lstm_layer(all_inputs, initial_state=initial_state)\\n\\nall_outputs = tf.keras.layers.Dense(1)(all_lstm_outputs)\\n\\nsplit_outputs = tf.split(all_outputs, num_unrollings, axis=0)```\\n</code></pre>\\n<p>I got this error:</p>\\n<pre><code>AttributeError: module 'keras.api._v2.keras.layers' has no attribute 'LSTMStateTuple'\\n</code></pre>\\n\"},\n",
       "  {'tags': ['python', 'docker', 'machine-learning', 'fastapi', 'doctr'],\n",
       "   'owner': {'account_id': 23156047,\n",
       "    'reputation': 21,\n",
       "    'user_id': 17257285,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/AATXAJzafUf3efnyg28FCfdnYySyG2YFGmuKdfusSmcO=k-s256',\n",
       "    'display_name': 'Ebrahim Soliman',\n",
       "    'link': 'https://stackoverflow.com/users/17257285/ebrahim-soliman'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 247,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1736256539,\n",
       "   'creation_date': 1701010014,\n",
       "   'last_edit_date': 1736256539,\n",
       "   'question_id': 77552150,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77552150/running-doctr-ml-model-in-docker-hangs-forever',\n",
       "   'title': 'Running doctr ml model in docker hangs forever',\n",
       "   'body': \"<p>I have a fastapi app that loads the doctr model inside the code it just takes an image path and convert it to text</p>\\n<p>here is the code</p>\\n<pre><code>from doctr.io import DocumentFile\\nfrom doctr.models import ocr_predictor\\n__version__ = &quot;0.1.1&quot;\\nmodel = ocr_predictor(pretrained=True)\\n\\n\\n\\ndef process_image(image_path):\\n    document = DocumentFile.from_images(image_path)\\n    result = model(document)\\n    json_response = result.export()\\n    return json_response\\n</code></pre>\\n<p>I just want to expose the model through an API.</p>\\n<p>main.py file where my api code set</p>\\n<pre><code>\\n\\nfrom app.model.model import __version__ as model_version\\nfrom app.model.model import process_image\\nfrom fastapi import FastAPI, HTTPException, UploadFile\\n\\napp = FastAPI()\\n\\n@app.get(&quot;/&quot;)\\ndef home():\\n    return {&quot;health_check&quot;: &quot;OK&quot;, &quot;model_version&quot;: 'model_version'}\\n</code></pre>\\n<p>When I run the main.py using this code</p>\\n<p><code>uvicorn app.main:app --reload</code></p>\\n<p>everything works</p>\\n<p>but when I tried to dockerize it it just hangs forever and here is my <code>Dockerfile</code></p>\\n<pre><code>FROM tiangolo/uvicorn-gunicorn-fastapi:python3.9\\n\\nRUN apt-get update\\n\\nRUN apt install -y libgl1-mesa-glx\\n\\nCOPY ./requirements.txt /app/requirements.txt\\n\\nRUN pip install --no-cache-dir --upgrade -r /app/requirements.txt\\n\\nCOPY ./app /app/app\\n</code></pre>\\n<p>NOTE: I have an m1 mac</p>\\n<p>I tried everything to make it works including moving the imports inside the function</p>\\n<pre><code>__version__ = &quot;0.1.1&quot;\\n\\ndef process_image(image_path):\\n    from doctr.io import DocumentFile\\n    from doctr.models import ocr_predictor\\n    model = ocr_predictor(pretrained=True)\\n    document = DocumentFile.from_images(image_path)\\n    result = model(document)\\n    json_response = result.export()\\n    return json_response\\n</code></pre>\\n<p>but nothing is working when ever the api reaches the imports it just hangs forever and that is only when using docker</p>\\n\"},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'scikit-learn',\n",
       "    'grid-search',\n",
       "    'lightgbm'],\n",
       "   'owner': {'account_id': 23905938,\n",
       "    'reputation': 1,\n",
       "    'user_id': 17903525,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/AATXAJw_rUEWEEcF2bC0U_Puf1wW2-hKGlDzAxJnDVf_=k-s256',\n",
       "    'display_name': 'Bourne813',\n",
       "    'link': 'https://stackoverflow.com/users/17903525/bourne813'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 107,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701011996,\n",
       "   'creation_date': 1701007351,\n",
       "   'question_id': 77551983,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77551983/how-to-use-a-custom-scorer-function-in-gridsearch',\n",
       "   'title': 'How to use a custom scorer function in gridsearch',\n",
       "   'body': '<p>I am using lightgbm as a estimator in gridsearch. I want to change the score funtion in GridSearchCV, and this function calculates the threshold below the target false positive rate and passes the X_test and y_test parameters. i\\'ve tried a custom scorer</p>\\n<pre><code>grid_parametaers = {\\n    \\'bagging_fraction\\':[0.8, 0.9],\\n    \\'bagging_freq\\': [2, 3],\\n    \\'min_data_in_leaf\\': [100,250]\\n}\\ngbm_model = lgb.LGBMClassifier(boosting_type = &quot;gbdt&quot;,\\n                              objective = &quot;binary&quot;,\\n                             metric = &quot;l2,auc,binary&quot;, \\n                             learning_rate = 0.1,\\n                             num_iterations = 5000,\\n                              )\\n\\ndef summary_of_threshold(estimator,X,y):\\n    fpr_target = 0.0006\\n    y_pred = estimator.predict(X_test)\\n    threshold, fpr = find_threshold(y_test.label, y_pred, fpr_target)\\n    print(y_pred[:10])\\n    print(&quot;#####################{}&quot;.format(fpr))\\n    return fpr\\n##X_test, y_test (global variables) are my test dataset. and the score funtion is to get the threshold at a target false positive rate\\n\\ngsearch = GridSearchCV(gbm_model, param_grid=grid_parametaers, scoring=summary_of_threshold, cv=3, n_jobs = 1)\\nearly_stopping_rounds = 300\\ngsearch.fit(\\n    X=X_train, \\n    y=y_train.label,\\n    eval_set=[(X_val, y_val.label)],\\n    eval_metric=[&quot;l2&quot;,&quot;auc&quot;,&quot;binary&quot;],\\n    callbacks=[early_stopping(early_stopping_rounds)],\\n    categorical_feature=categorical_features,\\n    verbose=10\\n)\\n</code></pre>\\n<p>The result is not as I imagined, the <code>y_pred</code> values predicted by the estimator are all 0. I print the first ten values\\n<a href=\"https://i.sstatic.net/k3UlD.png\" rel=\"nofollow noreferrer\">enter image description here</a>\\nI want to know why the values predicted by the estimator are all 0</p>\\n<p>Before this I tried make_scorer factory,</p>\\n<pre><code>customized_scoer = make_scorer(summary_of_threshold, greater_is_better=False)\\ngsearch = GridSearchCV(gbm_model, param_grid=grid_parametaers, scoring=customized_scoer, cv=3, n_jobs = 1)\\nearly_stopping_rounds = 300\\ngsearch.fit(\\n    X=X_train, \\n    y=y_train.label,\\n    eval_set=[(X_val, y_val.label)],\\n    eval_metric=[&quot;l2&quot;,&quot;auc&quot;,&quot;binary&quot;],\\n    callbacks=[early_stopping(early_stopping_rounds)],\\n    categorical_feature=categorical_features,\\n    verbose=10\\n)\\n</code></pre>\\n<p>but encountered the following error</p>\\n<pre><code>/root/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:821: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \\nTraceback (most recent call last):\\n  File &quot;/root/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py&quot;, line 810, in _score\\n    scores = scorer(estimator, X_test, y_test)\\n  File &quot;/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py&quot;, line 266, in __call__\\n    return self._score(partial(_cached_call, None), estimator, X, y_true, **_kwargs)\\n  File &quot;/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_scorer.py&quot;, line 355, in _score\\n    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)\\nTypeError: summary_of_threshold() missing 1 required positional argument: \\'y\\'\\n</code></pre>\\n'},\n",
       "  {'tags': ['machine-learning', 'logistic-regression'],\n",
       "   'owner': {'account_id': 25849919,\n",
       "    'reputation': 29,\n",
       "    'user_id': 19584077,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a-/AFdZucrCmYzTwHvRn8tLsiv140lphA95CX5ny2dTw7uO=k-s256',\n",
       "    'display_name': 'jhfw',\n",
       "    'link': 'https://stackoverflow.com/users/19584077/jhfw'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 340,\n",
       "   'closed_date': 1701047044,\n",
       "   'accepted_answer_id': 77550993,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700993252,\n",
       "   'creation_date': 1700982008,\n",
       "   'last_edit_date': 1700993252,\n",
       "   'question_id': 77550758,\n",
       "   'link': 'https://stackoverflow.com/questions/77550758/is-there-a-standard-format-for-saving-trained-logistic-regression-models',\n",
       "   'closed_reason': 'Not suitable for this site',\n",
       "   'title': 'Is there a standard format for saving trained logistic regression models?',\n",
       "   'body': \"<p>I was asked in an interview the other day what data structure I would use to save a trained logistic regression model, and I was kind of at a loss for words. I asked for some clarification, but didn't receive much other than just having the question restated. I just said something about how I would use <code>sklearn.linear_model.Logistic_Regression</code> to instantiate a model object, and then use the <code>.fit()</code> method to fit it, at which point you could save the model with <code>pickle</code>/<code>joblib</code>. This seemed like it was probably not the right answer, but it was the best I could think of in the moment.</p>\\n<p>I'm familiar with saving PyTorch models as a <code>state_dict</code> which are basically just Python <code>dict</code>s, but as far as I'm aware, using <code>pickle</code> or <code>joblib</code> just save them in a binary format, and I'm not really sure if that even counts as a data structure. (Just a note - the question was not specific to <code>sklearn</code> or even to Python, but those are the tools I use most so I defaulted to them.)</p>\\n<p>After some Googling and digging on SO, I have not been able to find anything that answers my question.</p>\\n<p>My questions are a) what data structure could you use to hold a trained logistic regression model? And b) is there some widely accepted way of doing this that I'm unaware of? Is there just some gap in my knowledge here?</p>\\n\"},\n",
       "  {'tags': ['pandas', 'dataframe', 'machine-learning', 'dataset'],\n",
       "   'owner': {'account_id': 25730386,\n",
       "    'reputation': 1,\n",
       "    'user_id': 19486071,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a-/AOh14GioBRaY24rZn_GXNdKA08-ODepJj2aq8itMwmdenw=k-s256',\n",
       "    'display_name': 'Sameer Kulkarni',\n",
       "    'link': 'https://stackoverflow.com/users/19486071/sameer-kulkarni'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 49,\n",
       "   'answer_count': 1,\n",
       "   'score': -1,\n",
       "   'last_activity_date': 1700976415,\n",
       "   'creation_date': 1700966267,\n",
       "   'last_edit_date': 1700976415,\n",
       "   'question_id': 77550258,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77550258/in-titanic-dataset-question-is-to-find-the-survived-male-passengers-i-couldnt-f',\n",
       "   'title': 'In titanic dataset question is to find the survived male passengers i couldn&#39;t find the right output',\n",
       "   'body': \"<p><code>[titanic_train.iloc[np.where(titanic_train[&quot;Sex&quot;] == 'male')][&quot;Survived&quot;] == 1]</code>\\nthis line gives  the correct output in terms of true false conditions but gives error while extracting the name col from that dataset your text\\n<code>titanic_train[titanic_train.iloc[np.where(titanic_train[&quot;Sex&quot;] == 'male')][&quot;Survived&quot;] == 1]</code></p>\\n<p>error</p>\\n<pre><code>IndexingError                             Traceback (most recent call last)\\nCell In[87], line 1\\n----&gt; 1 titanic_train[titanic_train.iloc[np.where(titanic_train[&quot;Sex&quot;] == 'male')][&quot;Survived&quot;] == 1]\\n\\nFile C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\frame.py:3798, in DataFrame.__getitem__(self, key)\\n   3796 # Do we have a (boolean) 1d indexer?\\n   3797 if com.is_bool_indexer(key):\\n-&gt; 3798     return self._getitem_bool_array(key)\\n   3800 # We are left with two options: a single key, and a collection of keys,\\n   3801 # We interpret tuples as collections only for non-MultiIndex\\n   3802 is_single_key = isinstance(key, tuple) or not is_list_like(key)\\n\\nFile C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\frame.py:3851, in DataFrame._getitem_bool_array(self, key)\\n   3845     raise ValueError(\\n   3846         f&quot;Item wrong length {len(key)} instead of {len(self.index)}.&quot;\\n   3847     )\\n   3849 # check_bool_indexer will throw exception if Series key cannot\\n   3850 # be reindexed to match DataFrame rows\\n-&gt; 3851 key = check_bool_indexer(self.index, key)\\n   3852 indexer = key.nonzero()[0]\\n   3853 return self._take_with_is_copy(indexer, axis=0)\\n\\nFile C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\indexing.py:2552, in check_bool_indexer(index, key)\\n   2550 indexer = result.index.get_indexer_for(index)\\n   2551 if -1 in indexer:\\n-&gt; 2552     raise IndexingError(\\n   2553         &quot;Unalignable boolean Series provided as &quot;\\n   2554         &quot;indexer (index of the boolean Series and of &quot;\\n   2555         &quot;the indexed object do not match).&quot;\\n   2556     )\\n   2558 result = result.take(indexer)\\n   2560 # fall through for boolean\\n\\nIndexingError: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\\n</code></pre>\\n<p>extracting names from above code</p>\\n\"},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'random-forest',\n",
       "    'h2o',\n",
       "    'feature-selection'],\n",
       "   'owner': {'account_id': 9039375,\n",
       "    'reputation': 233,\n",
       "    'user_id': 7214340,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh6.googleusercontent.com/-GcX9rg-nZfE/AAAAAAAAAAI/AAAAAAAAAKY/JeEO03DyIWU/photo.jpg?sz=256',\n",
       "    'display_name': 'Mirko',\n",
       "    'link': 'https://stackoverflow.com/users/7214340/mirko'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 133,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701118919,\n",
       "   'creation_date': 1700953406,\n",
       "   'question_id': 77549868,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77549868/recursive-feature-elimination-with-h2o-random-forest',\n",
       "   'title': 'Recursive Feature Elimination with H2O Random Forest',\n",
       "   'body': \"<p>I am using h2o package in python to build a fairly complex model.\\nIt has around 1500 features, but I know that most of them are not really important and I would like to extract the subset of a given size (let's say 100) that maximizes the R squared of my model.<br />\\nIs there some method that is already implementing this for h2o in python?</p>\\n<p>Otherwise I would need to code it myself, but that also implies to run the model multiple times and I am not sure i would code it in the correct way.</p>\\n<p>One possible way to code it is this one:</p>\\n<ul>\\n<li>Save the R2 for the model, then remove the k less important features</li>\\n<li>Create a second model without the removed features</li>\\n<li>Calculate the R2 for the new model and compare to the previous R2. Use a metric to decide whether to keep the new model or stick with the old.</li>\\n<li>Iterate these steps until the previous step chooses the old model as the best one\\nI am pretty sure this will not give me the 'best subset' of feature but I really hope it would be sufficient.</li>\\n</ul>\\n<p>The second method I thought of is the following:</p>\\n<ul>\\n<li>set the number of feature <code>N</code> you want in the new model and the number of iterations <code>K</code></li>\\n<li>Save the original model R2 as reference</li>\\n<li>Extract <code>N</code> features at random from the original model, using their relative importance as probability of being extracted (more important features more likely extracted)</li>\\n<li>For each model save the list of features and the new R2</li>\\n<li>After iterating <code>K</code> times stop the algorithm and compare the R2</li>\\n<li>Choose the set of features with the closest R2 to the original one</li>\\n</ul>\\n\"},\n",
       "  {'tags': ['python', 'tensorflow', 'machine-learning'],\n",
       "   'owner': {'account_id': 24367614,\n",
       "    'reputation': 15,\n",
       "    'user_id': 18301110,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/82eeea28b65f33d2492ecd9c6a6b45fc?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'darkdragontr',\n",
       "    'link': 'https://stackoverflow.com/users/18301110/darkdragontr'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 183,\n",
       "   'accepted_answer_id': 77550575,\n",
       "   'answer_count': 1,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1700977199,\n",
       "   'creation_date': 1700949411,\n",
       "   'last_edit_date': 1700976389,\n",
       "   'question_id': 77549710,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77549710/np-argmax-gives-wrong-index-on-prediction-class',\n",
       "   'title': 'Np.argmax gives wrong index on prediction class',\n",
       "   'body': \"<p>I've written so simple handwritten classifier example. It works great, it has %98 accuracy on test samples. However, when I have given an sample input to it, np.argmax gives wrong index (even the ones larger than the size of the classes). Could you help me?</p>\\n<pre><code>import numpy as np\\nimport cv2 as cv\\nimport matplotlib.pyplot as plt\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nfrom keras import layers,datasets,models\\n\\nos.environ['TFF_CPP_MIN_LOG_LEVEL'] = '2'\\n\\n\\n(training_images,training_labels),(testing_images,testing_labels)= datasets.mnist.load_data()\\ntraining_images,testing_images = training_images/255.0,testing_images/255.0\\n\\nclass_names = [0,1,2,3,4,5,6,7,8,9]\\n\\nmodel = keras.Sequential(\\n    [   \\n        layers.Flatten(input_shape=(28,28)),\\n        layers.Dense(512,activation=&quot;relu&quot;),\\n        layers.Dense(256,activation=&quot;relu&quot;),\\n        layers.Dense(10),\\n    ]\\n)\\n\\nmodel.compile(\\n    optimizer=keras.optimizers.Adam(lr=0.001),\\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\\n    metrics=[&quot;accuracy&quot;]\\n)\\nmodel.fit(training_images,training_labels,epochs=5,batch_size=32,verbose=2)\\nloss,accuracy=model.evaluate(testing_images,testing_labels)\\nprint(f&quot;Loss: {loss}&quot;)\\nprint(f&quot;Accuracy: {accuracy}&quot;)\\nmodel.save(&quot;handwritten_classifier.model&quot;)\\n\\n\\nmodel = models.load_model(&quot;handwritten_classifier.model&quot;)\\n\\nimg = cv.imread(&quot;seven.png&quot;)\\nimg = cv.cvtColor(img,cv.COLOR_BGR2RGB)\\nprint(np.array(img).shape)\\nplt.imshow(img,cmap=plt.cm.binary)\\n\\nprediction = model.predict(img.reshape(-1,28,28)/255.0)\\n\\nplt.show()\\nprint(prediction)\\nindex = np.argmax(prediction)\\nprint(index)\\nprint(f&quot;Prediction is {class_names[index]}&quot;)```\\n\\nI've tried reshaping the input matrix but it did not work..\\n</code></pre>\\n\"},\n",
       "  {'tags': ['machine-learning', 'predictive'],\n",
       "   'owner': {'account_id': 24082596,\n",
       "    'reputation': 1,\n",
       "    'user_id': 18057065,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/a01ca19ee48cd8aceb51ad7a0dc28bb6?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'Harsha',\n",
       "    'link': 'https://stackoverflow.com/users/18057065/harsha'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 167,\n",
       "   'answer_count': 1,\n",
       "   'score': -2,\n",
       "   'last_activity_date': 1700977350,\n",
       "   'creation_date': 1700946739,\n",
       "   'last_edit_date': 1700976377,\n",
       "   'question_id': 77549616,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77549616/sensitivity-analysis-based-on-variance',\n",
       "   'title': 'Sensitivity analysis based on variance',\n",
       "   'body': \"<p>Please help me in understanding the sensitivity analysis based on variance in ml models</p>\\n<p>I am working on a binary classification problem apart from predicting the target variable I am also interested in finding the most important features contributing to the occurrence of specific class.</p>\\n<p>To achieve this I'm performing sensitivity analysis;I am using k cross validation and I am removing one column at a time and calculating the change in variance when all the columns are included</p>\\n<p>I expected the variable which when removed has got the highest variance, to be present at top of random forest and decision tree but it's not at the top</p>\\n<p>Please help me understand how the sensitivity analysis I'm doing is different from the variables at the top of random forest</p>\\n<p>Thank you</p>\\n\"},\n",
       "  {'tags': ['python',\n",
       "    'tensorflow',\n",
       "    'machine-learning',\n",
       "    'neural-network',\n",
       "    'lstm'],\n",
       "   'owner': {'account_id': 16726592,\n",
       "    'reputation': 61,\n",
       "    'user_id': 12090057,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a-/AAuE7mA-r2c3rGAh-8tgLu4ecDRwAbgR-eCBebL9TPe-=k-s256',\n",
       "    'display_name': 'Baldomero',\n",
       "    'link': 'https://stackoverflow.com/users/12090057/baldomero'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 443,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700917066,\n",
       "   'creation_date': 1700879750,\n",
       "   'last_edit_date': 1700917066,\n",
       "   'question_id': 77546459,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77546459/training-a-neural-network-to-play-rock-paper-scissors-gets-stuck-in-a-single-re',\n",
       "   'title': 'Training a neural network to play Rock Paper Scissors, gets stuck in a single reply',\n",
       "   'body': '<p><strong>Background</strong>: I am trying to teach a neural network to play rock-paper-scissors against 4 different bots, to solve a task for a coding course (freecodecamp.com) Each bot is a pre-determined strategy, and some are more complex than others. I\\'m a beginner in neural networks and I\\'m trying to become self-taught. I\\'m having problems creating a network that defeats the strongest of them all. I\\'ve tried to follow instructions mainly from the course and also from this blog entry by Svilen Todorov <a href=\"https://svilentodorov.xyz/blog/rps/\" rel=\"nofollow noreferrer\">https://svilentodorov.xyz/blog/rps/</a></p>\\n<p><strong>The problem</strong>: I\\'m trying to create a neural network that &quot;defeats&quot; the following bot:</p>\\n<pre><code>def abbey(prev_opponent_play,\\n          opponent_history=[],\\n          play_order=[{\\n              &quot;RR&quot;: 0,\\n              &quot;RP&quot;: 0,\\n              &quot;RS&quot;: 0,\\n              &quot;PR&quot;: 0,\\n              &quot;PP&quot;: 0,\\n              &quot;PS&quot;: 0,\\n              &quot;SR&quot;: 0,\\n              &quot;SP&quot;: 0,\\n              &quot;SS&quot;: 0,\\n          }]):\\n\\n    if not prev_opponent_play:\\n        prev_opponent_play = \\'R\\'\\n    opponent_history.append(prev_opponent_play)\\n\\n    last_two = &quot;&quot;.join(opponent_history[-2:])\\n    if len(last_two) == 2:\\n        play_order[0][last_two] += 1\\n\\n    potential_plays = [\\n        prev_opponent_play + &quot;R&quot;,\\n        prev_opponent_play + &quot;P&quot;,\\n        prev_opponent_play + &quot;S&quot;,\\n    ]\\n\\n    sub_order = {\\n        k: play_order[0][k]\\n        for k in potential_plays if k in play_order[0]\\n    }\\n\\n    prediction = max(sub_order, key=sub_order.get)[-1:]\\n\\n    ideal_response = {\\'P\\': \\'S\\', \\'R\\': \\'P\\', \\'S\\': \\'R\\'}\\n    return ideal_response[prediction]\\n</code></pre>\\n<p>As you can see, the bot is deterministic (no randomness) but capable of a varied response to the opponent\\'s &quot;history&quot;.</p>\\n<p><strong>Attempted solution</strong> The first thing I did was obtain a large (n=2000) dataset of games where random moves were played against &quot;Abby&quot;. After that, trained my neural network on that data, but couldn\\'t reach a high (&gt;80%) accuracy using 10 epochs. I decided not to use more epochs, but rather to fight this new version of the neural network to fight Abby, then obtain that data, and re-feed it into the neural network, as to try and &quot;update&quot; its database. For some reason, every time I updated it, the result was that the neural network got stuck into a loop after a few plays, sometimes from the outset, and of course the bot beat it. Sometimes it was a 1-play loop (R - R - R - R etc) sometimes it was 2 or 3-play loops (P - S - R - P - S - R - P etc)</p>\\n<p>This is the code for the neural network:</p>\\n<pre><code>import tensorflow as tf\\nfrom tensorflow import keras\\nfrom dataManipulation import open_csv\\nimport numpy as np\\nimport csv\\n\\nTRAINING_ITER = 0   # If at 0, check the file_path manually\\n\\nUPDATING = True\\nRESET_STATES = True\\n\\nNUM_GAMES = 20000\\nSEQ_LENGTH = 10  # I can feel this is going to be tricky. We are back to len 10.\\nTRAINING = True\\nDENSE_NEURONS = 24\\nRNN_UNITS = 512\\nEPOCH_NUM = 2\\n\\n\\nif TRAINING_ITER == 0:\\n    file_path = &quot;db_random_n20000.csv&quot;\\nelse:\\n    file_path = f\\'./databases/abbey/db1_nn_abbey_n{NUM_GAMES}_0{TRAINING_ITER-1}.csv\\'  # Raw data\\nload_chk_path = f\\'./checkpoints/abbey/trained{TRAINING_ITER-1}\\'\\nlast_saved_chk_path = f\\'./checkpoints/abbey/trained{TRAINING_ITER}\\'\\n\\n# OHE the data\\nmove_to_ohe = {  # Create a dictionary to OHE the data\\n    &quot;R&quot;: [1, 0, 0],\\n    &quot;P&quot;: [0, 1, 0],\\n    &quot;S&quot;: [0, 0, 1],\\n}\\n\\n\\ndef build_model(seq_l, vocab_size, rnn_units, dense_neurons):\\n    this_model = tf.keras.Sequential([\\n        tf.keras.layers.Input(shape=(seq_l, vocab_size), dtype=\\'int32\\'),\\n        tf.keras.layers.Dense(dense_neurons, activation=\\'relu\\'),\\n        tf.keras.layers.LSTM(rnn_units,\\n                             return_sequences=True),\\n        tf.keras.layers.Dense(3, activation=\\'softmax\\')\\n    ])\\n    return this_model\\n\\n\\nif __name__ == &quot;__main__&quot;:\\n    # Import the data\\n    raw_data = open_csv(file_path)\\n    data = np.array([[move_to_ohe[val] for val in row] for row in raw_data])  \\n    # Next, we need to sequence the data, since the model won\\'t operate on single-move games, nor on games like Todorov\\'s\\n    total_seqs = len(data) - (SEQ_LENGTH + 1)\\n\\n    # Creating the training sequences\\n    Xy1_sequences = np.empty((total_seqs, SEQ_LENGTH + 1, 2, 3), dtype=int)\\n    for i, d in enumerate(data):\\n        if i == total_seqs:\\n            break\\n        Xy1_sequences[i] = data[i: i + (SEQ_LENGTH + 1)]\\n    x1_seq = Xy1_sequences[:, :-1, :, :]\\n    y1_seq = Xy1_sequences[:, 1:, 0, :]  # y1 has to be only about the opponent        \\n\\n    x1_seq, y1_seq = np.reshape(x1_seq, (total_seqs, SEQ_LENGTH, 6)), \\\\\\n        np.reshape(y1_seq, (total_seqs, SEQ_LENGTH, 3))\\n\\n\\n    # I\\'ll try building the model following Russica, since Todorov\\'s method is not working.\\n\\n    model = build_model(SEQ_LENGTH, 6, RNN_UNITS, DENSE_NEURONS)\\n    print(&quot;Database len: &quot;, len(x1_seq))\\n\\n    if TRAINING:\\n        model.summary()\\n        opt = keras.optimizers.Adam(learning_rate=0.001)\\n        if UPDATING:\\n            if TRAINING_ITER != 0:\\n                model.load_weights(load_chk_path).expect_partial()\\n        model.compile(loss=\\'categorical_crossentropy\\', optimizer=opt,\\n                      metrics=[\\'accuracy\\'])\\n        model.fit(x1_seq, y1_seq, epochs=EPOCH_NUM, shuffle=False, batch_size=8, verbose=2)\\n\\n        # Save the weights\\n        model.save_weights(last_saved_chk_path)\\n        print(&quot;Iteration number &quot;, TRAINING_ITER)\\n        print(&quot;Weights saved to &quot;, last_saved_chk_path)\\n    else:\\n        model.load_weights(load_chk_path).expect_partial()  # We are OK with working with weights only. This silences warnings.\\n        model.summary()\\n</code></pre>\\n<p>While this code contains the &quot;player&quot;, a function that defines a response to the bot, and that is used in another &quot;Play&quot; function. I didn\\'t write the Play function, it was incorporated in the task.</p>\\n<p>I have the feeling that I\\'m doing something wrong with the loss function, but I don\\'t know what could it be.</p>\\n<p>To make matters more complicated, I already used this code to beat a simpler bot:</p>\\n<pre><code>def quincy(prev_play, counter=[0]):\\n    counter[0] += 1\\n    choices = [&quot;R&quot;, &quot;R&quot;, &quot;P&quot;, &quot;P&quot;, &quot;S&quot;]\\n    return choices[counter[0] % len(choices)]\\n</code></pre>\\n<p>So I don\\'t think there\\'s a bug in the code itself (that is, the Neural Network can at least recognize the patterns, and there doesn\\'t seem to be any errors in encoding that might be causing a wrong result) I don\\'t know if I\\'m doing something wrong while building my model, if I\\'m choosing the wrong layers, or if there is some sort of hyperparameter I\\'m not taking into consideration. I have changed:</p>\\n<p>The # of epochs</p>\\n<p>The activation functions for all layers</p>\\n<p>The number of neurons for all layers</p>\\n<p>Updating the model or just using the new dataset, when iterating the model.</p>\\n<p>Stateful LSTM on or off</p>\\n<p>Reset state of the model before every play</p>\\n'},\n",
       "  {'tags': ['machine-learning',\n",
       "    'scikit-learn',\n",
       "    'precision',\n",
       "    'xgboost',\n",
       "    'gridsearchcv'],\n",
       "   'owner': {'account_id': 12719725,\n",
       "    'reputation': 641,\n",
       "    'user_id': 9240584,\n",
       "    'user_type': 'registered',\n",
       "    'accept_rate': 62,\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/959e6b35956149adcb917edd05a2fbb2?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'thileepan',\n",
       "    'link': 'https://stackoverflow.com/users/9240584/thileepan'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 285,\n",
       "   'accepted_answer_id': 77547630,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700939474,\n",
       "   'creation_date': 1700850167,\n",
       "   'last_edit_date': 1700914484,\n",
       "   'question_id': 77545038,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77545038/precision-and-recall-for-which-class-is-calculated-in-gridsearchcv',\n",
       "   'title': 'Precision and recall for which class is calculated in GridSearchCV?',\n",
       "   'body': '<p>I\\'m running GridSearchCV on XGBoost model to tune some parameters. My target variable is binary. When I use, \\'recall\\' or \\'precision\\' as scoring metrics in cross validation, I get a single value. I\\'m not using make_scorer, I\\'m only passing strings for whichever metric I want to calculate. I\\'m confused if the recall and precision is for class \\'0\\' or class \\'1\\'.</p>\\n<p>So, which precision and recall is for which class?</p>\\n<p>As you can see in the example image below (not related to my model) from sklearn\\'s classification_report module, both precision and recall can be calculated for individual labels/classes in the target variable.\\n<a href=\"https://i.sstatic.net/9Seki.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/9Seki.png\" alt=\"enter image description here\" /></a></p>\\n'},\n",
       "  {'tags': ['python', 'machine-learning', 'tf-idf', 'tfidfvectorizer'],\n",
       "   'owner': {'account_id': 19505251,\n",
       "    'reputation': 1,\n",
       "    'user_id': 14270452,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a-/AOh14GjsZPBXjWwHDNddTGtNmBCLSBJQs_brsgKF3fP8MQ=k-s256',\n",
       "    'display_name': 'sherwin desouza',\n",
       "    'link': 'https://stackoverflow.com/users/14270452/sherwin-desouza'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 63,\n",
       "   'answer_count': 2,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1721830272,\n",
       "   'creation_date': 1700817378,\n",
       "   'last_edit_date': 1700837662,\n",
       "   'question_id': 77541978,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77541978/i-do-not-understand-the-working-of-tfidfvectorizer-of-sckit-learn',\n",
       "   'title': 'I do not understand the working of tfidfvectorizer of sckit-learn',\n",
       "   'body': \"<p>The formula I know to calculate tf-idf is TF * IDF where TF is the number of times the word occurs in a document D and IDF is Number Of Documents/ Number Of Documents which contains the word + 1.</p>\\n<p>This is my dataset.\\n<code>corpus = [ 'This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?', ]</code>\\nNow I calculated td-idf of the word 'document' in document 1, the output was 0.22.\\nBut when I used sckit's tfidf vectorizer, the output was:\\n1.22314355\\nThe vectorizer I used had the following parameters:\\n<code>vectorizer = TfidfVectorizer(norm=None) </code>\\nPlease explain me why is the answer different.</p>\\n\"},\n",
       "  {'tags': ['machine-learning', 'coreml', 'coremltools', 'yolov8'],\n",
       "   'owner': {'account_id': 6355139,\n",
       "    'reputation': 1084,\n",
       "    'user_id': 4932747,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://i.sstatic.net/cxgoL.jpg?s=256',\n",
       "    'display_name': 'Sajjad Sarkoobi',\n",
       "    'link': 'https://stackoverflow.com/users/4932747/sajjad-sarkoobi'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 795,\n",
       "   'accepted_answer_id': 77543096,\n",
       "   'answer_count': 2,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1718257524,\n",
       "   'creation_date': 1700816973,\n",
       "   'last_edit_date': 1700835583,\n",
       "   'question_id': 77541935,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77541935/inconsistent-object-detection-performance-between-yolov8n-pytorch-and-converted',\n",
       "   'title': 'Inconsistent Object Detection Performance Between YOLOv8n PyTorch and Converted CoreML Model',\n",
       "   'body': '<p>I\\'m experiencing a significant performance discrepancy between my YOLOv8n object detection model in its original PyTorch format (.pt) and after converting it to CoreML format for deployment on iOS devices. The original model, trained on a custom dataset, detects objects successfully in a given image. However, the converted CoreML model fails to detect any objects in the same image.\\nI tested in some other images. Although it detects objects in iOS and Mac devices, it does not perform the same as the original .pt detection.</p>\\n<p><strong>Details:</strong></p>\\n<p>Original Model: YOLOv8n, trained on a custom dataset using Ultralytics\\' implementation. in CoLap by using A100.</p>\\n<p>Conversion Tool:</p>\\n<pre><code>!pip install coremltools\\n</code></pre>\\n<pre><code>from ultralytics import YOLO\\nmodel_path=f&quot;{HOME}/runs/detect/train/weights/best.pt&quot;\\nmodel=YOLO(model_path)\\nmodel.export(format=\\'coreml\\', nms=True)\\n</code></pre>\\n<p><strong>Questions:</strong></p>\\n<ol>\\n<li><p>Are there specific layers or operations in YOLOv8n that are known to have compatibility issues with CoreML?</p>\\n</li>\\n<li><p>What are the recommended steps for debugging such a discrepancy in object detection performance between the original and converted models?</p>\\n</li>\\n</ol>\\n<p>Any insights or suggestions for further troubleshooting?</p>\\n<p><a href=\"https://i.sstatic.net/drwfV.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/drwfV.jpg\" alt=\"enter image description here\" /></a>\\n<a href=\"https://i.sstatic.net/EhU1M.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/EhU1M.jpg\" alt=\"enter image description here\" /></a></p>\\n'},\n",
       "  {'tags': ['python', 'tensorflow', 'machine-learning', 'dqn'],\n",
       "   'owner': {'account_id': 29981079,\n",
       "    'reputation': 1,\n",
       "    'user_id': 22976061,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/4c7469dae414d03eebd4c77d116d1c8e?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'Simon',\n",
       "    'link': 'https://stackoverflow.com/users/22976061/simon'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 74,\n",
       "   'accepted_answer_id': 77613851,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701872602,\n",
       "   'creation_date': 1700812220,\n",
       "   'last_edit_date': 1700835703,\n",
       "   'question_id': 77541451,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77541451/error-while-using-tensorflow-dqn-agent-collect-policy',\n",
       "   'title': 'Error while using Tensorflow dqn_agent collect_policy',\n",
       "   'body': \"<p>I'm using the tensorflow DQN Agent with a Simulink Environment.\\nWhile calling the agents collect policy</p>\\n<pre><code>agent.collect_policy.action(time_step)\\n</code></pre>\\n<p>I get the following error:</p>\\n<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node \\n__wrapped__Select_device_/job:localhost/replica:0/task:0/device:CPU:0}} 'then' and 'else' must have the same size.  but received: [1] vs. [] [Op:Select] name:\\n</code></pre>\\n<p>calling the standard policy is working</p>\\n<pre><code>agent.policy.action(time_step)\\n</code></pre>\\n<p>I double checked the wether my <code>TimeStep</code> matches my <code>TimeStepSpec</code> and it matches.\\n(I guess the agent.policy wouldn't if it wouldn't match)</p>\\n<p>As far as I know the call of both policies is pretty similar in <code>tf_policy.py</code> so I have no idea what's causing the problem.\\nIf anybody has an idea what causes the error feel free to help :)</p>\\n<p>Heres a code snippet of my agent, etc. I hope this will help</p>\\n<p>the specification:</p>\\n<pre><code>discount = 0.95\\n\\nreward = 0.0\\n\\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\\n\\ntime_step_spec = TimeStep(step_type = tensor_spec.BoundedTensorSpec(shape=(1,), dtype=tf.int32, minimum=0, maximum=2),\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 reward = tensor_spec.TensorSpec(shape=(1,), dtype=tf.float32),\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 discount = tensor_spec.TensorSpec(shape=(1,), dtype=tf.float32), #fix\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 observation = \\xa0tensor_spec.TensorSpec(shape=(1,amountMachines), dtype=tf.int32)\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 )\\n\\n\\nnum_possible_actions = 729\\n\\naction_spec = tensor_spec.BoundedTensorSpec(\\n\\n\\xa0 \\xa0 shape=(), dtype=tf.int32, minimum=0, maximum=num_possible_actions - 1)\\n\\n\\nagent = dqn_agent.DqnAgent(\\n\\n\\xa0 \\xa0 time_step_spec,\\n\\n\\xa0 \\xa0 action_spec,\\n\\n\\xa0 \\xa0 q_network=model,\\n\\n\\xa0 \\xa0 optimizer=optimizer,\\n\\n\\xa0 \\xa0 epsilon_greedy= 1.0,\\n\\n\\xa0 \\xa0 td_errors_loss_fn=common.element_wise_squared_loss,\\n\\n\\xa0 \\xa0 train_step_counter=train_step_counter)\\n\\nagent.initialize()\\n</code></pre>\\n<p>the call:</p>\\n<pre><code>current_state = get_states() #gets a np.array looking like this [4,4,4,4,4,6]\\n\\ncurrent_state_batch = tf.expand_dims( tf.convert_to_tensor(current_state, dtype=tf.int32), axis=0\\n\\n\\n\\n\\ntime_step = TimeStep(step_type=tf.convert_to_tensor([step_type], dtype=tf.int32),\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 reward=tf.convert_to_tensor([reward], dtype=tf.float32),\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 discount=tf.convert_to_tensor([discount], dtype=tf.float32),\\n\\n\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 observation= current_state_batch)\\n\\n\\n\\n\\naction_step = agent.collect_policy.action(time_step)\\n</code></pre>\\n<p>This is the whole error code:</p>\\n<pre><code>Traceback (most recent call last):   File &quot;C:\\\\Users\\\\STestUser\\\\AppData\\\\Local\\\\anaconda3\\\\Lib\\\\runpy.py&quot;, line 198, in _run_module_as_main\\n    return _run_code(code, main_globals, None,\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File &quot;C:\\\\Users\\\\STestUser\\\\AppData\\\\Local\\\\anaconda3\\\\Lib\\\\runpy.py&quot;, line 88,  in _run_code\\n    exec(code, run_globals)   File &quot;c:\\\\Users\\\\STestUser\\\\.vscode\\\\extensions\\\\ms-python.python-2023.20.0\\\\pythonFiles\\\\lib\\\\python\\\\debugpy\\\\adapter/../..\\\\debugpy\\\\launcher/../..\\\\debugpy\\\\__main__.py&quot;, line 39, in &lt;module&gt;\\n    cli.main()   File &quot;c:\\\\Users\\\\STestUser\\\\.vscode\\\\extensions\\\\ms-python.python-2023.20.0\\\\pythonFiles\\\\lib\\\\python\\\\debugpy\\\\adapter/../..\\\\debugpy\\\\launcher/../..\\\\debugpy/..\\\\debugpy\\\\server\\\\cli.py&quot;, line 430, in main\\n    run()   File &quot;c:\\\\Users\\\\STestUser\\\\.vscode\\\\extensions\\\\ms-python.python-2023.20.0\\\\pythonFiles\\\\lib\\\\python\\\\debugpy\\\\adapter/../..\\\\debugpy\\\\launcher/../..\\\\debugpy/..\\\\debugpy\\\\server\\\\cli.py&quot;, line 284, in run_file\\n    runpy.run_path(target, run_name=&quot;__main__&quot;)   File &quot;c:\\\\Users\\\\STestUser\\\\.vscode\\\\extensions\\\\ms-python.python-2023.20.0\\\\pythonFiles\\\\lib\\\\python\\\\debugpy\\\\_vendored\\\\pydevd\\\\_pydevd_bundle\\\\pydevd_runpy.py&quot;, line 321, in run_path\\n    return _run_module_code(code, init_globals, run_name,\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File &quot;c:\\\\Users\\\\STestUser\\\\.vscode\\\\extensions\\\\ms-python.python-2023.20.0\\\\pythonFiles\\\\lib\\\\python\\\\debugpy\\\\_vendored\\\\pydevd\\\\_pydevd_bundle\\\\pydevd_runpy.py&quot;, line 135, in _run_module_code\\n    _run_code(code, mod_globals, init_globals,   File &quot;c:\\\\Users\\\\STestUser\\\\.vscode\\\\extensions\\\\ms-python.python-2023.20.0\\\\pythonFiles\\\\lib\\\\python\\\\debugpy\\\\_vendored\\\\pydevd\\\\_pydevd_bundle\\\\pydevd_runpy.py&quot;, line 124, in _run_code\\n    exec(code, run_globals)   File &quot;d:\\\\Hochschule\\\\Master\\\\Masterarbeit\\\\energy-efficiency-optimation\\\\RL-Modell\\\\OP10_QLearning.py&quot;, line 449, in &lt;module&gt;\\n    action_step = agent.collect_policy.action(time_step = time_step_t)        \\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^           File &quot;C:\\\\Users\\\\STestUser\\\\AppData\\\\Local\\\\anaconda3\\\\Lib\\\\site-packages\\\\tf_agents\\\\policies\\\\tf_policy.py&quot;, line 333, in action\\n    step = action_fn(time_step=time_step, policy_state=policy_state, seed=seed)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File &quot;C:\\\\Users\\\\STestUser\\\\AppData\\\\Local\\\\anaconda3\\\\Lib\\\\site-packages\\\\tf_agents\\\\utils\\\\common.py&quot;, line 193, in with_check_resource_vars\\n    return fn(*fn_args, **fn_kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^   File &quot;C:\\\\Users\\\\STestUser\\\\AppData\\\\Local\\\\anaconda3\\\\Lib\\\\site-packages\\\\tf_agents\\\\policies\\\\epsilon_greedy_policy.py&quot;, line 141, in _action\\n    action = tf.nest.map_structure(\\n             ^^^^^^^^^^^^^^^^^^^^^^   File &quot;C:\\\\Users\\\\STestUser\\\\AppData\\\\Local\\\\anaconda3\\\\Lib\\\\site-packages\\\\tensorflow\\\\python\\\\util\\\\nest.py&quot;, line 629, in map_structure\\n    return nest_util.map_structure(\\n           ^^^^^^^^^^^^^^^^^^^^^^^^   File &quot;C:\\\\Users\\\\STestUser\\\\AppData\\\\Local\\\\anaconda3\\\\Lib\\\\site-packages\\\\tensorflow\\\\python\\\\util\\\\nest_util.py&quot;, line 1168, in map_structure\\n    return _tf_core_map_structure(func, *structure, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File &quot;C:\\\\Users\\\\STestUser\\\\AppData\\\\Local\\\\anaconda3\\\\Lib\\\\site-packages\\\\tensorflow\\\\python\\\\util\\\\nest_util.py&quot;, line 1208, in _tf_core_map_structure\\n    [func(*x) for x in entries],\\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^   File &quot;C:\\\\Users\\\\STestUser\\\\AppData\\\\Local\\\\anaconda3\\\\Lib\\\\site-packages\\\\tensorflow\\\\python\\\\util\\\\nest_util.py&quot;, line 1208, in &lt;listcomp&gt;\\n    [func(*x) for x in entries],\\n     ^^^^^^^^   File &quot;C:\\\\Users\\\\STestUser\\\\AppData\\\\Local\\\\anaconda3\\\\Lib\\\\site-packages\\\\tf_agents\\\\policies\\\\epsilon_greedy_policy.py&quot;, line 142, in &lt;lambda&gt;\\n    lambda g, r: tf.compat.v1.where(cond, g, r),\\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File &quot;C:\\\\Users\\\\STestUser\\\\AppData\\\\Local\\\\anaconda3\\\\Lib\\\\site-packages\\\\tensorflow\\\\python\\\\util\\\\traceback_utils.py&quot;, line 153, in error_handler\\n    raise e.with_traceback(filtered_tb) from None   File &quot;C:\\\\Users\\\\STestUser\\\\AppData\\\\Local\\\\anaconda3\\\\Lib\\\\site-packages\\\\tensorflow\\\\python\\\\framework\\\\ops.py&quot;, line 5888, in raise_from_not_ok_status\\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node \\n__wrapped__Select_device_/job:localhost/replica:0/task:0/device:CPU:0}} 'then' and 'else' must have the same size.  but received: [1] vs. [] [Op:Select] name:\\n</code></pre>\\n\"},\n",
       "  {'tags': ['performance',\n",
       "    'machine-learning',\n",
       "    'gpu',\n",
       "    'artificial-intelligence',\n",
       "    'nvidia'],\n",
       "   'owner': {'account_id': 29979389,\n",
       "    'reputation': 11,\n",
       "    'user_id': 22974738,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/5dba8d62ef0fe92205f874d9a2ad189a?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'dabanana',\n",
       "    'link': 'https://stackoverflow.com/users/22974738/dabanana'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 1283,\n",
       "   'answer_count': 1,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1700859790,\n",
       "   'creation_date': 1700792491,\n",
       "   'last_edit_date': 1700846695,\n",
       "   'question_id': 77540410,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77540410/calculating-arithmetic-intensity',\n",
       "   'title': 'Calculating Arithmetic Intensity',\n",
       "   'body': '<p>Nvidia\\'s GPU Performance Background User\\'s Guide (<a href=\"https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html\" rel=\"nofollow noreferrer\">https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html</a>) lists the following arithmetic intensities for fp16 data opreations.</p>\\n<p>I\\'m trying to derive the AI math behind each of them.</p>\\n<div class=\"s-table-container\">\\n<table class=\"s-table\">\\n<thead>\\n<tr>\\n<th>#</th>\\n<th>Operation</th>\\n<th>Arithmetic Intensity</th>\\n<th>Usually limited by</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>1</td>\\n<td>Linear layer (4096 outputs, 1024 inputs, batch size 512)</td>\\n<td>315 FLOPS/B</td>\\n<td>arithmetic</td>\\n</tr>\\n<tr>\\n<td>2</td>\\n<td>Linear layer (4096 outputs, 1024 inputs, batch size 1)</td>\\n<td>1 FLOPS/B</td>\\n<td>memory</td>\\n</tr>\\n<tr>\\n<td>3</td>\\n<td>Max pooling with 3x3 window and unit stride</td>\\n<td>2.25 FLOPS/B</td>\\n<td>memory</td>\\n</tr>\\n<tr>\\n<td>4</td>\\n<td>ReLU activation</td>\\n<td>0.25 FLOPS/B</td>\\n<td>memory</td>\\n</tr>\\n<tr>\\n<td>5</td>\\n<td>Layer normalization</td>\\n<td>&lt; 10 FLOPS/B</td>\\n<td>memory</td>\\n</tr>\\n</tbody>\\n</table>\\n</div>\\n<p>For #2 I get:</p>\\n<ul>\\n<li>Flops =  2 ops * 1024 inputs * 4096 weights each</li>\\n<li>Memory =  2 bytes * (1024 input reads + 1024*4096 weight reads + 4096 write outputs)</li>\\n<li>--&gt; Flops/Memory ~= 1 Flops/B</li>\\n</ul>\\n<p>What about the remaining operations?</p>\\n'},\n",
       "  {'tags': ['numpy',\n",
       "    'machine-learning',\n",
       "    'random',\n",
       "    'random-seed',\n",
       "    'numpy-random'],\n",
       "   'owner': {'account_id': 10857207,\n",
       "    'reputation': 374,\n",
       "    'user_id': 7983179,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/f83586bb4f2fbe308619ce7524731597?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'saper0',\n",
       "    'link': 'https://stackoverflow.com/users/7983179/saper0'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 676,\n",
       "   'accepted_answer_id': 77538119,\n",
       "   'answer_count': 2,\n",
       "   'score': 3,\n",
       "   'last_activity_date': 1701049192,\n",
       "   'creation_date': 1700753768,\n",
       "   'question_id': 77538082,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77538082/correctly-seeding-numpy-random-generator',\n",
       "   'title': 'Correctly seeding numpy random generator',\n",
       "   'body': '<p>For my scientific experiments, I usually seed using:</p>\\n<p><code>rng = np.random.Generator(np.random.PCG64(seed))</code></p>\\n<p>which for the current numpy version is equivalent to</p>\\n<p><code>rng = np.random.Generator(np.random.default_rng(seed))</code></p>\\n<p>As I repeat my experiments <code>n</code> times and average their results, I usually set the <code>seed</code> to all the numbers between <code>0</code> and <code>n</code>.</p>\\n<p>However, reading the documentations <a href=\"https://numpy.org/doc/stable/reference/random/index.html#random-quick-start\" rel=\"nofollow noreferrer\">here</a> and <a href=\"https://numpy.org/doc/stable/reference/random/bit_generators/index.html#seeding-and-entropy\" rel=\"nofollow noreferrer\">here</a> it states that</p>\\n<blockquote>\\n<p>Seeds should be large positive integers.</p>\\n</blockquote>\\n<p>or</p>\\n<blockquote>\\n<p>We default to using a 128-bit integer using entropy gathered from the OS. This is a good amount of entropy to initialize all of the generators that we have in numpy. We do not recommend using small seeds below 32 bits for general use.</p>\\n</blockquote>\\n<p>However, in the second reference, it also states</p>\\n<blockquote>\\n<p>There will not be anything wrong with the results, per se; even a seed of 0 is perfectly fine thanks to the processing that SeedSequence does.</p>\\n</blockquote>\\n<p>This feels contradictory and I wonder, if small seeds are now totally fine to use, or one should move towards higher seeds. Especially, I wonder, (i) at which point (if any) would a large seed make a difference to a low seed and (ii) if one does scientific experiments (e.g. machine learning / algorithmic research) should one prefer higher to lower seeds or should it not make a difference?</p>\\n<p>PS: This question is highly related to <a href=\"https://stackoverflow.com/questions/41336548/random-number-seed-in-numpy\">Random number seed in numpy</a> but concerns the now recommended Generator. Furthermore, the answer seems not in-depth enough as it does not include a discussion about high and low seeds.</p>\\n'},\n",
       "  {'tags': ['python', 'tensorflow', 'machine-learning', 'kaggle'],\n",
       "   'owner': {'account_id': 25922441,\n",
       "    'reputation': 143,\n",
       "    'user_id': 19644799,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/AItbvmmyxHUa0O1hnK4x3I8ZHeGzqtlhcXclNFES66yG=k-s256',\n",
       "    'display_name': 'WinterSolstice',\n",
       "    'link': 'https://stackoverflow.com/users/19644799/wintersolstice'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 1360,\n",
       "   'answer_count': 0,\n",
       "   'score': 4,\n",
       "   'last_activity_date': 1738960099,\n",
       "   'creation_date': 1700750978,\n",
       "   'question_id': 77537786,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77537786/how-do-i-use-python-version-3-7-1-in-kaggle',\n",
       "   'title': 'How do I use python version 3.7.1 in Kaggle',\n",
       "   'body': \"<p>I'm trying to train a tensorflow ML model scripted in python version 3.7.1. However the python version in kaggle is 3.10.2. Is there any method through which I can use a python 3.7 environment and is it possible to make it the default env?</p>\\n\"},\n",
       "  {'tags': ['python', 'machine-learning', 'mediapipe'],\n",
       "   'owner': {'account_id': 29975881,\n",
       "    'reputation': 1,\n",
       "    'user_id': 22972120,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/e250c708014f07e0465f6fcd7b8622de?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'mvw007',\n",
       "    'link': 'https://stackoverflow.com/users/22972120/mvw007'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 152,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1704103378,\n",
       "   'creation_date': 1700749811,\n",
       "   'question_id': 77537666,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77537666/real-time-sign-language-translator-with-mediapipe',\n",
       "   'title': 'Real time sign language translator with mediapipe',\n",
       "   'body': \"<p>So, i'm making a sign language translator with mediapipe, and it works fine till i hold up both of my hands, it gives the following error: &quot;X has 84 features, but RandomForestClassifier is expecting 42 features as input&quot;. I've tried tons of different solutions but none of them have worked so far.\\nThis is the code i'm running:</p>\\n<pre><code>import pickle\\nimport cv2\\nimport mediapipe as mp\\nimport numpy as np\\n\\nmodel_dict = pickle.load(open('./model.p', 'rb'))\\nmodel = model_dict['model']\\n\\ncap = cv2.VideoCapture(0)\\n\\nmp_hands = mp.solutions.hands\\nmp_drawing = mp.solutions.drawing_utils\\nmp_drawing_styles = mp.solutions.drawing_styles\\n\\nhands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\\n\\nlabels_dict = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L',\\n               12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21: 'V', 22: 'W',\\n               23: 'X', 24: 'Y', 25: 'Z', 26: 'Middelvinger', 27: 'Dikke vette duim'}\\n\\ncurrent_char = &quot;niks&quot;  # Initialiseer current_char buiten de loop\\n\\nwhile True:\\n    try:\\n        # Initialiseer lege lijsten en cordinaten\\n        data_aux = []\\n        x_ = []\\n        y_ = []\\n\\n        # Lees een frame van de webcam\\n        ret, frame = cap.read()\\n\\n        # Haal de hoogte, breedte en kanalen van het frame op\\n        H, W, _ = frame.shape\\n\\n        # Converteer het frame naar RGB-formaat\\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\\n\\n        # Verwerk de handen in het frame met behulp van MediaPipe Hands\\n        results = hands.process(frame_rgb)\\n\\n        # Controleer of handen zijn gedetecteerd in het frame\\n        if results.multi_hand_landmarks:\\n            # Loop over alle gedetecteerde handen in het frame\\n            for hand_landmarks in results.multi_hand_landmarks:\\n                # Teken de landmarks en verbindingslijnen op het frame\\n                mp_drawing.draw_landmarks(\\n                    frame,  # afbeelding om op te tekenen\\n                    hand_landmarks,  # model output\\n                    mp_hands.HAND_CONNECTIONS,  # verbindingslijnen\\n                    mp_drawing_styles.get_default_hand_landmarks_style(),\\n                    mp_drawing_styles.get_default_hand_connections_style())\\n\\n            # Loop over alle gedetecteerde handen in het frame\\n            for hand_landmarks in results.multi_hand_landmarks:\\n                # Loop over alle landmarks (puntcordinaten) van de hand\\n                for i in range(len(hand_landmarks.landmark)):\\n                    x = hand_landmarks.landmark[i].x\\n                    y = hand_landmarks.landmark[i].y\\n\\n                    # Voeg de x- en y-cordinaten toe aan de respectievelijke lijsten\\n                    x_.append(x)\\n                    y_.append(y)\\n\\n                # Normaliseer de cordinaten door de minimale x- en y-waarden af te trekken\\n                for i in range(len(hand_landmarks.landmark)):\\n                    x = hand_landmarks.landmark[i].x\\n                    y = hand_landmarks.landmark[i].y\\n                    data_aux.append(x - min(x_))\\n                    data_aux.append(y - min(y_))\\n\\n            # Bereken de positie om een tekstvenster te plaatsen\\n            x1 = int(min(x_) * W) - 10\\n            y1 = int(min(y_) * H) - 10\\n\\n            # Voorspel het karakter met behulp van het getrainde model\\n            prediction = model.predict([np.asarray(data_aux)])\\n            predicted_character = labels_dict[int(prediction[0])]\\n\\n            # Haal de waarschijnlijkheid van de voorspelling op\\n            prob = model.predict_proba([np.asarray(data_aux)])\\n            accuracy_percentage = max(prob[0]) * 100\\n\\n            # Toon het voorspelde karakter en de nauwkeurigheid in de linkerbovenhoek\\n            cv2.putText(frame, f&quot;{predicted_character} {accuracy_percentage:.2f}%&quot;, (10, 30),\\n                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 3, cv2.LINE_AA)\\n\\n            # Controleer of het karakter is veranderd\\n            if predicted_character != current_char:\\n                print(f&quot;{accuracy_percentage:.2f}% zekerheid dat het {predicted_character} is&quot;)\\n                current_char = predicted_character\\n\\n        # Toon het frame met de getekende handen en voorspellingen\\n        cv2.imshow('frame', frame)\\n        cv2.waitKey(1)`your text`\\n\\n    except Exception as e:\\n        print(f&quot;Fout code: {e}&quot;)\\n\\ncap.release()\\ncv2.destroyAllWindows()\\n</code></pre>\\n<p>Any of you guys have any idea on how to fix my problem?\\nThanks in advance</p>\\n<p>I tried a lot of different solutions but none of them worked so far. i'm expecting the program to be able to scan and translate both of my hands at the same time</p>\\n\"},\n",
       "  {'tags': ['c++', 'machine-learning', 'onnx', 'onnxruntime'],\n",
       "   'owner': {'account_id': 17565577,\n",
       "    'reputation': 612,\n",
       "    'user_id': 16627522,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/-hZrDRnIU3wA/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rfYqaQxMS0XpsX3r40TYBxFN67YbA/photo.jpg?sz=256',\n",
       "    'display_name': 'Tommy Wolfheart',\n",
       "    'link': 'https://stackoverflow.com/users/16627522/tommy-wolfheart'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 476,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700748388,\n",
       "   'creation_date': 1700747966,\n",
       "   'last_edit_date': 1700748388,\n",
       "   'question_id': 77537482,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77537482/onnx-ort-invalid-argument2-exception-when-creating-tensor-with-ortvaluecre',\n",
       "   'title': 'ONNX ORT_INVALID_ARGUMENT(2) exception when creating tensor with Ort::Value::CreateTensor',\n",
       "   'body': \"<p>I am trying to create a tensor for ONNX with <code>Ort::Value::CreateTensor</code> using the following snippet:</p>\\n<pre><code>Ort::MemoryInfo MemoryInfo = Ort::MemoryInfo::CreateCpu(OrtAllocatorType::OrtDeviceAllocator, OrtMemType::OrtMemTypeCPU);\\n\\n\\nstd::vector&lt;Ort::Value&gt; vInputTensors;\\n    \\nfor (std::vector&lt;float&gt; vfInput : TestDataset.GetInputs()) {\\n    unsigned uSequenceLength = vfInput.size();\\n\\n    // Shape: (batch_size=1, sequence_length, num_norm_attributes, num_channels=1)\\n    const std::vector&lt;int64_t&gt; vi64InputShape = {BATCH_SIZE, uSequenceLength, 2 * NUM_ATTRIBUTES, NUM_CHANNELS};\\n\\n    Ort::Value::CreateTensor&lt;float&gt;(MemoryInfo, vfInput.data(), uSequenceLength, vi64InputShape.data(), vi64InputShape.size());\\n    //vInputTensors.push_back(Ort::Value::CreateTensor&lt;float&gt;(MemoryInfo, vfInput.data(), uSequenceLength, vi64InputShape.data(), vi64InputShape.size()));\\n}\\n</code></pre>\\n<p>I have printed the elements in <code>vfInput.data()</code> and <code>vi64InputShape.data()</code> and printed the values of <code>uSequenceLength</code> and <code>vi64InputShape.size()</code>. And everything is a expected.</p>\\n<p>However, the program throws an exception:</p>\\n<pre><code>Unhandled exception at 0x00007FFC7ABDCF19 in TestApp.exe: Microsoft C++ exception: Ort::Exception at memory location 0x00000036901AF6B0.\\n</code></pre>\\n<p>When I change <code>BATCH_SIZE, uSequenceLength, 2 * NUM_ATTRIBUTES, NUM_CHANNELS}</code> to <code>{BATCH_SIZE, 100, 2 * NUM_ATTRIBUTES, NUM_CHANNELS}</code>, it doesn't complain but of course that's not correct.</p>\\n<p>Please may you assist me with this.</p>\\n<p>Thank you.</p>\\n\"},\n",
       "  {'tags': ['machine-learning', 'deep-learning', 'data-transfer', 'llama'],\n",
       "   'owner': {'account_id': 29975226,\n",
       "    'reputation': 3,\n",
       "    'user_id': 22971610,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/ACg8ocLXgq8zeEPAXEIxODsA8C0EfeguEdQcvjziFsiUPlzp=k-s256',\n",
       "    'display_name': 'Suhaib Masood',\n",
       "    'link': 'https://stackoverflow.com/users/22971610/suhaib-masood'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 683,\n",
       "   'accepted_answer_id': 77537093,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700746505,\n",
       "   'creation_date': 1700744044,\n",
       "   'last_edit_date': 1700746505,\n",
       "   'question_id': 77537041,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77537041/how-to-install-llama-2-0-on-virtual-envirnoment',\n",
       "   'title': 'How to install LLama 2.0 on virtual Envirnoment',\n",
       "   'body': \"<p>I am trying to run llama on my local server. After running Quickstart.ipynb Iam getting error <code>llama_recipes not found</code>. I tried to install llama_recipes from directory but didn't work. Also while loading, I am getting error about <code>accelerator</code></p>\\n\"},\n",
       "  {'tags': ['python', 'numpy', 'machine-learning', 'keras', 'shap'],\n",
       "   'owner': {'account_id': 29974234,\n",
       "    'reputation': 47,\n",
       "    'user_id': 22970853,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/cdbb887c2bd4889f5ab69edc1c31e8c3?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'Pitone',\n",
       "    'link': 'https://stackoverflow.com/users/22970853/pitone'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 826,\n",
       "   'accepted_answer_id': 77544061,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1704868617,\n",
       "   'creation_date': 1700735228,\n",
       "   'last_edit_date': 1704868617,\n",
       "   'question_id': 77536125,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77536125/plot-the-shap-values-for-lstm-model-in-python',\n",
       "   'title': 'Plot the SHAP values for LSTM model in Python',\n",
       "   'body': \"<p>I have the following code that is working.</p>\\n<pre><code>import numpy as np\\nimport shap\\nfrom tensorflow import keras\\n\\nX = np.array([[(1,2,3,3,1),(3,2,1,3,2),(3,2,2,3,3),(2,2,1,1,2),(2,1,1,1,1)],\\n              [(4,5,6,4,4),(5,6,4,3,2),(5,5,6,1,3),(3,3,3,2,2),(2,3,3,2,1)],\\n              [(7,8,9,4,7),(7,7,6,7,8),(5,8,7,8,8),(6,7,6,7,8),(5,7,6,6,6)],\\n              [(7,8,9,8,6),(6,6,7,8,6),(8,7,8,8,8),(8,6,7,8,7),(8,6,7,8,8)],\\n              [(4,5,6,5,5),(5,5,5,6,4),(6,5,5,5,6),(4,4,3,3,3),(5,5,4,4,5)],\\n              [(4,5,6,5,5),(5,5,5,6,4),(6,5,5,5,6),(4,4,3,3,3),(5,5,4,4,5)],\\n              [(1,2,3,3,1),(3,2,1,3,2),(3,2,2,3,3),(2,2,1,1,2),(2,1,1,1,1)]])\\ny = np.array([0, 1, 2, 2, 1, 1, 0])\\n\\n# Updated model with correct input shape\\nmodel = keras.Sequential([\\n    keras.layers.LSTM(128, return_sequences=True, input_shape=(5, 5)),  # LSTM layer with return sequences\\n    keras.layers.LSTM(128, return_sequences=False),  # Another LSTM layer\\n    keras.layers.Flatten(),\\n    keras.layers.Dense(128, activation='relu'),\\n    keras.layers.Dense(3, activation='softmax')  # 3 output classes\\n])\\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\\n\\n# Train the model\\nmodel.fit(X, y, epochs=10)\\n\\n# Use GradientExplainer with the model itself\\nexplainer = shap.GradientExplainer(model, X)\\nshap_values = explainer.shap_values(X)\\nprint(shap_values)\\n</code></pre>\\n<p>I want to display a nice plot of <code>SHAP</code> values.</p>\\n<p>I tryied the following line of code\\n<code>shap.summary_plot(shap_values, X, feature_names=['Feature 1', 'Feature 2', 'Feature 3', 'Feature 4', 'Feature 5']) </code>\\nbut is not working</p>\\n\"},\n",
       "  {'tags': ['machine-learning',\n",
       "    'amazon-sagemaker',\n",
       "    'amazon-efs',\n",
       "    'amazon-sagemaker-studio'],\n",
       "   'owner': {'account_id': 2256358,\n",
       "    'reputation': 611,\n",
       "    'user_id': 1987469,\n",
       "    'user_type': 'registered',\n",
       "    'accept_rate': 47,\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/3fce2c6a872ea7192065420d19f5a499?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'Sid',\n",
       "    'link': 'https://stackoverflow.com/users/1987469/sid'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 175,\n",
       "   'answer_count': 0,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1700735086,\n",
       "   'creation_date': 1700735086,\n",
       "   'question_id': 77536110,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77536110/sagemaker-giving-oserror-errno-28-no-space-left-on-device-when-separate-mod',\n",
       "   'title': 'Sagemaker giving &#39;OSError: [Errno 28] No space left on device&#39; when separate model and code zip is given',\n",
       "   'body': \"<p>This looks like an interesting problem.</p>\\n<p>When I am zipping my model and code together(like below), things work fine.</p>\\n<pre><code>huggingface_model = HuggingFaceModel(\\n   model_data=&quot;s3://abc/xyz/model.tar.gz&quot;,      # s3 path having both model and code\\n   role=role,                    # iam role with permissions to create an Endpoint\\n   transformers_version=&quot;4.17&quot;,  # transformers version used\\n   pytorch_version=&quot;1.10&quot;,       # pytorch version used\\n   py_version='py38',            # python version used\\n)\\n</code></pre>\\n<p>On un-zipping, folder structure will be like below:</p>\\n<pre><code>model.tar.gz\\n  |\\n   - stable-diffusion\\n  |\\n   - controlnet\\n  |\\n   - code\\n</code></pre>\\n<p>Where code has two files <code>inference.py</code> and <code>requirements.txt</code></p>\\n<p>As mentioned before, this works fine.</p>\\n<p>Since creating zip of code and model together means that zipping and uploading time is very huge. So I want to de-couple model and code and tried below.</p>\\n<pre><code>huggingface_model = HuggingFaceModel(\\n   model_data=&quot;s3://abc/xyz/model.tar.gz&quot;,      # path to your model\\n   source_dir=&quot;s3://abc/xyz/sourcedir.tar.gz&quot;,       # path to you script\\n   entry_point=&quot;inference.py&quot;,\\n   role=role,                    # iam role with permissions to create an Endpoint\\n   transformers_version=&quot;4.17&quot;,  # transformers version used\\n   pytorch_version=&quot;1.10&quot;,       # pytorch version used\\n   py_version='py38',            # python version used\\n)\\n</code></pre>\\n<p>Here on unzipping, <code>model.tar.gz</code> will give you <code>stable-diffusion</code> and <code>controlnet</code>. And unzipping <code>sourcedir.tar.gz</code> will give you <code>inference.py</code> and <code>requirements.txt</code>.</p>\\n<p>This is giving following error:</p>\\n<pre><code>---------------------------------------------------------------------------\\nOSError                                   Traceback (most recent call last)\\nCell In[13], line 16\\n      5 huggingface_model = HuggingFaceModel(\\n      6    model_data=s3_model_uri,      # path to your model\\n      7    source_dir=s3_code_uri,       # parth to you script\\n   (...)\\n     12    py_version='py38',            # python version used\\n     13 )\\n     15 # deploy the endpoint endpoint\\n---&gt; 16 predictor = huggingface_model.deploy(\\n     17     initial_instance_count=1,\\n     18     instance_type=&quot;ml.g4dn.xlarge&quot;\\n     19     )\\n\\nFile /opt/conda/lib/python3.10/site-packages/sagemaker/huggingface/model.py:313, in HuggingFaceModel.deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\\n    306     inference_tool = &quot;neuron&quot; if instance_type.startswith(&quot;ml.inf1&quot;) else &quot;neuronx&quot;\\n    307     self.image_uri = self.serving_image_uri(\\n    308         region_name=self.sagemaker_session.boto_session.region_name,\\n    309         instance_type=instance_type,\\n    310         inference_tool=inference_tool,\\n    311     )\\n--&gt; 313 return super(HuggingFaceModel, self).deploy(\\n    314     initial_instance_count,\\n    315     instance_type,\\n    316     serializer,\\n    317     deserializer,\\n    318     accelerator_type,\\n    319     endpoint_name,\\n    320     tags,\\n    321     kms_key,\\n    322     wait,\\n    323     data_capture_config,\\n    324     async_inference_config,\\n    325     serverless_inference_config,\\n    326     volume_size=volume_size,\\n    327     model_data_download_timeout=model_data_download_timeout,\\n    328     container_startup_health_check_timeout=container_startup_health_check_timeout,\\n    329     inference_recommendation_id=inference_recommendation_id,\\n    330     explainer_config=explainer_config,\\n    331 )\\n\\nFile /opt/conda/lib/python3.10/site-packages/sagemaker/model.py:1406, in Model.deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\\n   1403     if self._base_name is not None:\\n   1404         self._base_name = &quot;-&quot;.join((self._base_name, compiled_model_suffix))\\n-&gt; 1406 self._create_sagemaker_model(\\n   1407     instance_type=instance_type,\\n   1408     accelerator_type=accelerator_type,\\n   1409     tags=tags,\\n   1410     serverless_inference_config=serverless_inference_config,\\n   1411 )\\n   1413 serverless_inference_config_dict = (\\n   1414     serverless_inference_config._to_request_dict() if is_serverless else None\\n   1415 )\\n   1416 production_variant = sagemaker.production_variant(\\n   1417     self.name,\\n   1418     instance_type,\\n   (...)\\n   1424     container_startup_health_check_timeout=container_startup_health_check_timeout,\\n   1425 )\\n\\nFile /opt/conda/lib/python3.10/site-packages/sagemaker/model.py:794, in Model._create_sagemaker_model(self, instance_type, accelerator_type, tags, serverless_inference_config)\\n    768 def _create_sagemaker_model(\\n    769     self,\\n    770     instance_type=None,\\n   (...)\\n    773     serverless_inference_config=None,\\n    774 ):\\n    775     &quot;&quot;&quot;Create a SageMaker Model Entity\\n    776 \\n    777     Args:\\n   (...)\\n    792             not provided in serverless inference. So this is used to find image URIs.\\n    793     &quot;&quot;&quot;\\n--&gt; 794     container_def = self.prepare_container_def(\\n    795         instance_type,\\n    796         accelerator_type=accelerator_type,\\n    797         serverless_inference_config=serverless_inference_config,\\n    798     )\\n    800     if not isinstance(self.sagemaker_session, PipelineSession):\\n    801         # _base_name, model_name are not needed under PipelineSession.\\n    802         # the model_data may be Pipeline variable\\n    803         # which may break the _base_name generation\\n    804         model_uri = None\\n\\nFile /opt/conda/lib/python3.10/site-packages/sagemaker/huggingface/model.py:498, in HuggingFaceModel.prepare_container_def(self, instance_type, accelerator_type, serverless_inference_config, inference_tool)\\n    489     deploy_image = self.serving_image_uri(\\n    490         region_name,\\n    491         instance_type,\\n   (...)\\n    494         inference_tool=inference_tool,\\n    495     )\\n    497 deploy_key_prefix = model_code_key_prefix(self.key_prefix, self.name, deploy_image)\\n--&gt; 498 self._upload_code(deploy_key_prefix, repack=True)\\n    499 deploy_env = dict(self.env)\\n    500 deploy_env.update(self._script_mode_env_vars())\\n\\nFile /opt/conda/lib/python3.10/site-packages/sagemaker/model.py:723, in Model._upload_code(self, key_prefix, repack)\\n    707     self.uploaded_code = fw_utils.UploadedCode(\\n    708         s3_prefix=repacked_model_data,\\n    709         script_name=os.path.basename(self.entry_point),\\n    710     )\\n    712 LOGGER.info(\\n    713     &quot;Repacking model artifact (%s), script artifact &quot;\\n    714     &quot;(%s), and dependencies (%s) &quot;\\n   (...)\\n    720     repacked_model_data,\\n    721 )\\n--&gt; 723 utils.repack_model(\\n    724     inference_script=self.entry_point,\\n    725     source_directory=self.source_dir,\\n    726     dependencies=self.dependencies,\\n    727     model_uri=self.model_data,\\n    728     repacked_model_uri=repacked_model_data,\\n    729     sagemaker_session=self.sagemaker_session,\\n    730     kms_key=self.model_kms_key,\\n    731 )\\n    733 self.repacked_model_data = repacked_model_data\\n\\nFile /opt/conda/lib/python3.10/site-packages/sagemaker/utils.py:517, in repack_model(inference_script, source_directory, dependencies, model_uri, repacked_model_uri, sagemaker_session, kms_key)\\n    510 local_download_dir = (\\n    511     None\\n    512     if sagemaker_session.settings is None\\n    513     or sagemaker_session.settings.local_download_dir is None\\n    514     else sagemaker_session.settings.local_download_dir\\n    515 )\\n    516 with _tmpdir(directory=local_download_dir) as tmp:\\n--&gt; 517     model_dir = _extract_model(model_uri, sagemaker_session, tmp)\\n    519     _create_or_update_code_dir(\\n    520         model_dir,\\n    521         inference_script,\\n   (...)\\n    525         tmp,\\n    526     )\\n    528     tmp_model_path = os.path.join(tmp, &quot;temp-model.tar.gz&quot;)\\n\\nFile /opt/conda/lib/python3.10/site-packages/sagemaker/utils.py:607, in _extract_model(model_uri, sagemaker_session, tmp)\\n    605     local_model_path = model_uri.replace(&quot;file://&quot;, &quot;&quot;)\\n    606 with tarfile.open(name=local_model_path, mode=&quot;r:gz&quot;) as t:\\n--&gt; 607     t.extractall(path=tmp_model_dir)\\n    608 return tmp_model_dir\\n\\nFile /opt/conda/lib/python3.10/tarfile.py:2059, in TarFile.extractall(self, path, members, numeric_owner)\\n   2057         tarinfo.mode = 0o700\\n   2058     # Do not set_attrs directories, as we will do that further down\\n-&gt; 2059     self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),\\n   2060                  numeric_owner=numeric_owner)\\n   2062 # Reverse sort directories.\\n   2063 directories.sort(key=lambda a: a.name)\\n\\nFile /opt/conda/lib/python3.10/tarfile.py:2100, in TarFile.extract(self, member, path, set_attrs, numeric_owner)\\n   2097     tarinfo._link_target = os.path.join(path, tarinfo.linkname)\\n   2099 try:\\n-&gt; 2100     self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\\n   2101                          set_attrs=set_attrs,\\n   2102                          numeric_owner=numeric_owner)\\n   2103 except OSError as e:\\n   2104     if self.errorlevel &gt; 0:\\n\\nFile /opt/conda/lib/python3.10/tarfile.py:2173, in TarFile._extract_member(self, tarinfo, targetpath, set_attrs, numeric_owner)\\n   2170     self._dbg(1, tarinfo.name)\\n   2172 if tarinfo.isreg():\\n-&gt; 2173     self.makefile(tarinfo, targetpath)\\n   2174 elif tarinfo.isdir():\\n   2175     self.makedir(tarinfo, targetpath)\\n\\nFile /opt/conda/lib/python3.10/tarfile.py:2222, in TarFile.makefile(self, tarinfo, targetpath)\\n   2220     target.truncate()\\n   2221 else:\\n-&gt; 2222     copyfileobj(source, target, tarinfo.size, ReadError, bufsize)\\n\\nFile /opt/conda/lib/python3.10/tarfile.py:251, in copyfileobj(src, dst, length, exception, bufsize)\\n    249     if len(buf) &lt; bufsize:\\n    250         raise exception(&quot;unexpected end of data&quot;)\\n--&gt; 251     dst.write(buf)\\n    253 if remainder != 0:\\n    254     buf = src.read(remainder)\\n\\nOSError: [Errno 28] No space left on device\\n</code></pre>\\n\"},\n",
       "  {'tags': ['tensorflow',\n",
       "    'machine-learning',\n",
       "    'bluetooth',\n",
       "    'audio-recording',\n",
       "    'tensorflow.js'],\n",
       "   'owner': {'account_id': 1089130,\n",
       "    'reputation': 40363,\n",
       "    'user_id': 1084174,\n",
       "    'user_type': 'registered',\n",
       "    'accept_rate': 89,\n",
       "    'profile_image': 'https://i.sstatic.net/BsASP.png?s=256',\n",
       "    'display_name': 'Sazzad Hissain Khan',\n",
       "    'link': 'https://stackoverflow.com/users/1084174/sazzad-hissain-khan'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 78,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701077328,\n",
       "   'creation_date': 1700732916,\n",
       "   'last_edit_date': 1701077328,\n",
       "   'question_id': 77535833,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77535833/how-to-change-mic-source-in-tensoraudio',\n",
       "   'title': 'How to change Mic source in TensorAudio?',\n",
       "   'body': '<p>I have used Tensorflow <a href=\"https://www.tensorflow.org/lite/inference_with_metadata/task_library/audio_classifier#step_2_using_the_model\" rel=\"nofollow noreferrer\">sample</a> for audio classification.</p>\\n<pre><code>AudioClassifier classifier = AudioClassifier.createFromFile(this, MODEL_FILE);\\nTensorAudio audioTensor = classifier.createInputTensorAudio();\\nAudioRecord record = classifier.createAudioRecord();\\nrecord.startRecording();\\n</code></pre>\\n<p>However, it seem recorder always picking the default Android Phones Mic even when I am connected with Bluetooth Mic (Earbud). Is there any way to let TensorAudio know the interesting mic source?</p>\\n<p><strong>Update:</strong></p>\\n<pre><code>AudioManager audioManager = (AudioManager) getSystemService(Context.AUDIO_SERVICE);\\nAudioDeviceInfo[] audioDevices = audioManager.getDevices(AudioManager.GET_DEVICES_INPUTS);\\nAudioDeviceInfo bluetoothMic = null;\\nfor (AudioDeviceInfo device : audioDevices) {\\n    if (device.getType() == AudioDeviceInfo.TYPE_BLUETOOTH_SCO) {\\n        bluetoothMic = device;\\n        break;\\n    }\\n}\\nrecord.setPreferredDevice(bluetoothMic);\\n</code></pre>\\n<p>I got a way around and used the above code but in that case, classification does not work. No class detected. When I skip setPreferredDevice (default mic), it works fine. What is the reason?</p>\\n'},\n",
       "  {'tags': ['python', 'machine-learning', 'conv-neural-network', 'pydicom'],\n",
       "   'owner': {'account_id': 23295472,\n",
       "    'reputation': 1,\n",
       "    'user_id': 17376948,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/99615c85f6256ebff691e9d3930a6cd8?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'M.H.Q',\n",
       "    'link': 'https://stackoverflow.com/users/17376948/m-h-q'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 262,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700740024,\n",
       "   'creation_date': 1700731769,\n",
       "   'last_edit_date': 1700740024,\n",
       "   'question_id': 77535704,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77535704/attributeerror-filedataset-object-has-no-attribute-modality',\n",
       "   'title': 'AttributeError: &#39;FileDataset&#39; object has no attribute &#39;Modality&#39;',\n",
       "   'body': \"<p>I am a novice to ML and trying to apply TotalSegmentator to my .dcm files. Initially, I was trying to convert my dcm files to nifti using dicom2nifti but I got this error</p>\\n<pre><code>AttributeError: 'FileDataset' object has no attribute 'Modality'\\n</code></pre>\\n<p>now I'm using totalsegmentator to and am getting the same.</p>\\n<pre><code>totalsegmentator('/PATH_TO_DIR/train_images/10004/21057','/outs')\\n</code></pre>\\n<p>I tired looking into the dcm file using pydicoms dcmread but could not find modality in it.\\nBut my dataset does suggest standardizing the dcms like this:</p>\\n<pre><code>def standardize_pixel_array(dcm: pydicom.dataset.FileDataset) -&gt; np.ndarray:\\n    # Correct DICOM pixel_array if PixelRepresentation == 1.\\n    pixel_array = dcm.pixel_array\\n    if dcm.PixelRepresentation == 1:\\n        bit_shift = dcm.BitsAllocated - dcm.BitsStored\\n        dtype = pixel_array.dtype \\n        pixel_array = (pixel_array &lt;&lt; bit_shift).astype(dtype) &gt;&gt;  bit_shift\\n        pixel_array = pydicom.pixel_data_handlers.util.apply_modality_lut(pixel_array, dcm)\\n    return pixel_array\\n</code></pre>\\n\"},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'scikit-learn',\n",
       "    'random-forest',\n",
       "    'joblib'],\n",
       "   'owner': {'account_id': 27465082,\n",
       "    'reputation': 65,\n",
       "    'user_id': 20954952,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://i.sstatic.net/jM3fH.jpg?s=256',\n",
       "    'display_name': 'Tiago',\n",
       "    'link': 'https://stackoverflow.com/users/20954952/tiago'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 86,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700740075,\n",
       "   'creation_date': 1700731492,\n",
       "   'last_edit_date': 1700740075,\n",
       "   'question_id': 77535670,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77535670/why-does-random-forest-give-different-result-when-run-in-paralel',\n",
       "   'title': 'Why does Random Forest give different result when run in paralel?',\n",
       "   'body': '<p>I am running Random Forest in Databricks (the skitlearn library one).\\nWhen I run the model :</p>\\n<pre><code>rf = RandomForestRegressor(n_estimators=150,max_features=3,min_samples_leaf=5,min_samples_split=12,n_jobs=7, random_state=0)\\n</code></pre>\\n<p>category at a time, it gives me good results.\\nBut when I run categories in parallel (using Joblib library) it gives me way worst results (sometimes the double than what I expected). Why is that? Is there a way of running it in parallel and get the same result?</p>\\n<p>I know that Random Forest are non-deterministic, but when running one category multiple times it gives me always good results.</p>\\n<p>EDIT:</p>\\n<p>Basically I have different categories for different products at a supermarket. Categories like A,B,C,...,Z. And inside each category I have different range of products.</p>\\n<p>Then I have a function where I can pass those categories and I do some data manipulation, feature engineering, etc..., then I split the data and apply the random forest regressor to the data.</p>\\n<p>I am using joblib function parallel so I can train and test multiple categories at a time.</p>\\n<p>The thing I am noticing is that when I run like this:</p>\\n<pre><code>model(&quot;A&quot;)\\n</code></pre>\\n<p>it gives me good results, everytime I run it it gives me different results because of being non-deterministic but I gives me always around the same values. For pretty much all categories and products</p>\\n<p>When running with joblib, I am running like this:</p>\\n<pre><code>parallel = Parallel(n_jobs=7, pre_dispatch=&quot;n_jobs&quot;, backend=&quot;threading&quot;)\\nout = parallel(delayed(lifecycle_model)(category) for category in list_of_categories)\\n</code></pre>\\n<p>And this gives me way off results, sometimes double that what I as expecting.</p>\\n'},\n",
       "  {'tags': ['python', 'machine-learning', 'deep-learning'],\n",
       "   'owner': {'account_id': 29972056,\n",
       "    'reputation': 9,\n",
       "    'user_id': 22969104,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/ACg8ocKeJ35fHzCOxybutdL-hU_D8krSqw8WbdaKK-hRnOsugn-P=k-s256',\n",
       "    'display_name': 'Jacob Lee',\n",
       "    'link': 'https://stackoverflow.com/users/22969104/jacob-lee'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 84,\n",
       "   'answer_count': 1,\n",
       "   'score': -6,\n",
       "   'last_activity_date': 1701246266,\n",
       "   'creation_date': 1700717492,\n",
       "   'last_edit_date': 1700740518,\n",
       "   'question_id': 77534533,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77534533/how-to-predict-the-household-water-consumption-in-2050',\n",
       "   'title': 'How to predict the household water consumption in 2050',\n",
       "   'body': \"<p>I have data related to the household water consumption.</p>\\n<p>The key issue is whether we need more water purification plant in the future or not.</p>\\n<p>The population will be decreasing. It means that household water consumption will be going down.</p>\\n<p>However, the number of one- and two-person household will be going up. it means that household water consumption will be going up.\\nBecause LPCD(Liter Per Capita Day) of one- or two-person household is larger than over three-person household based on the household water consumption.</p>\\n<p>Therefore, I want to know whether we need more water purification plant in 2050 or not.</p>\\n<p>I added the dataset and my code.\\nThe result of my code was not good.</p>\\n<p>Could you check my code and algorithm?</p>\\n<p>the data is below.</p>\\n<p>I would like to explain the data shortly.</p>\\n<p>It has 8 columns. The dependent variable is only one which is the household_water_consumption_unit_ton.\\nThe year column is the index.\\nI would like to predict the household_water_consumption_unit_ton in 2050.\\nFinally, I would like to add a new column whose name is the predicted household_water_consumption_unit_ton\\nnext to the household_water_consumption_unit_ton.</p>\\n<pre><code>year population one_person_household two_person_household three_person_household four_person_household over_five_person_household household_water_consumption_unit_ton\\n2000 9987930 504192 524864 672410 992600 398643 736533992\\n2001 10000519 547177 552125 680530 974176 377162 737784659\\n2002 9958941 587392 578067 691449 965620 360548 720900652\\n2003 9951247 627630 604403 702167 955230 343748 728580066\\n2004 9961348 664390 628852 712084 944864 327397 736276327\\n2005 9938574 680954 667925 734488 925190 314516 731047705\\n2006 9967058 715994 687962 747218 908864 305547 730195428\\n2007 9978147 752891 710167 758626 888833 296230 723904433\\n2008 10021075 789796 732572 770074 868434 286999 720192072\\n2009 10047820 827183 754501 781133 848992 278113 720065222\\n2010 9998015 873490 780516 793852 819605 273818 721759866\\n2011 9981786 912149 806492 810168 805059 267042 717440010\\n2012 9946147 942756 826655 816961 780740 256541 718772932\\n2013 9900300 974726 849422 827169 759895 247346 716525298\\n2014 9885586 1009040 872664 838736 742139 238961 709886455\\n2015 9851890 1091753 913206 809929 704925 221480 713414487\\n2016 9754115 1120942 922630 809776 685456 211842 713140965\\n2017 9678697 1156019 939794 807432 664832 201401 706671587\\n2018 9610366 1202059 961845 802325 640190 190013 707667562\\n2019 9570573 1264616 986824 796531 616312 178206 702018748\\n2020 9531427 1347931 1015012 788615 601117 165243 726236303\\n2021 9416204 1442762 1040803 776135 578948 152862 723061075\\n2022 9326928 1470829 1060934 772956 558695 146904 697989260\\n2023 9254416 1495933 1083138 770345 538590 141003\\n2024 9188176 1518992 1104487 768119 519801 135338\\n2025 9127282 1539412 1126594 765091 500890 129830\\n2026 9069928 1555388 1147789 762289 483018 124543\\n2027 9015625 1568297 1165635 760139 467095 119791\\n2028 8963931 1579372 1183329 757447 451001 115112\\n2029 8914421 1587827 1199847 754807 435503 110598\\n2030 8867287 1594075 1215913 751805 420406 106351\\n2031 8822602 1598285 1231512 749002 406041 102357\\n2032 8780403 1603390 1243871 745645 392901 98754\\n2033 8740499 1607832 1255842 741746 380126 95369\\n2034 8702725 1611749 1267765 737249 367466 92070\\n2035 8666810 1615786 1278195 732706 355650 89005\\n2036 8630627 1617794 1288960 728200 344472 86150\\n2037 8592309 1621666 1298302 721994 333335 83217\\n2038 8551772 1624230 1307370 715364 322635 80473\\n2039 8508790 1625820 1315638 708488 312078 77784\\n2040 8463136 1626839 1322376 701038 301812 75173\\n2041 8414718 1624355 1329443 693890 291957 72606\\n2042 8363292 1620407 1335766 686966 281866 69790\\n2043 8308708 1614025 1340496 680521 272400 67142\\n2044 8250943 1605772 1344088 673708 263321 64596\\n2045 8190084 1597183 1345850 666053 254362 62098\\n2046 8126485 1585580 1347217 659039 245656 59640\\n2047 8060229 1572726 1347318 652280 237100 57257\\n2048 7991561 1560062 1344803 645322 229015 55049\\n2049 7920659 1547577 1340961 637729 220988 52794\\n2050 7847750 1536003 1335075 630048 213087 50547\\n</code></pre>\\n<p>my code\\nThe code was written in colab.\\nThe result of my code was not good.</p>\\n<pre><code>!python -m pip install tensorflow scikit_learn pandas\\nimport pandas\\ndf = pandas.read_excel('dataset.xlsx', index_col = 0)\\ndf\\n\\nfrom sklearn.model_selection import train_test_split\\n\\ntrain_df, test_df = train_test_split(df, test_size=0.1, random_state=5) # train data : test data = 9:1\\ntrain_df, valid_df = train_test_split(train_df, test_size=0.2, random_state=5)# train data : valid data = 8 :2\\n\\nsplit independent variable and dependent variable\\nX_train, y_train = train_df.iloc[:, :-1], train_df.iloc[:,-1]\\nX_valid, y_valid = valid_df.iloc[:, :-1], valid_df.iloc[:,-1]\\nX_test, y_test = test_df.iloc[:, :-1], test_df.iloc[:,-1]\\n\\nprint(X_train.shape, y_train.shape)\\nprint(X_valid.shape, y_valid.shape)\\nprint(X_test.shape, y_test.shape)\\n\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import InputLayer, Dense\\n\\nmodel = Sequential()\\nmodel.add(InputLayer(input_shape = (6,))) # independent variables 6\\nmodel.add(Dense(1))\\n\\nfrom tensorflow.keras.losses import MeanSquaredError\\nmodel.compile(loss=MeanSquaredError())\\n\\nmodel.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs = 1000, verbose = 0)\\n</code></pre>\\n\"},\n",
       "  {'tags': ['machine-learning', 'huggingface-trainer'],\n",
       "   'owner': {'account_id': 29972055,\n",
       "    'reputation': 1,\n",
       "    'user_id': 22969103,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/ACg8ocJAdFIfmn1NBMMaVUkrUqPDctgzpQp_rZ9lnv_QzHv_xA=k-s256',\n",
       "    'display_name': 'Akshat Jain',\n",
       "    'link': 'https://stackoverflow.com/users/22969103/akshat-jain'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 125,\n",
       "   'answer_count': 1,\n",
       "   'score': -1,\n",
       "   'last_activity_date': 1708963462,\n",
       "   'creation_date': 1700717392,\n",
       "   'last_edit_date': 1708963462,\n",
       "   'question_id': 77534525,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77534525/finetuning-a-llama2-model-gives-typeerror-argument-of-type-bool-is-not-iterab',\n",
       "   'title': 'Finetuning a llama2 model gives TypeError: argument of type &#39;bool&#39; is not iterable',\n",
       "   'body': '<p>I am trying to finetune llama 2 on a small dataset\\ntraining having 22 rows\\neval dataset containing 8</p>\\n<p>I am getting the following error with SFT Trainer\\n<a href=\"https://i.sstatic.net/knxIe.png\" rel=\"nofollow noreferrer\">SFT Trainer Error</a></p>\\n<p>I tried changing training params, PEFT config but nothing worked\\nI am expecting the model to train on these few samples and then want to use inference to see how it work.</p>\\n<p>On Adding <code>remove_unused_columns=False</code> to the training Arguments\\nI get</p>\\n<pre><code>TypeError: argument of type \\'bool\\' is not iterable\\n</code></pre>\\n<p>Training Args:</p>\\n<pre><code>TrainingArguments(\\n_n_gpu=2,\\nadafactor=False,\\nadam_beta1=0.9,\\nadam_beta2=0.999,\\nadam_epsilon=1e-08,\\nauto_find_batch_size=False,\\nbf16=False,\\nbf16_full_eval=False,\\ndata_seed=None,\\ndataloader_drop_last=False,\\ndataloader_num_workers=0,\\ndataloader_pin_memory=True,\\nddp_backend=None,\\nddp_broadcast_buffers=None,\\nddp_bucket_cap_mb=None,\\nddp_find_unused_parameters=None,\\nddp_timeout=1800,\\ndebug=[],\\ndeepspeed=None,\\ndisable_tqdm=False,\\ndispatch_batches=None,\\ndo_eval=True,\\ndo_predict=False,\\ndo_train=False,\\neval_accumulation_steps=None,\\neval_delay=0,\\neval_steps=100,\\nevaluation_strategy=IntervalStrategy.EPOCH,\\nfp16=False,\\nfp16_backend=auto,\\nfp16_full_eval=False,\\nfp16_opt_level=O1,\\nfsdp=[],\\nfsdp_config={\\'min_num_params\\': 0, \\'xla\\': False, \\'xla_fsdp_grad_ckpt\\': False},\\nfsdp_min_num_params=0,\\nfsdp_transformer_layer_cls_to_wrap=None,\\nfull_determinism=False,\\ngradient_accumulation_steps=4,\\ngradient_checkpointing=False,\\ngreater_is_better=None,\\ngroup_by_length=False,\\nhalf_precision_backend=auto,\\nhub_always_push=False,\\nhub_model_id=None,\\nhub_private_repo=False,\\nhub_strategy=HubStrategy.EVERY_SAVE,\\nhub_token=&lt;HUB_TOKEN&gt;,\\nignore_data_skip=False,\\ninclude_inputs_for_metrics=False,\\ninclude_tokens_per_second=False,\\njit_mode_eval=False,\\nlabel_names=None,\\nlabel_smoothing_factor=0.0,\\nlearning_rate=5e-05,\\nlength_column_name=length,\\nload_best_model_at_end=False,\\nlocal_rank=0,\\nlog_level=passive,\\nlog_level_replica=warning,\\nlog_on_each_node=True,\\nlogging_dir=test_trainer/runs/Nov23_10-56-58_frontierrhel8,\\nlogging_first_step=False,\\nlogging_nan_inf_filter=True,\\nlogging_steps=500,\\nlogging_strategy=IntervalStrategy.STEPS,\\nlr_scheduler_type=SchedulerType.LINEAR,\\nmax_grad_norm=1.0,\\nmax_steps=20,\\nmetric_for_best_model=None,\\nmp_parameters=,\\nno_cuda=False,\\nnum_train_epochs=2,\\noptim=OptimizerNames.ADAMW_TORCH,\\noptim_args=None,\\noutput_dir=test_trainer,\\noverwrite_output_dir=False,\\npast_index=-1,\\nper_device_eval_batch_size=4,\\nper_device_train_batch_size=4,\\nprediction_loss_only=False,\\npush_to_hub=False,\\npush_to_hub_model_id=None,\\npush_to_hub_organization=None,\\npush_to_hub_token=&lt;PUSH_TO_HUB_TOKEN&gt;,\\nray_scope=last,\\nremove_unused_columns=False,\\nreport_to=[],\\nresume_from_checkpoint=None,\\nrun_name=test_trainer,\\nsave_on_each_node=False,\\nsave_safetensors=False,\\nsave_steps=500,\\nsave_strategy=IntervalStrategy.STEPS,\\nsave_total_limit=None,\\nseed=42,\\nsharded_ddp=[],\\nskip_memory_metrics=True,\\ntf32=None,\\ntorch_compile=False,\\ntorch_compile_backend=None,\\ntorch_compile_mode=None,\\ntorchdynamo=None,\\ntpu_metrics_debug=False,\\ntpu_num_cores=None,\\nuse_cpu=False,\\nuse_ipex=False,\\nuse_legacy_prediction_loop=False,\\nuse_mps_device=False,\\nwarmup_ratio=0.0,\\nwarmup_steps=10,\\nweight_decay=0.0,\\n)\\n</code></pre>\\n'},\n",
       "  {'tags': ['python', 'tensorflow', 'machine-learning', 'keras', 'generator'],\n",
       "   'owner': {'account_id': 3873970,\n",
       "    'reputation': 22574,\n",
       "    'user_id': 4281353,\n",
       "    'user_type': 'registered',\n",
       "    'accept_rate': 69,\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/2c965e9fbb680e2527285a75b1712497?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'mon',\n",
       "    'link': 'https://stackoverflow.com/users/4281353/mon'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 297,\n",
       "   'accepted_answer_id': 77535234,\n",
       "   'answer_count': 1,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1700740372,\n",
       "   'creation_date': 1700714481,\n",
       "   'last_edit_date': 1700740372,\n",
       "   'question_id': 77534387,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77534387/keras-model-fit-does-not-work-correctly-with-generator-and-sparse-categorical-cr',\n",
       "   'title': 'keras.Model.fit does not work correctly with generator and sparse categorical crossentropy loss',\n",
       "   'body': '<p><code>tf.keras.Model.fit(x=generator)</code> does not work correctly with\\n<code>SparseCategoricalCrossentropy</code>/<code>sparce_categorical_crossentropy</code> loss function with a generator as training data. The same symptom reported in <a href=\"https://stackoverflow.com/questions/64910527/accuracy-killed-when-using-imagedatagenerator-tensorflow-keras#\">Accuracy killed when using ImageDataGenerator TensorFlow Keras</a>.</p>\\n<p>Please advise if this behaviour is as expected or please point out if code is incorrect.</p>\\n<p>Code excerpt. Entire code at the bottom.</p>\\n<pre><code># --------------------------------------------------------------------------------\\n# CIFAR 10\\n# --------------------------------------------------------------------------------\\nUSE_SPARCE_LABEL = True\\n\\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\\nx_train, x_test = x_train / 255.0, x_test / 255.0\\nx_train, x_validation, y_train, y_validation = train_test_split(\\n    x_train, y_train, test_size=0.2, random_state=42\\n)\\n\\n# One Hot Encoding the labels when USE_SPARCE_LABEL is False\\nif not USE_SPARCE_LABEL:\\n    y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\\n    y_validation = keras.utils.to_categorical(y_validation, NUM_CLASSES)\\n    y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)\\n\\n\\n# --------------------------------------------------------------------------------\\n# Model\\n# --------------------------------------------------------------------------------\\nmodel: Model = Model(\\n    inputs=inputs, outputs=outputs, name=&quot;cifar10&quot;\\n)\\n\\n# --------------------------------------------------------------------------------\\n# Compile\\n# --------------------------------------------------------------------------------\\nif USE_SPARCE_LABEL:\\n    loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)   # &lt;--- cause incorrect behavior\\nelse:\\n    loss_fn=tf.keras.losses.CategoricalCrossentropy(from_logits=False)\\n\\nlearning_rate = 1e-3\\nmodel.compile(\\n    optimizer=Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\\n    loss=loss_fn,     # &lt;---- sparse categorical causes the incorrect behavior\\n    metrics=[&quot;accuracy&quot;]\\n)\\n\\n# --------------------------------------------------------------------------------\\n# Train \\n# --------------------------------------------------------------------------------\\nbatch_size = 16\\nnumber_of_epochs = 10\\n\\ndef data_label_generator(x, y):\\n    def _f():\\n        index = 0\\n        length = len(x)\\n        try: \\n            while True:                \\n                yield x[index:index+batch_size], y[index:index+batch_size]\\n                index = (index + batch_size) % length\\n        except StopIteration:\\n            return\\n        \\n    return _f\\n\\nearlystop_callback = tf.keras.callbacks.EarlyStopping(\\n    patience=5,\\n    restore_best_weights=True,\\n    monitor=\\'val_accuracy\\'\\n)\\n\\nsteps_per_epoch = len(y_train) // batch_size\\nvalidation_steps = (len(y_validation) // batch_size) - 1  # To avoid run out of data for validation\\n\\nhistory = model.fit(\\n    x=data_label_generator(x_train, y_train)(),  # &lt;--- Generator\\n    batch_size=batch_size,\\n    epochs=number_of_epochs,\\n    verbose=1,\\n    validation_data=data_label_generator(x_validation, y_validation)(),\\n    shuffle=True,\\n    steps_per_epoch=steps_per_epoch,\\n    validation_steps=validation_steps,\\n    validation_batch_size=batch_size,\\n    callbacks=[\\n        earlystop_callback\\n    ]\\n)\\n</code></pre>\\n<h2>Symptom</h2>\\n<p>Using Sparse Index as the labels and <code>SparseCategoricalCrossentropy</code> as the loss function (<code>USE_SPARSE_LABEL=True</code>). The accuracy values got unstable and low, causing early stop.</p>\\n<pre><code>2500/2500 [...] - 24s 8ms/step - loss: 1.4824 - accuracy: 0.0998 - val_loss: 1.1893 - val_accuracy: 0.1003\\nEpoch 2/10\\n2500/2500 [...] - 21s 8ms/step - loss: 1.0730 - accuracy: 0.1010 - val_loss: 0.8896 - val_accuracy: 0.0832\\nEpoch 3/10\\n2500/2500 [...] - 20s 8ms/step - loss: 0.9272 - accuracy: 0.1016 - val_loss: 0.9150 - val_accuracy: 0.0720\\nEpoch 4/10\\n2500/2500 [...] - 20s 8ms/step - loss: 0.7987 - accuracy: 0.1019 - val_loss: 0.8087 - val_accuracy: 0.0864\\nEpoch 5/10\\n2500/2500 [...] - 20s 8ms/step - loss: 0.7081 - accuracy: 0.1012 - val_loss: 0.8707 - val_accuracy: 0.0928\\nEpoch 6/10\\n2500/2500 [...] - 21s 8ms/step - loss: 0.6056 - accuracy: 0.1019 - val_loss: 0.7688 - val_accuracy: 0.0851\\n</code></pre>\\n<p>Using One Hot Encoding as the labels and <code>CategoricalCrossentropy</code> as the loss function (<code>USE_SPARSE_LABEL=True</code>). Work as expected.</p>\\n<pre><code>2500/2500 [...] - 24s 8ms/step - loss: 1.4146 - accuracy: 0.4997 - val_loss: 1.0906 - val_accuracy: 0.6105\\nEpoch 2/10\\n2500/2500 [...] - 21s 9ms/step - loss: 1.0306 - accuracy: 0.6375 - val_loss: 0.9779 - val_accuracy: 0.6532\\nEpoch 3/10\\n2500/2500 [...] - 22s 9ms/step - loss: 0.8780 - accuracy: 0.6925 - val_loss: 0.8194 - val_accuracy: 0.7127\\nEpoch 4/10\\n2500/2500 [...] - 21s 8ms/step - loss: 0.7641 - accuracy: 0.7315 - val_loss: 0.9330 - val_accuracy: 0.7014\\nEpoch 5/10\\n2500/2500 [...] - 21s 8ms/step - loss: 0.6797 - accuracy: 0.7614 - val_loss: 0.7908 - val_accuracy: 0.7311\\nEpoch 6/10\\n2500/2500 [...] - 21s 9ms/step - loss: 0.6182 - accuracy: 0.7841 - val_loss: 0.7371 - val_accuracy: 0.7533\\nEpoch 7/10\\n2500/2500 [...] - 21s 9ms/step - loss: 0.4981 - accuracy: 0.8217 - val_loss: 0.8221 - val_accuracy: 0.7373\\nEpoch 8/10\\n2500/2500 [...] - 22s 9ms/step - loss: 0.4363 - accuracy: 0.8437 - val_loss: 0.7865 - val_accuracy: 0.7525\\nEpoch 9/10\\n2500/2500 [...] - 23s 9ms/step - loss: 0.3962 - accuracy: 0.8596 - val_loss: 0.8198 - val_accuracy: 0.7505\\nEpoch 10/10\\n2500/2500 [...] - 22s 9ms/step - loss: 0.3463 - accuracy: 0.8776 - val_loss: 0.8472 - val_accuracy: 0.7512\\n</code></pre>\\n<h2>Code</h2>\\n<pre><code>import numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nfrom keras import (\\n    __version__\\n)\\n\\n\\nfrom keras.layers import (\\n    Layer,\\n    Normalization,\\n    Conv2D,\\n    MaxPooling2D,\\n    BatchNormalization,\\n    Dense,\\n    Flatten,\\n    Dropout,\\n    Reshape,\\n    Activation,\\n    ReLU,\\n    LeakyReLU,\\n)\\nfrom keras.models import (\\n    Model,\\n)\\nfrom keras.layers import (\\n    Layer\\n)\\nfrom keras.optimizers import (\\n    Adam\\n)\\nfrom sklearn.model_selection import train_test_split\\n\\nprint(&quot;TensorFlow version: {}&quot;.format(tf.__version__))\\ntf.keras.__version__ = __version__\\nprint(&quot;Keras version: {}&quot;.format(tf.keras.__version__))\\n\\n# --------------------------------------------------------------------------------\\n# CIFAR 10\\n# --------------------------------------------------------------------------------\\nNUM_CLASSES = 10\\nINPUT_SHAPE = (32, 32, 3)\\nUSE_SPARCE_LABEL = False   # Setting False make it work as expected\\n\\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\\nx_train, x_test = x_train / 255.0, x_test / 255.0\\nx_train, x_validation, y_train, y_validation = train_test_split(\\n    x_train, y_train, test_size=0.2, random_state=42\\n)\\n\\n# One Hot Encoding the labels\\nif not USE_SPARCE_LABEL:\\n    y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\\n    y_validation = keras.utils.to_categorical(y_validation, NUM_CLASSES)\\n    y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)\\n\\n# --------------------------------------------------------------------------------\\n# Model\\n# --------------------------------------------------------------------------------\\ninputs = tf.keras.Input(\\n    name=\\'image\\',\\n    shape=INPUT_SHAPE,\\n    dtype=tf.float32\\n) \\n\\nx = Conv2D(                                           \\n    filters=32, \\n    kernel_size=(3, 3), \\n    strides=(1, 1), \\n    padding=&quot;same&quot;,\\n    activation=\\'relu\\', \\n    input_shape=INPUT_SHAPE\\n)(inputs)\\nx = BatchNormalization()(x)\\nx = Conv2D(                                           \\n    filters=64, \\n    kernel_size=(3, 3), \\n    strides=(1, 1), \\n    padding=&quot;same&quot;,\\n    activation=\\'relu\\'\\n)(x)\\nx = MaxPooling2D(                                     \\n    pool_size=(2, 2)\\n)(x)\\nx = Dropout(0.20)(x)\\n\\nx = Conv2D(                                           \\n    filters=128, \\n    kernel_size=(3, 3), \\n    strides=(1, 1), \\n    padding=&quot;same&quot;,\\n    activation=\\'relu\\'\\n)(x)\\nx = BatchNormalization()(x)\\nx = MaxPooling2D(                                     \\n    pool_size=(2, 2)\\n)(x)\\nx = Dropout(0.20)(x)\\n\\nx = Flatten()(x)\\nx = Dense(300, activation=&quot;relu&quot;)(x)\\nx = BatchNormalization()(x)\\nx = Dropout(0.20)(x)\\nx = Dense(200, activation=&quot;relu&quot;)(x)\\noutputs = Dense(NUM_CLASSES, activation=&quot;softmax&quot;)(x)\\n\\nmodel: Model = Model(\\n    inputs=inputs, outputs=outputs, name=&quot;cifar10&quot;\\n)\\n\\n# --------------------------------------------------------------------------------\\n# Compile\\n# --------------------------------------------------------------------------------\\nlearning_rate = 1e-3\\n\\nif USE_SPARCE_LABEL:\\n    loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\\nelse:\\n    loss_fn=tf.keras.losses.CategoricalCrossentropy(from_logits=False)\\n\\nmodel.compile(\\n    optimizer=Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\\n    loss=loss_fn,\\n    metrics=[&quot;accuracy&quot;]\\n)\\nmodel.summary()\\n\\n\\n# --------------------------------------------------------------------------------\\n# Train\\n# --------------------------------------------------------------------------------\\nbatch_size = 16\\nnumber_of_epochs = 10\\n\\ndef data_label_generator(x, y):\\n    def _f():\\n        index = 0\\n        length = len(x)\\n        try: \\n            while True:                \\n                yield x[index:index+batch_size], y[index:index+batch_size]\\n                index = (index + batch_size) % length\\n        except StopIteration:\\n            return\\n        \\n    return _f\\n\\nearlystop_callback = tf.keras.callbacks.EarlyStopping(\\n    patience=5,\\n    restore_best_weights=True,\\n    monitor=\\'val_accuracy\\'\\n)\\n\\nsteps_per_epoch = len(y_train) // batch_size\\nvalidation_steps = (len(y_validation) // batch_size) - 1  # -1 to avoid run out of data for validation\\n\\nhistory = model.fit(\\n    x=data_label_generator(x_train, y_train)(),\\n    batch_size=batch_size,\\n    epochs=number_of_epochs,\\n    verbose=1,\\n    validation_data=data_label_generator(x_validation, y_validation)(),\\n    shuffle=True,\\n    steps_per_epoch=steps_per_epoch,\\n    validation_steps=validation_steps,\\n    validation_batch_size=batch_size,\\n    callbacks=[\\n        earlystop_callback\\n    ]\\n)\\n</code></pre>\\n<h2>Environment</h2>\\n<pre><code>TensorFlow version: 2.14.1\\nKeras version: 2.14.0\\nPython 3.10.12\\nUbuntu 22.04LTS\\n</code></pre>\\n<hr />\\n<h2>Workaround</h2>\\n<p>The answer by <a href=\"https://stackoverflow.com/users/9215780/innat\">innat</a> worked.</p>\\n<pre><code>model.compile(\\n    optimizer=Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08),\\n    #metrics=[&quot;accuracy&quot;]\\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\\'accuracy\\')])\\nmodel.summary()\\n</code></pre>\\n'},\n",
       "  {'tags': ['python', 'machine-learning', 'lstm', 'cross-validation', 'auc'],\n",
       "   'owner': {'account_id': 16926669,\n",
       "    'reputation': 2342,\n",
       "    'user_id': 12242085,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/d073a2a2f38d392c678ac04b1b6c3c18?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'dingaro',\n",
       "    'link': 'https://stackoverflow.com/users/12242085/dingaro'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 190,\n",
       "   'answer_count': 0,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1700740595,\n",
       "   'creation_date': 1700710832,\n",
       "   'last_edit_date': 1700740595,\n",
       "   'question_id': 77534243,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77534243/how-to-combine-cross-validation-and-early-stopping-round-in-lstm-model',\n",
       "   'title': 'How to combine Cross Validation and Early Stopping Round in LSTM model?',\n",
       "   'body': \"<p>I would like to combine Cross Validation and Early Stopping Round in LSTM model in Python. I would like to make cross validation on 3 folds and stop learning my LSTM model if for 3 consecutive epchos there is no improvement in the mean AUC of the 3 folds from the cross-validation.</p>\\n<p>In this way, I would like to build a model with the highest possible mean AUC of the 3 folds from cross-validation, which has not improved for 3 epchos in a row.</p>\\n<p>My current code is:</p>\\n<pre><code>import numpy as np\\nfrom keras.models import Sequential\\nfrom keras.layers import LSTM, Dense\\nfrom sklearn.model_selection import KFold\\nfrom sklearn.metrics import roc_auc_score\\nfrom keras.callbacks import EarlyStopping\\n\\nmodel = Sequential()\\nmodel.add(LSTM(units=32, input_shape=(X_train.shape[1], X_train.shape[2]), dropout=0.3))\\nmodel.add(Dense(1, activation='sigmoid'))\\n\\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['AUC'])\\n\\nkfold = KFold(n_splits=3, shuffle=True)\\n\\nearly_stopping = EarlyStopping(monitor='val_auc', patience=3, mode='max')\\n\\nfor train_idx, val_idx in kfold.split(X_train):\\n    X_train_fold, X_val_fold = X_train[train_idx], X_train[val_idx]\\n    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\\n\\n    auc_scores = []\\n\\n    for epoch in range(100):\\n        model.fit(X_train_fold, y_train_fold, epochs=1, validation_data=(X_val_fold, y_val_fold), callbacks=[early_stopping], verbose=0)\\n\\n        y_pred = model.predict(X_val_fold)\\n        auc = roc_auc_score(y_val_fold, y_pred)\\n        auc_scores.append(auc)\\n\\n        print(&quot;Epoch:&quot;, epoch+1, &quot;Avg AUC from 3 folds:&quot;, np.mean(auc_scores))\\n\\n    print(&quot;Model stopped learn after&quot;, early_stopping.stopped_epoch)\\n</code></pre>\\n<p>How can I modify my code to achieve that ?</p>\\n\"},\n",
       "  {'tags': ['python', 'machine-learning', 'pipeline', 'data-transform'],\n",
       "   'owner': {'account_id': 5995917,\n",
       "    'reputation': 165,\n",
       "    'user_id': 4710562,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh6.googleusercontent.com/-RubYlMFNCeU/AAAAAAAAAAI/AAAAAAAAAHE/ifRDioESiCk/photo.jpg?sz=256',\n",
       "    'display_name': 'Md. Ehsanul Haque Kanan',\n",
       "    'link': 'https://stackoverflow.com/users/4710562/md-ehsanul-haque-kanan'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 37,\n",
       "   'accepted_answer_id': 77533817,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700700521,\n",
       "   'creation_date': 1700699381,\n",
       "   'question_id': 77533782,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77533782/data-transformation-issue-on-end-to-end-ml-project-convert-to-minutes-takes',\n",
       "   'title': 'Data Transformation Issue on End-to-End ML Project - convert_to_minutes() Takes 1 Positional Argument But 2 Were Given',\n",
       "   'body': '<p>I am following the process shown on <a href=\"https://youtu.be/pxk1Fr33-L4?si=zqJ_Ice6YT8pLYzJ\" rel=\"nofollow noreferrer\">Wine Quality Prediction End-to-End ML Project</a> on Krish Naik\\'s YouTube channel to do a Flight Fare Prediction Project.</p>\\n<p>I run this cell of data transformation pipeline on <a href=\"https://github.com/MdEhsanulHaqueKanan/Flight-Fare-Prediction-End-to-End-ML-Project/blob/main/research/03_data_transformation.ipynb\" rel=\"nofollow noreferrer\">03_data_transformation.ipynb</a>:</p>\\n<pre><code>try:\\n    config = ConfigurationManager()\\n    data_transformation_config = config.get_data_transformation_config()\\n    data_transformation = DataTransformation(config=data_transformation_config)\\n    # data_transformation.train_test_spliting()\\n    # New Line\\n    data_transformation.initiate_data_transformation()\\nexcept Exception as e:\\n    raise e\\n</code></pre>\\n<p>I get this error:</p>\\n<pre><code>TypeError: convert_to_minutes() takes 1 positional argument but 2 were given\\n</code></pre>\\n<p>Here is the traceback:</p>\\n<pre><code>TypeError                                 Traceback (most recent call last)\\ng:\\\\Machine_Learning_Projects\\\\iNeuron internship\\\\Flight-Fare-Prediction-End-to-End-ML-Project\\\\research\\\\03_data_transformation.ipynb Cell 10 line 9\\n      &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=6\\'&gt;7&lt;/a&gt;     data_transformation.initiate_data_transformation()\\n      &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=7\\'&gt;8&lt;/a&gt; except Exception as e:\\n----&gt; &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=8\\'&gt;9&lt;/a&gt;     raise e\\n\\ng:\\\\Machine_Learning_Projects\\\\iNeuron internship\\\\Flight-Fare-Prediction-End-to-End-ML-Project\\\\research\\\\03_data_transformation.ipynb Cell 10 line 7\\n      &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=3\\'&gt;4&lt;/a&gt;     data_transformation = DataTransformation(config=data_transformation_config)\\n      &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=4\\'&gt;5&lt;/a&gt;     # data_transformation.train_test_spliting()\\n      &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=5\\'&gt;6&lt;/a&gt;     # New Line\\n----&gt; &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=6\\'&gt;7&lt;/a&gt;     data_transformation.initiate_data_transformation()\\n      &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=7\\'&gt;8&lt;/a&gt; except Exception as e:\\n      &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=8\\'&gt;9&lt;/a&gt;     raise e\\n\\ng:\\\\Machine_Learning_Projects\\\\iNeuron internship\\\\Flight-Fare-Prediction-End-to-End-ML-Project\\\\research\\\\03_data_transformation.ipynb Cell 10 line 6\\n     &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=55\\'&gt;56&lt;/a&gt; df = pd.concat([df, df_airline, df_source, df_dest], axis = 1)\\n     &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=57\\'&gt;58&lt;/a&gt; ## handling duration column\\n     &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=58\\'&gt;59&lt;/a&gt; # df[\\'duration\\'] = df[\\'Duration\\'].apply(convert_to_minutes)\\n     &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=59\\'&gt;60&lt;/a&gt; # New Line Added\\n---&gt; &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=60\\'&gt;61&lt;/a&gt; df[\\'duration\\'] = df[\\'Duration\\'].apply(self.convert_to_minutes)\\n     &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=61\\'&gt;62&lt;/a&gt; upper_time_limit = df.duration.mean() + 1.5 * df.duration.std()\\n     &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=62\\'&gt;63&lt;/a&gt; df[\\'duration\\'] = df[\\'duration\\'].clip(upper = upper_time_limit)\\n\\nFile c:\\\\Users\\\\2021\\\\.conda\\\\envs\\\\flightfareprediction\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\series.py:4630, in Series.apply(self, func, convert_dtype, args, **kwargs)\\n   4520 def apply(\\n   4521     self,\\n   4522     func: AggFuncType,\\n   (...)\\n   4525     **kwargs,\\n   4526 ) -&gt; DataFrame | Series:\\n   4527     &quot;&quot;&quot;\\n   4528     Invoke function on values of Series.\\n   4529 \\n   (...)\\n   4628     dtype: float64\\n   4629     &quot;&quot;&quot;\\n-&gt; 4630     return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\\n\\nFile c:\\\\Users\\\\2021\\\\.conda\\\\envs\\\\flightfareprediction\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\apply.py:1025, in SeriesApply.apply(self)\\n   1022     return self.apply_str()\\n   1024 # self.f is Callable\\n-&gt; 1025 return self.apply_standard()\\n\\nFile c:\\\\Users\\\\2021\\\\.conda\\\\envs\\\\flightfareprediction\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\apply.py:1076, in SeriesApply.apply_standard(self)\\n   1074     else:\\n   1075         values = obj.astype(object)._values\\n-&gt; 1076         mapped = lib.map_infer(\\n   1077             values,\\n   1078             f,\\n   1079             convert=self.convert_dtype,\\n   1080         )\\n   1082 if len(mapped) and isinstance(mapped[0], ABCSeries):\\n   1083     # GH#43986 Need to do list(mapped) in order to get treated as nested\\n   1084     #  See also GH#25959 regarding EA support\\n   1085     return obj._constructor_expanddim(list(mapped), index=obj.index)\\n\\nFile c:\\\\Users\\\\2021\\\\.conda\\\\envs\\\\flightfareprediction\\\\lib\\\\site-packages\\\\pandas\\\\_libs\\\\lib.pyx:2834, in pandas._libs.lib.map_infer()\\n\\nTypeError: convert_to_minutes() takes 1 positional argument but 2 were given\\n</code></pre>\\n<p>Here is the code of data transformation cell, which contains <code>convert_to_minutes()</code> function.</p>\\n<pre><code>class DataTransformation:\\n\\n    # New Function Added\\n    # https://github.com/yash1314/Flight-Price-Prediction/blob/main/src/utils.py\\n    def convert_to_minutes(duration):\\n        try:\\n            hours, minute = 0, 0\\n            for i in duration.split():\\n                if \\'h\\' in i:\\n                    hours = int(i[:-1])\\n                elif \\'m\\' in i:\\n                    minute = int(i[:-1])\\n            return hours * 60 + minute\\n        except :\\n            return None \\n\\n    def __init__(self, config: DataTransformationConfig):\\n        self.config = config\\n\\n    \\n    ## Note: You can add different data transformation techniques such as Scaler, PCA and all\\n    #You can perform all kinds of EDA in ML cycle here before passing this data to the model\\n\\n    # I am only adding train_test_spliting cz this data is already cleaned up\\n\\n    # New Code Added Start\\n    def initiate_data_transformation(self):\\n        ## reading the data\\n        # df = pd.read_csv(self.config.data_path)\\n        # New Line\\n        df = pd.read_excel(self.config.data_path)\\n\\n        logger.info(\\'Read data completed\\')\\n        logger.info(f\\'df dataframe head: \\\\n{df.head().to_string()}\\')\\n\\n        ## dropping null values\\n        df.dropna(inplace = True)\\n\\n        ## Date of journey column transformation\\n        df[\\'journey_date\\'] = pd.to_datetime(df[\\'Date_of_Journey\\'], format =&quot;%d/%m/%Y&quot;).dt.day\\n        df[\\'journey_month\\'] = pd.to_datetime(df[\\'Date_of_Journey\\'], format =&quot;%d/%m/%Y&quot;).dt.month\\n\\n        ## encoding total stops.\\n        df.replace({\\'Total_Stops\\': {\\'non-stop\\' : 0, \\'1 stop\\': 1, \\'2 stops\\': 2, \\'3 stops\\': 3, \\'4 stops\\': 4}}, inplace = True)\\n\\n        ## ecoding airline, source, and destination\\n        df_airline = pd.get_dummies(df[\\'Airline\\'], dtype=int)\\n        df_source = pd.get_dummies(df[\\'Source\\'],  dtype=int)\\n        df_dest = pd.get_dummies(df[\\'Destination\\'], dtype=int)\\n\\n        ## dropping first columns of each categorical variables.\\n        df_airline.drop(\\'Trujet\\', axis = 1, inplace = True)\\n        df_source.drop(\\'Banglore\\', axis = 1, inplace = True)\\n        df_dest.drop(\\'Banglore\\', axis = 1, inplace = True)\\n\\n        df = pd.concat([df, df_airline, df_source, df_dest], axis = 1)\\n       \\n        ## handling duration column\\n        # df[\\'duration\\'] = df[\\'Duration\\'].apply(convert_to_minutes)\\n        # New Line Added\\n        df[\\'duration\\'] = df[\\'Duration\\'].apply(self.convert_to_minutes)\\n        upper_time_limit = df.duration.mean() + 1.5 * df.duration.std()\\n        df[\\'duration\\'] = df[\\'duration\\'].clip(upper = upper_time_limit)\\n\\n        ## encodign duration column\\n        bins = [0, 120, 360, 1440]  # custom bin intervals for \\'Short,\\' \\'Medium,\\' and \\'Long\\'\\n        labels = [\\'Short\\', \\'Medium\\', \\'Long\\'] # creating labels for encoding\\n\\n        df[\\'duration\\'] = pd.cut(df[\\'duration\\'], bins=bins, labels=labels)\\n        df.replace({\\'duration\\': {\\'Short\\':1, \\'Medium\\':2, \\'Long\\': 3}}, inplace = True)\\n        \\n        ## dropping the columns\\n        cols_to_drop = cols_to_drop = [\\'Airline\\', \\'Date_of_Journey\\', \\'Source\\', \\'Destination\\', \\'Route\\', \\'Dep_Time\\', \\'Arrival_Time\\', \\'Duration\\', \\'Additional_Info\\', \\'Delhi\\', \\'Kolkata\\']\\n\\n        df.drop(cols_to_drop, axis = 1, inplace = True)\\n\\n        logger.info(\\'df data transformation completed\\')\\n        logger.info(f\\' transformed df data head: \\\\n{df.head().to_string()}\\')\\n\\n        # df.to_csv(self.data_transformation_config.transformed_data_file_path, index = False, header= True)\\n        # New Line\\n        df.to_excel(self.data_transformation_config.transformed_data_file_path, index = False, header= True)\\n        logger.info(&quot;transformed data is stored&quot;)\\n        df.head(1)\\n        ## splitting the data into training and target data\\n        X = df.drop(\\'Price\\', axis = 1)\\n        y = df[\\'Price\\']\\n        \\n        ## accessing the feature importance.\\n        select = ExtraTreesRegressor()\\n        select.fit(X, y)\\n\\n        # plt.figure(figsize=(12, 8))\\n        # fig_importances = pd.Series(select.feature_importances_, index=X.columns)\\n        # fig_importances.nlargest(20).plot(kind=\\'barh\\')\\n    \\n        # ## specify the path to the &quot;visuals&quot; folder using os.path.join\\n        # visuals_folder = \\'visuals\\'\\n        # if not os.path.exists(visuals_folder):\\n        #     os.makedirs(visuals_folder)\\n\\n        # ## save the plot in the visuals folder\\n        # plt.savefig(os.path.join(visuals_folder, \\'feature_importance_plot.png\\'))\\n        # logger.info(\\'feature imp figure saving is successful\\')\\n\\n        ## further Splitting the data.\\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, shuffle = True) \\n        logger.info(\\'final splitting the data is successful\\')\\n        \\n\\n        ## returning splitted data and data_path.\\n        return (\\n            X_train, \\n            X_test, \\n            y_train, \\n            y_test,\\n            self.data_transformation_config.transformed_data_file_path\\n        )    \\n</code></pre>\\n<p>Here is <a href=\"https://github.com/MdEhsanulHaqueKanan/Flight-Fare-Prediction-End-to-End-ML-Project/blob/main/research/03_data_transformation.ipynb\" rel=\"nofollow noreferrer\">my file in GitHub</a>.</p>\\n<p>My file encoding is UTF-8</p>\\n<p>Would you please help me to fix this issue?</p>\\n'},\n",
       "  {'tags': ['python', 'machine-learning', 'pytorch', 'dataset'],\n",
       "   'owner': {'account_id': 29517376,\n",
       "    'reputation': 11,\n",
       "    'user_id': 22621040,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/ca0af6bc25ad20a683c1b26676f5991c?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'Pretbc',\n",
       "    'link': 'https://stackoverflow.com/users/22621040/pretbc'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 73,\n",
       "   'answer_count': 1,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1700676589,\n",
       "   'creation_date': 1700673824,\n",
       "   'last_edit_date': 1700676589,\n",
       "   'question_id': 77532011,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77532011/pytorch-emotion-model-labels-normalization-approach',\n",
       "   'title': 'PyTorch emotion model labels normalization approach',\n",
       "   'body': '<p>Im trying to build ML emotion model in PyTorch.</p>\\n<p>I have emotion labels taken from CMU-MOSEI dataset in data frame as below:</p>\\n<div class=\"s-table-container\">\\n<table class=\"s-table\">\\n<thead>\\n<tr>\\n<th>happy</th>\\n<th>sad</th>\\n<th>anger</th>\\n<th>surprise</th>\\n<th>disgust</th>\\n<th>fear</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>1.33</td>\\n<td>0.0</td>\\n<td>0.0</td>\\n<td>0.0</td>\\n<td>0.0</td>\\n<td>0.0</td>\\n</tr>\\n<tr>\\n<td>2.0</td>\\n<td>0.0</td>\\n<td>0.0</td>\\n<td>0.33</td>\\n<td>0.0</td>\\n<td>0.0</td>\\n</tr>\\n<tr>\\n<td>0.0</td>\\n<td>0.0</td>\\n<td>1.33</td>\\n<td>0.33</td>\\n<td>2.0</td>\\n<td>0.0</td>\\n</tr>\\n</tbody>\\n</table>\\n</div>\\n<p>Each emotion can be in range between <code>0.0 -&gt; 3.0</code></p>\\n<p>Question is:</p>\\n<h3>How to perform normalization to range <code>0 -&gt; 1</code> on this data:</h3>\\n<h4>1. Normalize each column by:</h4>\\n<pre><code>from sklearn.preprocessing import minmax_scale\\n\\nfor emo in [\\'happy\\', \\'sad\\', \\'anger\\', \\'surprise\\', \\'disgust\\', \\'fear\\']:\\n    mosei[emo] = minmax_scale(mosei[emo])\\n</code></pre>\\n<p>which give me ie:</p>\\n<p><code>1.33,0.0,0.0,0.0,0.0,0.0</code> -&gt; <code>0.44,0.0,0.0,0.0,0.0,0.0</code></p>\\n<p><code>2.0,0.0,0.0,0.33,0.0,0.0</code> -&gt; <code>0.67,0.0,0.0,0.11,0.0,0.0</code></p>\\n<p><code>0.0,0.0,1.33,0.33,2.0,0.0</code> -&gt; <code>0.0,0.0,0.44,0.11,0.67,0.0</code></p>\\n<p>but for last example <code>sum() &gt; 1</code></p>\\n<h4>2. Normalize each column and in data loader perform <code>softmax()</code></h4>\\n<pre><code>&gt;&gt;&gt; F.softmax(torch.tensor([0.44,0.0,0.0,0.0,0.0,0.0]), dim=0)\\ntensor([0.2370, 0.1526, 0.1526, 0.1526, 0.1526, 0.1526])\\n\\n\\n&gt;&gt;&gt; F.softmax(torch.tensor([0.0,0.0,0.44,0.11,0.67,0.0]), dim=0)\\ntensor([0.1312, 0.1312, 0.2037, 0.1464, 0.2564, 0.1312])\\n</code></pre>\\n<h4>3. Perform normalization by each row instead of columns</h4>\\n<pre><code>&gt;&gt;&gt; minmax_scale([1.33,0.0,0.0,0.0,0.0,0.0])\\narray([1., 0., 0., 0., 0., 0.])\\n\\n\\n&gt;&gt;&gt; minmax_scale([0.0,0.0,1.33,0.33,2.0,0.0])\\narray([0.   , 0.   , 0.665, 0.165, 1.   , 0.   ])\\n</code></pre>\\n<p>but again last example <code>sum() &gt; 1</code></p>\\n<ul>\\n<li>maybe softmax again?</li>\\n</ul>\\n<pre><code>F.softmax(torch.tensor([0.   , 0.   , 0.665, 0.165, 1.   , 0.   ]), dim=0)\\ntensor([0.1131, 0.1131, 0.2199, 0.1334, 0.3074, 0.1131])\\n</code></pre>\\n<p>or maybe there could be different/better normalization approach ?</p>\\n'},\n",
       "  {'tags': ['python', 'machine-learning', 'scikit-learn'],\n",
       "   'owner': {'account_id': 29966826,\n",
       "    'reputation': 1,\n",
       "    'user_id': 22965221,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/ACg8ocJXAUVyQsxwvlYlK16OxaD8ryBsqpgz6s0eBnh3eK0cXA=k-s256',\n",
       "    'display_name': 'ketchup',\n",
       "    'link': 'https://stackoverflow.com/users/22965221/ketchup'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 103,\n",
       "   'accepted_answer_id': 77530633,\n",
       "   'answer_count': 1,\n",
       "   'score': -1,\n",
       "   'last_activity_date': 1700676791,\n",
       "   'creation_date': 1700661697,\n",
       "   'last_edit_date': 1700676791,\n",
       "   'question_id': 77530586,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77530586/valueerror-found-input-variables-with-inconsistent-numbers-of-samples-915-22',\n",
       "   'title': 'ValueError: Found input variables with inconsistent numbers of samples: [915, 229] in easy mashine learning sample',\n",
       "   'body': \"<pre><code>import pandas as pd\\nfrom sklearn.metrics import mean_squared_error, r2_score\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\n\\ndf=pd.read_csv('https://raw.githubusercontent.com/dataprofessor/data/master/delaney_solubility_with_descriptors.csv')\\n\\nX = df.drop('logS', axis=1)\\ny = df['logS']\\n\\nX_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\\n\\nlr = LinearRegression()\\nlr.fit(X_train, y_train)\\n</code></pre>\\n<p>I have a code that tries to predict the logS values. This code is not mine, but from the guide. There are no errors there. So what could be the problem?</p>\\n<pre><code>full error message here:\\nTraceback (most recent call last):\\n  File &quot;D:\\\\Python\\\\AI\\\\test\\\\main.py&quot;, line 14, in &lt;module&gt;\\n    lr.fit(X_train, y_train)\\n  File &quot;D:\\\\Python\\\\AI\\\\test\\\\venv\\\\lib\\\\site-packages\\\\sklearn\\\\base.py&quot;, line 1152, in wrapper\\n    return fit_method(estimator, *args, **kwargs)\\n  File &quot;D:\\\\Python\\\\AI\\\\test\\\\venv\\\\lib\\\\site-packages\\\\sklearn\\\\linear_model\\\\_base.py&quot;, line 678, in fit\\n    X, y = self._validate_data(\\n  File &quot;D:\\\\Python\\\\AI\\\\test\\\\venv\\\\lib\\\\site-packages\\\\sklearn\\\\base.py&quot;, line 622, in _validate_data\\n    X, y = check_X_y(X, y, **check_params)\\n  File &quot;D:\\\\Python\\\\AI\\\\test\\\\venv\\\\lib\\\\site-packages\\\\sklearn\\\\utils\\\\validation.py&quot;, line 1164, in check_X_y\\n    check_consistent_length(X, y)\\n  File &quot;D:\\\\Python\\\\AI\\\\test\\\\venv\\\\lib\\\\site-packages\\\\sklearn\\\\utils\\\\validation.py&quot;, line 407, in check_consistent_length\\n    raise ValueError(\\nValueError: Found input variables with inconsistent numbers of samples: [915, 229]\\n</code></pre>\\n<p>I changed the data-file, and values in ValueError was change too.\\nI change test_size value, and values in ValueError was change\\nFor test_size = 0.4 :</p>\\n<pre><code>ValueError: Found input variables with inconsistent numbers of samples: [686, 458]\\n</code></pre>\\n\"},\n",
       "  {'tags': ['python', 'machine-learning', 'pipeline', 'data-transform'],\n",
       "   'owner': {'account_id': 5995917,\n",
       "    'reputation': 165,\n",
       "    'user_id': 4710562,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh6.googleusercontent.com/-RubYlMFNCeU/AAAAAAAAAAI/AAAAAAAAAHE/ifRDioESiCk/photo.jpg?sz=256',\n",
       "    'display_name': 'Md. Ehsanul Haque Kanan',\n",
       "    'link': 'https://stackoverflow.com/users/4710562/md-ehsanul-haque-kanan'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 49,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700677051,\n",
       "   'creation_date': 1700659672,\n",
       "   'last_edit_date': 1700677051,\n",
       "   'question_id': 77530336,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77530336/data-transformation-pipeline-issue-on-end-to-end-machine-learning-project',\n",
       "   'title': 'Data Transformation Pipeline Issue on End-to-End Machine Learning Project',\n",
       "   'body': '<p>I am following the process shown on <a href=\"https://youtu.be/pxk1Fr33-L4?si=zqJ_Ice6YT8pLYzJ\" rel=\"nofollow noreferrer\">Wine Quality Prediction End-to-End ML Project</a> on Krish Naik\\'s YouTube channel to do a Flight Fare Prediction Project.</p>\\n<p>I am facing an issue with Data Transformation Pipeline.</p>\\n<p>I run this cell of data transformation pipeline on <a href=\"https://github.com/MdEhsanulHaqueKanan/Flight-Fare-Prediction-End-to-End-ML-Project/blob/main/research/03_data_transformation.ipynb\" rel=\"nofollow noreferrer\">03_data_transformation.ipynb</a>:</p>\\n<pre><code>try:\\n    config = ConfigurationManager()\\n    data_transformation_config = config.get_data_transformation_config()\\n    data_transformation = DataTransformation(config=data_transformation_config)\\n    # data_transformation.train_test_spliting()\\n    # New Line\\n    data_transformation.initiate_data_transformation()\\nexcept Exception as e:\\n    raise e\\n</code></pre>\\n<p>I get this error:</p>\\n<pre><code>UnicodeDecodeError: \\'utf-8\\' codec can\\'t decode byte 0xa1 in position 14: invalid \\nstart byte\\n</code></pre>\\n<p>Here is the traceback:</p>\\n<pre><code>[2023-11-21 11:13:08,772: INFO: common: yaml file: config\\\\config.yaml loaded successfully]\\n[2023-11-21 11:13:08,776: INFO: common: yaml file: params.yaml loaded successfully]\\n[2023-11-21 11:13:08,783: INFO: common: yaml file: schema.yaml loaded successfully]\\n[2023-11-21 11:13:08,784: INFO: common: created directory at: artifacts]\\n[2023-11-21 11:13:08,788: INFO: common: created directory at: artifacts/data_transformation]\\n---------------------------------------------------------------------------\\nUnicodeDecodeError                        Traceback (most recent call last)\\ng:\\\\Machine_Learning_Projects\\\\iNeuron internship\\\\Flight-Fare-Prediction-End-to-End-ML-Project\\\\research\\\\03_data_transformation.ipynb Cell 10 line 9\\n      &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=6\\'&gt;7&lt;/a&gt;     data_transformation.initiate_data_transformation()\\n      &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=7\\'&gt;8&lt;/a&gt; except Exception as e:\\n----&gt; &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=8\\'&gt;9&lt;/a&gt;     raise e\\n\\ng:\\\\Machine_Learning_Projects\\\\iNeuron internship\\\\Flight-Fare-Prediction-End-to-End-ML-Project\\\\research\\\\03_data_transformation.ipynb Cell 10 line 7\\n      &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=3\\'&gt;4&lt;/a&gt;     data_transformation = DataTransformation(config=data_transformation_config)\\n      &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=4\\'&gt;5&lt;/a&gt;     # data_transformation.train_test_spliting()\\n      &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=5\\'&gt;6&lt;/a&gt;     # New Line\\n----&gt; &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=6\\'&gt;7&lt;/a&gt;     data_transformation.initiate_data_transformation()\\n      &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=7\\'&gt;8&lt;/a&gt; except Exception as e:\\n      &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=8\\'&gt;9&lt;/a&gt;     raise e\\n\\ng:\\\\Machine_Learning_Projects\\\\iNeuron internship\\\\Flight-Fare-Prediction-End-to-End-ML-Project\\\\research\\\\03_data_transformation.ipynb Cell 10 line 2\\n     &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=26\\'&gt;27&lt;/a&gt; def initiate_data_transformation(self):\\n     &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=27\\'&gt;28&lt;/a&gt;     ## reading the data\\n---&gt; &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=28\\'&gt;29&lt;/a&gt;     df = pd.read_csv(self.config.data_path)\\n     &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=30\\'&gt;31&lt;/a&gt;     logger.info(\\'Read data completed\\')\\n     &lt;a href=\\'vscode-notebook-cell:/g%3A/Machine_Learning_Projects/iNeuron%20internship/Flight-Fare-Prediction-End-to-End-ML-Project/research/03_data_transformation.ipynb#X12sZmlsZQ%3D%3D?line=31\\'&gt;32&lt;/a&gt;     logger.info(f\\'df dataframe head: \\\\n{df.head().to_string()}\\')\\n\\nFile c:\\\\Users\\\\2021\\\\.conda\\\\envs\\\\flightfareprediction\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\parsers\\\\readers.py:912, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\\n    899 kwds_defaults = _refine_defaults_read(\\n    900     dialect,\\n    901     delimiter,\\n   (...)\\n    908     dtype_backend=dtype_backend,\\n    909 )\\n    910 kwds.update(kwds_defaults)\\n--&gt; 912 return _read(filepath_or_buffer, kwds)\\n\\nFile c:\\\\Users\\\\2021\\\\.conda\\\\envs\\\\flightfareprediction\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\parsers\\\\readers.py:577, in _read(filepath_or_buffer, kwds)\\n    574 _validate_names(kwds.get(&quot;names&quot;, None))\\n    576 # Create the parser.\\n--&gt; 577 parser = TextFileReader(filepath_or_buffer, **kwds)\\n    579 if chunksize or iterator:\\n    580     return parser\\n\\nFile c:\\\\Users\\\\2021\\\\.conda\\\\envs\\\\flightfareprediction\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\parsers\\\\readers.py:1407, in TextFileReader.__init__(self, f, engine, **kwds)\\n   1404     self.options[&quot;has_index_names&quot;] = kwds[&quot;has_index_names&quot;]\\n   1406 self.handles: IOHandles | None = None\\n-&gt; 1407 self._engine = self._make_engine(f, self.engine)\\n\\nFile c:\\\\Users\\\\2021\\\\.conda\\\\envs\\\\flightfareprediction\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\parsers\\\\readers.py:1679, in TextFileReader._make_engine(self, f, engine)\\n   1676     raise ValueError(msg)\\n   1678 try:\\n-&gt; 1679     return mapping[engine](f, **self.options)\\n   1680 except Exception:\\n   1681     if self.handles is not None:\\n\\nFile c:\\\\Users\\\\2021\\\\.conda\\\\envs\\\\flightfareprediction\\\\lib\\\\site-packages\\\\pandas\\\\io\\\\parsers\\\\c_parser_wrapper.py:93, in CParserWrapper.__init__(self, src, **kwds)\\n     90 if kwds[&quot;dtype_backend&quot;] == &quot;pyarrow&quot;:\\n     91     # Fail here loudly instead of in cython after reading\\n     92     import_optional_dependency(&quot;pyarrow&quot;)\\n---&gt; 93 self._reader = parsers.TextReader(src, **kwds)\\n     95 self.unnamed_cols = self._reader.unnamed_cols\\n     97 # error: Cannot determine type of \\'names\\'\\n\\nFile c:\\\\Users\\\\2021\\\\.conda\\\\envs\\\\flightfareprediction\\\\lib\\\\site-packages\\\\pandas\\\\_libs\\\\parsers.pyx:550, in pandas._libs.parsers.TextReader.__cinit__()\\n\\nFile c:\\\\Users\\\\2021\\\\.conda\\\\envs\\\\flightfareprediction\\\\lib\\\\site-packages\\\\pandas\\\\_libs\\\\parsers.pyx:639, in pandas._libs.parsers.TextReader._get_header()\\n\\nFile c:\\\\Users\\\\2021\\\\.conda\\\\envs\\\\flightfareprediction\\\\lib\\\\site-packages\\\\pandas\\\\_libs\\\\parsers.pyx:850, in pandas._libs.parsers.TextReader._tokenize_rows()\\n\\nFile c:\\\\Users\\\\2021\\\\.conda\\\\envs\\\\flightfareprediction\\\\lib\\\\site-packages\\\\pandas\\\\_libs\\\\parsers.pyx:861, in pandas._libs.parsers.TextReader._check_tokenize_status()\\n\\nFile c:\\\\Users\\\\2021\\\\.conda\\\\envs\\\\flightfareprediction\\\\lib\\\\site-packages\\\\pandas\\\\_libs\\\\parsers.pyx:2021, in pandas._libs.parsers.raise_parser_error()\\n\\nUnicodeDecodeError: \\'utf-8\\' codec can\\'t decode byte 0xa1 in position 14: invalid start byte\\n</code></pre>\\n<p>I have been stuck here for days. But can\\'t fix the issue.</p>\\n<p>In the data transformation process, I am not getting any error.</p>\\n<p>I am getting error in only the code of data transformation pipeline.</p>\\n<p>Here is <a href=\"https://github.com/MdEhsanulHaqueKanan/Flight-Fare-Prediction-End-to-End-ML-Project/blob/main/research/03_data_transformation.ipynb\" rel=\"nofollow noreferrer\">my file in GitHub</a>.</p>\\n<p>My file encoding is UTF-8</p>\\n<p>Would you please help me to fix this issue?</p>\\n'},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'scikit-learn',\n",
       "    'nlp',\n",
       "    'tfidfvectorizer'],\n",
       "   'owner': {'account_id': 29966518,\n",
       "    'reputation': 1,\n",
       "    'user_id': 22964973,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/ACg8ocLYfzQYoMWzw_HyWxwsg2EfnkV0mtThRDjjyXBdOj9b=k-s256',\n",
       "    'display_name': 'Sunflower_Poet',\n",
       "    'link': 'https://stackoverflow.com/users/22964973/sunflower-poet'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 48,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700677079,\n",
       "   'creation_date': 1700657920,\n",
       "   'last_edit_date': 1700657951,\n",
       "   'question_id': 77530140,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77530140/why-do-i-keep-getting-the-attributeerror-lower-not-found-error-when-using-a-v',\n",
       "   'title': 'Why do I keep getting the &quot;AttributeError: lower not found&quot; error when using a Vectorizer command?',\n",
       "   'body': \"<p>I'm trying to set up a text classification model, that can split text into two categories. I've checked the shape of my arrays, and they're of the (n, ) format. But when I run my vectorizer command, I seem to run into this error. Any advice would be appreciated :)</p>\\n<pre><code>x,y = dfclean\\\\['text'\\\\],dfclean\\\\['class'\\\\]\\n\\nx.shape\\n\\n&gt; output = (19998,)\\n* * *\\n\\nvectorizer = TfidfVectorizer(min_df=50,max_features=5000)\\n\\nx =  vectorizer.fit(x)\\n&gt; AttributeError: lower not found\\n</code></pre>\\n\"},\n",
       "  {'tags': ['tensorflow',\n",
       "    'machine-learning',\n",
       "    'deep-learning',\n",
       "    'conv-neural-network',\n",
       "    'image-classification'],\n",
       "   'owner': {'account_id': 16822825,\n",
       "    'reputation': 113,\n",
       "    'user_id': 12236342,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh6.googleusercontent.com/-L1mu9V4-Izg/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rfoVZMGD3LywhV_vo6MiO13OEJ7VA/photo.jpg?sz=256',\n",
       "    'display_name': 'Subhodip Roy',\n",
       "    'link': 'https://stackoverflow.com/users/12236342/subhodip-roy'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 74,\n",
       "   'answer_count': 1,\n",
       "   'score': -1,\n",
       "   'last_activity_date': 1700982478,\n",
       "   'creation_date': 1700638652,\n",
       "   'last_edit_date': 1700677497,\n",
       "   'question_id': 77528083,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77528083/vgg16-accuracy-too-low',\n",
       "   'title': 'VGG16 accuracy too low',\n",
       "   'body': \"<p>I am trying to make a ML model with transfer learning (VGG16) in tensorflow, to identify images of medicinal plant from the image of the leaves. The problem is the accuracy of the model is not increasing it is as low as 0.05.</p>\\n<p>My dataset have 30 different folder. Each folder has been named after the class to which the images belong. I have 30 different class labels with name of the plant to which it belongs. The 30 different classes are - 'Alpinia Galanga (Rasna)', 'Amaranthus Viridis (Arive-Dantu)',\\n'Artocarpus Heterophyllus (Jackfruit)',\\n'Azadirachta Indica (Neem)', 'Basella Alba (Basale)',\\n'Brassica Juncea (Indian Mustard)', 'Carissa Carandas (Karanda)',\\n'Citrus Limon (Lemon)', 'Ficus Auriculata (Roxburgh fig)',\\n'Ficus Religiosa (Peepal Tree)', 'Hibiscus Rosa-sinensis',\\n'Jasminum (Jasmine)', 'Mangifera Indica (Mango)', 'Mentha (Mint)',\\n'Moringa Oleifera (Drumstick)',\\n'Muntingia Calabura (Jamaica Cherry-Gasagase)',\\n'Murraya Koenigii (Curry)', 'Nerium Oleander (Oleander)',\\n'Nyctanthes Arbor-tristis (Parijata)',\\n'Ocimum Tenuiflorum (Tulsi)', 'Piper Betle (Betel)',\\n'Plectranthus Amboinicus (Mexican Mint)',\\n'Pongamia Pinnata (Indian Beech)', 'Psidium Guajava (Guava)',\\n'Punica Granatum (Pomegranate)', 'Santalum Album (Sandalwood)',\\n'Syzygium Cumini (Jamun)', 'Syzygium Jambos (Rose Apple)',\\n'Tabernaemontana Divaricata (Crape Jasmine)',\\n'Trigonella Foenum-graecum (Fenugreek)'</p>\\n<p>I am using image <code>ImageDataGenerator</code> for augmentation. The generator is as follows -</p>\\n<pre><code>#Definig ImageDataGenerator With Augmentation\\ndatagen = ImageDataGenerator(\\n        rotation_range=20,\\n        width_shift_range=0.1,\\n        height_shift_range=0.1,\\n        shear_range=0.1,\\n        zoom_range=0.1,\\n        fill_mode='nearest',\\n        validation_split=0.2\\n        ) \\n</code></pre>\\n<p>Yes, data is already splitted in into train, test and val. The main directory contain 3 sub directory and each sub directory has 30 folders with folder-name of same class to which the images inside belongs. The data is loaded using flow_from_directory with the following code</p>\\n<pre><code>#Loading Training DS\\ndef load_ttv_ds(train_dir,test_dir,val_dir):\\n  train_ds = datagen.flow_from_directory(\\n      train_dir,\\n      target_size=(IMAGE_SIZE, IMAGE_SIZE),\\n      batch_size=BATCH_SIZE,\\n      shuffle=True,\\n      # subset='training'\\n  )\\n  test_ds = datagen.flow_from_directory(\\n      test_dir,\\n      target_size=(IMAGE_SIZE, IMAGE_SIZE),\\n      batch_size=BATCH_SIZE,\\n      shuffle=False\\n  )\\n  val_ds = datagen.flow_from_directory(\\n      val_dir,\\n      target_size=(IMAGE_SIZE, IMAGE_SIZE),\\n      batch_size=BATCH_SIZE,\\n      shuffle=False,\\n    #   subset=&quot;validation&quot;\\n  )\\n\\n  return train_ds,test_ds,val_ds\\n</code></pre>\\n<p>While loading the images I have resized it to 256x256 pixels\\ni.e. <code>IMAGE_SIZE=256</code>,<code>BATCH_SIZE=16</code>\\nThere are total 30 classes in my dataset.</p>\\n<p>Found 917 images belonging to 30 classes in train\\nFound 570 images belonging to 30 classes in val\\nFound 357 images belonging to 30 classes in test.</p>\\n<pre><code>from tensorflow.keras import layers, models\\nfrom tensorflow.keras.applications import VGG16\\n\\ndef build_model(lr=0.0001):\\n    base_model = VGG16(\\n        weights='imagenet',\\n        include_top=False,\\n        input_shape=(256, 256, 3)\\n    )\\n\\n    # Freeze the VGG16 layers\\n\\n    base_model.trainable = False\\n\\n    model = models.Sequential()\\n    model.add(base_model)\\n\\n    model.add(layers.Conv2D(64, kernel_size=3, activation='relu', padding='same'))\\n    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\\n\\n    model.add(layers.Conv2D(128, kernel_size=3, activation='relu', padding='same'))\\n    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\\n\\n    model.add(layers.Conv2D(256, kernel_size=3, activation='relu', padding='same'))\\n    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\\n\\n    model.add(layers.Flatten())\\n\\n    model.add(layers.Dense(512, activation='relu'))\\n    model.add(layers.Dropout(0.5))\\n\\n    model.add(layers.Dense(len(class_names), activation='softmax'))\\n\\n    model.compile(\\n        optimizer=keras.optimizers.Adam(learning_rate=lr),\\n        loss='categorical_crossentropy',\\n        metrics=['accuracy']\\n    )\\n\\n    model.summary()\\n    return model\\n</code></pre>\\n<p>This is the model I am using. I am training it for 25 epochs and this is the output.</p>\\n<pre><code>29/29 [==============================] - 36s 1s/step - loss: 5238.8730 - accuracy: 0.0556 - val_loss: 4.9995 - val_accuracy: 0.1211\\nEpoch 2/25\\n29/29 [==============================] - 35s 1s/step - loss: 7077.9985 - accuracy: 0.0774 - val_loss: 6.2652 - val_accuracy: 0.0825\\nEpoch 3/25\\n29/29 [==============================] - 34s 1s/step - loss: 7518.0254 - accuracy: 0.0720 - val_loss: 5.7178 - val_accuracy: 0.1035\\nEpoch 4/25\\n29/29 [==============================] - 34s 1s/step - loss: 6460.3145 - accuracy: 0.0665 - val_loss: 5.5556 - val_accuracy: 0.1404\\nEpoch 5/25\\n29/29 [==============================] - 34s 1s/step - loss: 10283.3672 - accuracy: 0.0774 - val_loss: 4.8519 - val_accuracy: 0.1509\\nEpoch 6/25\\n29/29 [==============================] - 34s 1s/step - loss: 7203.8867 - accuracy: 0.0742 - val_loss: 5.3104 - val_accuracy: 0.1140\\nEpoch 7/25\\n29/29 [==============================] - 34s 1s/step - loss: 7828.9897 - accuracy: 0.0752 - val_loss: 5.2341 - val_accuracy: 0.1175\\nEpoch 8/25\\n29/29 [==============================] - 34s 1s/step - loss: 10705.5059 - accuracy: 0.0709 - val_loss: 5.3684 - val_accuracy: 0.1333\\nEpoch 9/25\\n29/29 [==============================] - 35s 1s/step - loss: 8385.2148 - accuracy: 0.0774 - val_loss: 5.1373 - val_accuracy: 0.1246\\n</code></pre>\\n<p>The test accuracy is 0.05</p>\\n<p>What is the mistake in here? How can I solve it?</p>\\n<p>I have also tried it with different other models. EfficientNetB2 is giving quite good result of almost 0.9875. But we are strictly instructed to use VGG.</p>\\n\"},\n",
       "  {'tags': ['python', 'machine-learning', 'scikit-learn', 'dbscan'],\n",
       "   'owner': {'account_id': 27137080,\n",
       "    'reputation': 19,\n",
       "    'user_id': 20678385,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/aa0e06fa4d5b768515e30f87ca80559d?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'DarthKibo',\n",
       "    'link': 'https://stackoverflow.com/users/20678385/darthkibo'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 86,\n",
       "   'answer_count': 0,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1711050017,\n",
       "   'creation_date': 1700628338,\n",
       "   'last_edit_date': 1711050017,\n",
       "   'question_id': 77527379,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77527379/is-my-python-dbscan-workflow-correct-for-identifying-users-that-have-similar-use',\n",
       "   'title': 'Is my python DBSCAN workflow correct for identifying users that have similar user ratings and genre profiles? Horizontal-Like graph produced',\n",
       "   'body': '<p><a href=\"https://i.sstatic.net/9xpuy.png\" rel=\"nofollow noreferrer\">Horizontal-Like graph</a></p>\\n<pre><code>import requests\\nimport pandas as pd\\nfrom sklearn.cluster import DBSCAN\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.decomposition import PCA\\nimport matplotlib.pyplot as plt\\n\\n# Dataframe structure\\n\\n| Title | User Score | Genre |\\n\\n# One-Hot Encoding the Genre column (there are many genres)\\n\\nanime_dataframe_encoded = pd.get_dummies(anime_dataframe_separated, columns=[\\'Genre\\'], prefix=\\'Genres\\')\\n\\nanime_dataframe_encoded =  anime_dataframe_encoded.groupby([\\'Title\\',\\'Score\\']).sum().reset_index()\\nanime_dataframe_features = anime_dataframe_encoded.drop(\\'Title\\', axis=1)\\n\\n\\nscaler = StandardScaler()\\nanime_dataframe_scaled = scaler.fit_transform(anime_dataframe_features)\\n\\npca = PCA(n_components=1)\\nreduced_features = pca.fit_transform(anime_dataframe_scaled)\\n\\ndbscan = DBSCAN(eps=0.5, min_samples=5)\\nlabels = dbscan.fit_predict(reduced_features)\\n\\n# Visualize the results\\nplt.figure(figsize=(8, 8))\\nplt.scatter(reduced_features[:, 0], labels, c=labels, cmap=\\'viridis\\', s=50)\\nplt.title(\\'DBSCAN Clustering Results\\')\\nplt.xlabel(\\'Principal Component 1\\')\\nplt.ylabel(\\'Cluster Label\\')\\nplt.show()\\n\\n\\n</code></pre>\\n<p>I only had 1 user\\'s list. But is that the correct path forward? The images I saw for DBSCAN are donut shapes which is most likely due to a lot data (which I need to add more user\\'s list). However, I am not sure if what i\\'m doing is correct as I am a beginner.</p>\\n'},\n",
       "  {'tags': ['python', 'machine-learning', 'signals'],\n",
       "   'owner': {'account_id': 29959603,\n",
       "    'reputation': 1,\n",
       "    'user_id': 22959684,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://i.sstatic.net/eTeka.jpg?s=256',\n",
       "    'display_name': 'KAUSTAV NARJEENARY',\n",
       "    'link': 'https://stackoverflow.com/users/22959684/kaustav-narjeenary'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 172,\n",
       "   'answer_count': 1,\n",
       "   'score': -1,\n",
       "   'last_activity_date': 1711050106,\n",
       "   'creation_date': 1700587111,\n",
       "   'last_edit_date': 1711050106,\n",
       "   'question_id': 77524730,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77524730/a-way-to-count-the-number-of-cycles-of-an-given-attribute-in-a-given-dataset',\n",
       "   'title': 'a way to count the number of cycles of an given attribute in a given dataset',\n",
       "   'body': '<p>i want to find the number of cycles in the given data set of motor fault current , it\\'s given by the motor currents which were measured at different times , i want to know if it can even be considered a signal or it\\'s just a pattern ?</p>\\n<p><a href=\"https://i.sstatic.net/0dKM1.png\" rel=\"nofollow noreferrer\">motor fault current</a></p>\\n<p>i tried some libraries , but they\\'re for different conditions like they have zero crossing points for signals used and i can\\'t find a point that i can turn into zero crossing for this as the motor current is always +ve and as such it just fluctuates with time a bit , and thus i\\'m getting a plot</p>\\n<pre><code>data = pd.read_csv(&quot;healthy.csv&quot;)\\ny = np.array(data.Current_A)\\nx = data.index\\ndate_array = pd.array(data.TimeStamp)\\nplt.plot(date_array,y)\\n</code></pre>\\n<p><a href=\"https://data.mendeley.com/datasets/gxdd74czwh/1\" rel=\"nofollow noreferrer\">i used the 3ph healthy motor current dataset here , one thing , the dataset has whitespace as&quot; Current-A&quot; , it should be fixed as &quot;Current_A&quot;</a></p>\\n'},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'scikit-learn',\n",
       "    'classification',\n",
       "    'ensemble-learning'],\n",
       "   'owner': {'account_id': 23428229,\n",
       "    'reputation': 305,\n",
       "    'user_id': 17487457,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/AATXAJx5whUjQpqy8vjb87FpxlGd571pMsem-JYZhHu9=k-s256',\n",
       "    'display_name': 'Amina Umar',\n",
       "    'link': 'https://stackoverflow.com/users/17487457/amina-umar'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 206,\n",
       "   'answer_count': 2,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701376662,\n",
       "   'creation_date': 1700586745,\n",
       "   'last_edit_date': 1701021656,\n",
       "   'question_id': 77524700,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77524700/creating-an-ensemble-of-classifiers-based-on-predefined-feature-subsets',\n",
       "   'title': 'Creating an ensemble of classifiers based on predefined feature subsets',\n",
       "   'body': '<p>The following MWE creates an ensemble method from the features selected using <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\" rel=\"nofollow noreferrer\">SelectKBest</a> algorithm and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\" rel=\"nofollow noreferrer\">RandomForest</a> classifier.</p>\\n<pre class=\"lang-py prettyprint-override\"><code># required import\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.feature_selection import SelectKBest, f_classif\\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\\nfrom sklearn.pipeline import Pipeline\\n\\n# ensemble created from features selected\\ndef get_ensemble(n_features):\\n  # define base models\\n  models = []\\n  # enumerate the features in the training dataset\\n  for i in range(1, n_features + 1):\\n    # feature selection transform\\n    fs = SelectKBest(score_func=f_classif, k=i)\\n    # create the model\\n    model = RandomForestClassifier(n_estimators=50)\\n    # create the pipeline\\n    pipe = Pipeline([(\\'fs\\', fs), (\\'m\\', model)])\\n    # list of tuple of models for voting\\n    models.append((str(i), pipe))\\n\\n  # define the voting ensemble\\n  ensemble_clf = VotingClassifier(estimators=models, voting=\\'hard\\')\\n\\n  return ensemble_clf\\n</code></pre>\\n<p>So, to use the ensemble model:</p>\\n<pre class=\"lang-py prettyprint-override\"><code># generate data for a 3-class classification\\nX, y = make_classification(n_samples=1000, n_features=10, n_classes=3,\\n                             n_informative=3)\\n\\nX = pd.DataFrame(X, columns=list(\\'ABCDEFGHIJ\\'))\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y,\\n         test_size=0.3, random_state=42)\\n\\nX_train.head()\\n       A       B       C       D       E       F       G       H       I      J\\n541  0.1756 -0.3772 -1.6396 -0.7524  0.2138  0.3113 -1.4906 -0.2885  0.1226  0.2057\\n440 -0.4381 -0.3302  0.7514 -0.4684 -1.2477 -0.5081 -0.7934 -0.3138  0.8423 -0.4038\\n482 -0.6648  1.2337 -0.2878 -1.6737 -1.2377 -0.4479 -1.1843 -0.2424 -0.9935 -1.4537\\n422  0.6099  0.2475  0.9612 -0.7339  0.6926 -1.5761 -1.6061 -0.3879 -0.1895  1.3738\\n778 -1.4893  0.5234  1.6126  0.8704 -2.7363 -1.3818 -0.2196 -0.7894 -1.1755 -2.8779\\n\\n# get the ensemble model\\nensemble_clssifier = get_ensemble(X_train.shape[1])\\n\\nensemble_clssifier.fit(X_train, y_train)\\n</code></pre>\\n<p>Creates 10 base models (<code>n_features=10</code>) and then an ensemble VotingClassifier based on majority (<code>voting = hard</code>).</p>\\n<p><strong>Question:</strong></p>\\n<p>The MWE described above works fine. However, I would like to replace the <code>SelectKBest</code> feature selection process in the <code>get_ensemble</code> function.</p>\\n<p>I have conducted a different feature selection process, and discovered the &quot;optimal&quot; feature subset for each class in this dataset as follows:</p>\\n<pre><code>             | best predictors\\n-------------+-------------------\\n   class 0   |  A, B, C\\n   class 1   |  D, E, F, G\\n   class 2   |  G, H, I, J\\n-------------+-------------------\\n</code></pre>\\n<p>So the modification I would like to make to <code>get_ensemble</code> is that, instead of iterating  over the number of available features, creating <code>n</code> base-models, it should create 3 (no. of classes) base models, where:</p>\\n<ul>\\n<li><p><code>base-model 1</code> will be fitted using the feature subset <code>[\\'A\\', \\'B\\', \\'C\\']</code>.</p>\\n</li>\\n<li><p><code>base-model 2</code> will be fitted using the feature subset <code>[\\'D\\', \\'E\\', \\'F\\', \\'G\\']</code>.</p>\\n</li>\\n<li><p><code>base-model 3</code> will be fitted using the feature subset <code>[\\'G\\', \\'H\\', \\'I\\', \\'J\\']</code>.</p>\\n</li>\\n<li><p>finally the <code>ensemble_classifier</code> based on majority voting of the sub-models output.</p>\\n</li>\\n</ul>\\n<p>That\\'s, I when I make the call to:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>ensemble_clssifier.fit(X_train, y_train)\\n</code></pre>\\n<p>It proceeds like so:</p>\\n<pre class=\"lang-py prettyprint-override\"><code># 1st base model on fitted on its feature subset\\nmodel.fit(X_train[[\\'A\\', \\'B\\', \\'C\\']], y_train)\\n# 2nd base model\\nmodel.fit(X_train[[\\'D\\', \\'E\\', \\'F\\', \\'G\\']], y_train)\\n# 3rd model also\\nmodel.fit(X_train[[\\'G\\', \\'H\\', \\'I\\', \\'J\\']], y_train)\\n</code></pre>\\n<p>This scenario should apply as well during prediction, making sure each base model selects the appropriate feature subset from <code>X_test</code> to make its prediction on <code>ensemble_clssifier.fit(X_test)</code> before the final voting.</p>\\n<p>I am not sure how to proceed. Any ideas?</p>\\n<p><strong>EDIT</strong></p>\\n<p>Regarding this question, I made some changes (e.g. not using the <code>VotingClassifier</code>) to further train the final ensemble on the output of the base models (base models confidences). Then finally make predictions.</p>\\n<p>I created the following ensemble class:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn.base import clone\\n\\nclass CustomEnsemble:\\n    def __init__(self, base_model, best_feature_subsets):\\n        self.base_models = {class_label: clone(base_model) for class_label in best_feature_subsets}\\n        self.best_feature_subsets = best_feature_subsets\\n        self.final_model = base_model\\n\\n    def train_base_models(self, X_train, y_train):\\n        for class_label, features in self.best_feature_subsets.items():\\n            model = self.base_models[class_label]\\n            model.fit(X_train[features], (y_train == class_label))\\n        \\n        return self\\n    \\n    def train_final_model(self, X_train, y_train):\\n        &quot;&quot;&quot;\\n        Probably better to implement the train methods (base models &amp; ensemble)\\n        in one method suc as the train_base_models  altogether.\\n        &quot;&quot;&quot;\\n        predictions = pd.DataFrame()\\n\\n        for class_label, model in self.base_models.items():\\n            predictions[class_label] = model.predict_proba(X_train[self.best_feature_subsets[class_label]])[:, 1]\\n\\n        self.final_model.fit(predictions, y_train)\\n\\n\\n    def predict_base_models(self, X_test):\\n        predictions = pd.DataFrame()\\n\\n        for class_label, model in self.base_models.items():\\n            predictions[class_label] = model.predict_proba(X_test[self.best_feature_subsets[class_label]])[:, 1]\\n\\n        return predictions\\n\\n    def predict(self, X_test):\\n        base_model_predictions = self.predict_base_models(X_test)\\n        return self.final_model.predict(base_model_predictions)\\n\\n    def predict_proba_base_models(self, X_test):\\n        predictions = pd.DataFrame()\\n\\n        for class_label, model in self.base_models.items():\\n            predictions[class_label] = model.predict_proba(X_test[self.best_feature_subsets[class_label]])[:, 1]\\n\\n        return predictions\\n\\n    def predict_proba(self, X_test):\\n        base_model_predictions = self.predict_proba_base_models(X_test)\\n        return self.final_model.predict_proba(base_model_predictions)\\n\\n</code></pre>\\n<p><em><strong>Usage:</strong></em></p>\\n<ol>\\n<li>Define dictionary of best feature subsets for classes:</li>\\n</ol>\\n<pre class=\"lang-py prettyprint-override\"><code>optimal_features = {\\n    0: [\\'A\\', \\'B\\', \\'C\\'],\\n\\n    1: [\\'D\\', \\'E\\', \\'F\\', \\'G\\'],\\n\\n    2: [\\'G\\', \\'H\\', \\'I\\', \\'J\\']\\n}\\n\\n</code></pre>\\n<ol start=\"2\">\\n<li>Instantiate class and train models:</li>\\n</ol>\\n<pre class=\"lang-py prettyprint-override\"><code>classifier = RandomForestClassifier()\\nensemble   = CustomEnsemble(classifier, optimal_features)\\n</code></pre>\\n<ol start=\"3\">\\n<li>Train models:</li>\\n</ol>\\n<pre class=\"lang-py prettyprint-override\"><code># first, train base models\\nensemble.train_base_models(X_train, y_train)\\n# then, train the ensemble\\nensemble.train_final_model(X_train, y_train)\\n</code></pre>\\n<ol start=\"4\">\\n<li>Make predictions:</li>\\n</ol>\\n<pre class=\"lang-py prettyprint-override\"><code>yhat = ensemble.predict(X_test)\\nyhat_proba = ensemble.predict_proba(X_test) # so as to calculate roc_auc_score() \\n</code></pre>\\n<ol>\\n<li><p>However, it appears I am not doing things right. I am not training the ensemble on the output of base models, but on the original input features.</p>\\n</li>\\n<li><p>Also, I am not sure if separating <code>train_base_models()</code> and <code>train_final_model()</code> is the best approach (this implies fitting twice: base models then final model as in the usage). Or better to combine these into one method (say <code>train_ensemble()</code>).</p>\\n</li>\\n</ol>\\n'},\n",
       "  {'tags': ['python', 'machine-learning', 'scikit-learn', 'linear-regression'],\n",
       "   'owner': {'account_id': 29215604,\n",
       "    'reputation': 58,\n",
       "    'user_id': 22382498,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://i.sstatic.net/7mnaQ.jpg?s=256',\n",
       "    'display_name': 'Beitian Ma',\n",
       "    'link': 'https://stackoverflow.com/users/22382498/beitian-ma'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 286,\n",
       "   'answer_count': 1,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1700594194,\n",
       "   'creation_date': 1700580163,\n",
       "   'last_edit_date': 1700594167,\n",
       "   'question_id': 77523966,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77523966/keep-0-1-samples-to-be-outliers-in-huberregressor',\n",
       "   'title': 'Keep 0.1% samples to be outliers in HuberRegressor',\n",
       "   'body': \"<p>When I run Huber regression on a certain data set using <code>HuberRegressor()</code> form <code>sklearn.linear_model</code>, I want to keep 0.1% of all samples to be outliers. As I know, HuberRegressor() controls the size of outliers by param <code>epsilon</code>, but the it doesn's support a percentage form, and my response variable <strong>does not come from a normal distribution</strong>.</p>\\n<p>The desired results are shown below</p>\\n<pre><code>import numpy as np\\nfrom sklearn.linear_model import HuberRegressor\\n\\nX = np.random.rand(100, 3)\\ny = np.random.rand(100, 1)\\nmodel = HuberRegressor(epsilon=?, fit_intercept=True, alpha=0)\\nmodel.fit(X, y)\\n\\n&gt;&gt;&gt; (model.outliers_).sum / len(y)\\n&gt;&gt;&gt; 0.001\\n</code></pre>\\n<p>Further, is there a way to generally make such adjustments to models that use the huber loss function? Such as GLS+Huber or GBRT+Huber?</p>\\n\"},\n",
       "  {'tags': ['machine-learning', 'neural-network', 'julia'],\n",
       "   'owner': {'account_id': 27832007,\n",
       "    'reputation': 13,\n",
       "    'user_id': 21250240,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/AEdFTp4abBdFvn52p7jSr5BwvPP2yyLAInih9FODjOAG=k-s256',\n",
       "    'display_name': 'Torsten Buchert',\n",
       "    'link': 'https://stackoverflow.com/users/21250240/torsten-buchert'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 67,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700580059,\n",
       "   'creation_date': 1700567317,\n",
       "   'last_edit_date': 1700580059,\n",
       "   'question_id': 77522577,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77522577/dimensionmismatch-matrix-a-has-dimensions-100-7-matrix-b-has-dimensions-1-1',\n",
       "   'title': 'DimensionMismatch: matrix A has dimensions (100,7), matrix B has dimensions (1,10)',\n",
       "   'body': '<p>I want to train my neural network to classify 7 segments LED digits.The inputlayer in my neural network is 7 for every row in my inputmatrix and the output lyer is 10 for the outputmatrix dimension. But when i want to train it this error comes up:</p>\\n<pre><code>DimensionMismatch: matrix A has dimensions (100,7), matrix B has dimensions (1,10)\\n\\nmacro expansion@interface2.jl:101[inlined]\\n_pullback(::Zygote.Context{true}, ::typeof(throw), ::DimensionMismatch)@interface2.jl:101\\n_pullback@matmul.jl:856[inlined]\\n_pullback(::Zygote.Context{true}, ::typeof(LinearAlgebra._generic_matmatmul!), ::Matrix{LinearAlgebra.Transpose{Float32, Vector{Float32}}}, ::Char, ::Char, ::Matrix{Float32}, ::Vector{Vector{Int64}}, ::LinearAlgebra.MulAddMul{true, true, Bool, Bool})@interface2.jl:0\\n_pullback@matmul.jl:847[inlined]\\n_pullback@matmul.jl:407[inlined]\\n_pullback@matmul.jl:276[inlined]\\n_pullback(::Zygote.Context{true}, ::typeof(LinearAlgebra.mul!), ::Matrix{LinearAlgebra.Transpose{Float32, Vector{Float32}}}, ::Matrix{Float32}, ::LinearAlgebra.Transpose{LinearAlgebra.Transpose{Int64, Vector{Int64}}, Vector{Vector{Int64}}})@interface2.jl:0\\n_pullback@matmul.jl:141[inlined]\\n_pullback(::Zygote.Context{true}, ::typeof(*), ::Matrix{Float32}, ::LinearAlgebra.Transpose{LinearAlgebra.Transpose{Int64, Vector{Int64}}, Vector{Vector{Int64}}})@interface2.jl:0\\n_pullback@Other: 10[inlined]\\n_pullback(::Zygote.Context{true}, ::Main.var&quot;workspace#4&quot;.Layer, ::LinearAlgebra.Transpose{LinearAlgebra.Transpose{Int64, Vector{Int64}}, Vector{Vector{Int64}}})@interface2.jl:0\\n_apply(::Function, ::Vararg{Any})@boot.jl:838\\nadjoint@lib.jl:203[inlined]\\n_pullback@adjoint.jl:66[inlined]\\n_pullback@operators.jl:1035[inlined]\\n_pullback@operators.jl:1034[inlined]\\n_pullback@operators.jl:1031[inlined]\\n_pullback(::Zygote.Context{true}, ::Base.var&quot;##_#97&quot;, ::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}}, ::ComposedFunction{Main.var&quot;workspace#4&quot;.Layer, Main.var&quot;workspace#4&quot;.Layer}, ::LinearAlgebra.Transpose{LinearAlgebra.Transpose{Int64, Vector{Int64}}, Vector{Vector{Int64}}})@interface2.jl:0\\n_apply(::Function, ::Vararg{Any})@boot.jl:838\\nadjoint@lib.jl:203[inlined]\\n_pullback@adjoint.jl:66[inlined]\\n_pullback@operators.jl:1031[inlined]\\n_pullback(::Zygote.Context{true}, ::ComposedFunction{Main.var&quot;workspace#4&quot;.Layer, Main.var&quot;workspace#4&quot;.Layer}, ::LinearAlgebra.Transpose{LinearAlgebra.Transpose{Int64, Vector{Int64}}, Vector{Vector{Int64}}})@interface2.jl:0\\n_pullback@Other: 8[inlined]\\n_pullback(::Zygote.Context{true}, ::Main.var&quot;workspace#124&quot;.Network, ::LinearAlgebra.Transpose{LinearAlgebra.Transpose{Int64, Vector{Int64}}, Vector{Vector{Int64}}})@interface2.jl:0\\n_pullback@Local: 17[inlined]\\n_pullback(::Zygote.Context{true}, ::Main.var&quot;workspace#300&quot;.var&quot;#1#2&quot;)@interface2.jl:0\\npullback(::Function, ::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})@interface.jl:414\\ngradient(::Function, ::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})@interface.jl:96\\ntop-level scope@Local: 16\\n</code></pre>\\n<p>How to resolve this error?</p>\\n<pre><code>begin\\nstruct Layer\\n   W::Matrix{Float32}\\n   gradients\\n   b::Vector{Float32} \\n   activation::Function\\n   Layer(in::Int64,out::Int64,activation: \\n   :Function=identityFunction) =\\n   new(randn(out,in),randn(out),activation) \\nend\\n\\nstruct Network\\n  layers::Vector{Layer}\\n  Network(layers::Vararg{Layer}) = new(vcat(layers...))\\nend\\n\\n(m::Network)(x) = reduce((left,right)-&gt;rightleft, m.layers)(x)\\n\\ninputs = [\\n    [1, 1, 1, 1, 1, 1, 0],  # 0\\n    [0, 1, 1, 0, 0, 0, 0],  # 1\\n    [1, 1, 0, 1, 1, 0, 1],  # 2\\n    [1, 1, 1, 1, 0, 0, 1],  # 3\\n    [0, 1, 1, 0, 0, 1, 1],  # 4\\n    [1, 0, 1, 1, 0, 1, 1],  # 5\\n    [1, 0, 1, 1, 1, 1, 1],  # 6\\n    [1, 1, 1, 0, 0, 0, 0],  # 7\\n    [1, 1, 1, 1, 1, 1, 1],  # 8\\n    [1, 1, 1, 1, 0, 1, 1]   # 9\\n] \\n    \\n    targetOutput =  [\\n    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],  # 0\\n    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],  # 1\\n    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],  # 2\\n    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],  # 3\\n    [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],  # 4\\n    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],  # 5\\n    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],  # 6\\n    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],  # 7\\n    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],  # 8\\n    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]   # 9\\n] \\n    \\n    mse(x,y) = sum((x .- y).^2)/length(x) \\n    \\n    using Random\\n    Random.seed!(54321) \\n    \\n    twoLayerNeuralNet = Network(Layer(7,100,ReLu), Layer(100,10)) \\n    \\n    Flux.@functor Layer \\n    Flux.@functor Network \\n    parameters = Flux.params(twoLayerNeuralNet)\\n\\n    optimizer = ADAM(0.05) \\n    netOutput = [] \\n    lossCurve = [] \\n    for i in 1:1000\\n\\n    gradients = Zygote.gradient(\\n    () -&gt; mse(\\n    twoLayerNeuralNet(transpose(inputs))[:],\\n    targetOutput),\\n    parameters)\\n\\n    Flux.Optimise.update!(optimizer, parameters, gradients)\\n\\n    actualOutput = twoLayerNeuralNet(transpose(inputs))[:]\\n    push!(netOutput, actualOutput)\\n    push!(lossCurve,\\n    mse(actualOutput,targetOutput))\\n    \\n    end\\nend\\n</code></pre>\\n'},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'azure-functions',\n",
       "    'azureml-python-sdk'],\n",
       "   'owner': {'user_type': 'does_not_exist', 'display_name': 'user12704077'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 362,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700645449,\n",
       "   'creation_date': 1700565318,\n",
       "   'question_id': 77522368,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77522368/can-you-load-ml-models-from-azureml-registry-to-an-azure-function-app',\n",
       "   'title': 'can you load ml models from azureml registry to an Azure function app',\n",
       "   'body': '<p>Is it possible to load  registered ml models from azureml model registry to an azure function app and carry out inference or do these models need to be downloaded inside the function app first? any examples or guides?</p>\\n'},\n",
       "  {'tags': ['kubernetes',\n",
       "    'machine-learning',\n",
       "    'message-queue',\n",
       "    'distributed-system',\n",
       "    'task-queue'],\n",
       "   'owner': {'account_id': 21129696,\n",
       "    'reputation': 1,\n",
       "    'user_id': 15534336,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/751247c911dc3d06fd70df78561712e9?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'marcOwn',\n",
       "    'link': 'https://stackoverflow.com/users/15534336/marcown'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 91,\n",
       "   'answer_count': 1,\n",
       "   'score': -1,\n",
       "   'last_activity_date': 1700645161,\n",
       "   'creation_date': 1700563573,\n",
       "   'last_edit_date': 1700645161,\n",
       "   'question_id': 77522167,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77522167/distributed-system-ml-queue',\n",
       "   'title': 'Distributed System: ML Queue',\n",
       "   'body': \"<p>I want to design a distributed queue for a machine learning inference system (using kubernetes). The problem is the following:</p>\\n<p>Each request also contains a model_id the request should be executed with (there are a lot of models). So when a consumer gets a queue message it has to download the model file from a blob storage (maybe cache it locally) and put it onto the gpu and then execute it with a given datapoint (also given in the request).</p>\\n<p>This download and gpu load takes some time. Therefore, it would be really beneficial if the next queue element the worker gets also contains a request with the same, already loaded model_id.</p>\\n<p>Is this possible with any of the existing queue systems like rabbitmq or redis?</p>\\n<p>Furthermore, it would be good to have some sort of equal chance to get a request for each model_id independent of the number of requests for each model_id.</p>\\n<p>I tried to build this with redis but I don't see any mechanism to tell the queue to filter the messages and give me one with the specified model_id if it exists oder any other random model_id.</p>\\n\"},\n",
       "  {'tags': ['r', 'machine-learning', 'onnx', 'mlr3'],\n",
       "   'owner': {'account_id': 18965720,\n",
       "    'reputation': 3002,\n",
       "    'user_id': 13840270,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh5.googleusercontent.com/-6D3hEgEPMp8/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucm2Wc0c7p8KxWNIp_ZANzog298z7g/photo.jpg?sz=256',\n",
       "    'display_name': 'DuesserBaest',\n",
       "    'link': 'https://stackoverflow.com/users/13840270/duesserbaest'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 234,\n",
       "   'answer_count': 1,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1721617771,\n",
       "   'creation_date': 1700561939,\n",
       "   'last_edit_date': 1700563409,\n",
       "   'question_id': 77522002,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77522002/is-it-possible-to-transfer-a-ml-model-from-r-into-onnx-format',\n",
       "   'title': 'Is it possible to transfer a ML model from R into ONNX format',\n",
       "   'body': '<p>I am currently training a ML model in R (specifically using the <code>mlr3</code> framework - if this is a no-go I would be open to using other packages also). Later I want to apply the model into production, but for this it needs to be in ONNX format. My online research did not jet yield any possible solution on converting any ML model trained in R into ONNX format. Is this at all possible?</p>\\n'},\n",
       "  {'tags': ['python', 'tensorflow', 'machine-learning'],\n",
       "   'owner': {'account_id': 3873970,\n",
       "    'reputation': 22574,\n",
       "    'user_id': 4281353,\n",
       "    'user_type': 'registered',\n",
       "    'accept_rate': 69,\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/2c965e9fbb680e2527285a75b1712497?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'mon',\n",
       "    'link': 'https://stackoverflow.com/users/4281353/mon'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 780,\n",
       "   'answer_count': 2,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700574293,\n",
       "   'creation_date': 1700550877,\n",
       "   'last_edit_date': 1700552146,\n",
       "   'question_id': 77520936,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77520936/tensorflow-tf-keras-model-fit-causes-run-out-of-data-for-validation-data-with',\n",
       "   'title': 'tensorflow - tf.keras.Model.fit causes run out of data for validation data with validation_steps being set',\n",
       "   'body': '<p>Trying to understand the <code>validation_steps</code> parameter of <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\" rel=\"nofollow noreferrer\">tf.keras.Model.fit</a>.</p>\\n<blockquote>\\n<p>Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch.</p>\\n</blockquote>\\n<p>For instance, <a href=\"https://www.tensorflow.org/datasets/catalog/mnist\" rel=\"nofollow noreferrer\">TFDS MNIST</a> dataset has <code>60,000</code> train and <code>10,000</code> test data records. Trying to consume all the records during <code>num_epochs=2</code> epochs with <code>batch_size=8</code> using generators as the data sources to the model.</p>\\n<pre><code>(train, test), info = tfds.load(\\n    \\'mnist\\',\\n    split=[\\'train\\', \\'test\\'],\\n    shuffle_files=True,\\n    as_supervised=True,\\n    with_info=True,\\n)\\n\\nx_generator = train.batch(batch_size).as_numpy_iterator()\\nv_generator = test.batch(batch_size).as_numpy_iterator()   # using \\'test\\' for validation here\\n</code></pre>\\n<p>The training data can afford <code>3750=(60000 / batch_size=8 / epochs=2)</code> batches, and the test data can afford <code>625=(10000 / batch_size=8 / epochs=2)</code> batches.</p>\\n<pre><code>def f(image, label):\\n    return 1\\n\\nnum_total_train_records = len(list(        # 60000\\n    train.map(f)\\n))\\nnum_total_test_records = len(list(         # 10000\\n    test.map(f)\\n))\\nprint(num_total_train_records, num_total_test_records)\\n-----\\n60000 10000\\n</code></pre>\\n<pre><code>num_epochs = 2\\nbatch_size = 8\\n\\nnum_x_batches_per_epoch = int(np.floor(num_total_train_records / batch_size / num_epochs))\\nnum_v_batches_per_epoch = int(np.floor(num_total_test_records / batch_size / num_epochs)) \\nprint(num_x_batches_per_epoch, num_v_batches_per_epoch)\\n# ---\\n# show 3750 625\\n</code></pre>\\n<p>However, setting <code>tf.keras.Model.fit(validation_steps=625)</code> causes the error <code>Your input ran out of data... Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 625 batches)</code>.</p>\\n<pre><code>model.fit(\\n    x=x_generator ,\\n    epochs=num_epochs,\\n    batch_size=batch_size,    # not using batch_size arg makes no difference\\n    steps_per_epoch=num_x_batches_per_epoch,\\n    validation_data=v_generator,\\n    validation_steps=num_v_batches_per_epoch,\\n    validation_batch_size=batch_size\\n)\\n</code></pre>\\n<pre><code>Your input ran out of data; interrupting training. \\nMake sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches \\n(in this case, 625 batches). You may need to use the repeat() function when building your dataset.\\n\\n2023-11-21 17:39:33.226528: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17391114698345974101\\n2023-11-21 17:39:33.226580: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8226056677969075330\\nWARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 625 batches). You may need to use the repeat() function when building your dataset.\\n</code></pre>\\n<h2>Code</h2>\\n<pre><code>import numpy as np\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nimport tensorflow_datasets as tfds\\n\\n\\n(train, test), info = tfds.load(\\n    \\'mnist\\',\\n    split=[\\'train\\', \\'test\\'],\\n    shuffle_files=True,\\n    as_supervised=True,\\n    with_info=True,\\n)\\n\\n\\ndef f(image, label):\\n    return 1\\n\\nnum_total_train_records = len(list(\\n    train.map(f)\\n))\\nnum_total_test_records = len(list(\\n    test.map(f)\\n))\\nprint(num_total_train_records, num_total_test_records)\\n\\nmodel = tf.keras.models.Sequential([\\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\\n  tf.keras.layers.Dense(128, activation=\\'relu\\'),\\n  tf.keras.layers.Dense(10)\\n])\\nmodel.compile(\\n    optimizer=tf.keras.optimizers.Adam(0.001),\\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\\n)\\n\\nnum_epochs = 2\\nbatch_size = 8\\n\\nnum_x_batches_per_epoch = int(np.floor(num_total_train_records / batch_size / num_epochs))\\nnum_v_batches_per_epoch = int(np.floor(num_total_test_records / batch_size / num_epochs)) \\nprint(num_x_batches_per_epoch, num_v_batches_per_epoch)\\n# ---\\n# will show 3750 625\\n\\n\\nx_generator = train.batch(batch_size).as_numpy_iterator()\\nv_generator = test.batch(batch_size).as_numpy_iterator()\\n\\nmodel.fit(\\n    x=x_generator ,\\n    epochs=num_epochs,\\n    batch_size=batch_size,\\n    steps_per_epoch=num_x_batches_per_epoch,\\n    validation_data=v_generator,\\n    validation_steps=num_v_batches_per_epoch,\\n    validation_batch_size=batch_size\\n)\\n</code></pre>\\n<p>By minus 1, it works.</p>\\n<pre><code>num_v_batches_per_epoch = int(np.floor(num_total_test_records / batch_size / num_epochs)) -1  # Cuase ran out of data without -1\\n</code></pre>\\n<p>Please help understand this behavior. Also the document says <code>Only relevant if validation_data is provided and is a tf.data dataset.</code> but obviously it is not only for <code>tf.data.Dataset</code>.</p>\\n<h2>Environment</h2>\\n<pre><code>tensorflow 2.14.1\\nPython 3.10.12\\nUbuntu 22.04 LTS\\n</code></pre>\\n'},\n",
       "  {'tags': ['python',\n",
       "    'opencv',\n",
       "    'machine-learning',\n",
       "    'image-processing',\n",
       "    'data-acquisition'],\n",
       "   'owner': {'account_id': 29954673,\n",
       "    'reputation': 1,\n",
       "    'user_id': 22955856,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/ACg8ocLCZUP6_P4jYZA0--9fIZ5IiBox_f8iVThEP712PkpzFqg=k-s256',\n",
       "    'display_name': 'Mehul Kini',\n",
       "    'link': 'https://stackoverflow.com/users/22955856/mehul-kini'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 47,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700593760,\n",
       "   'creation_date': 1700547324,\n",
       "   'last_edit_date': 1700593760,\n",
       "   'question_id': 77520661,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77520661/how-to-dynamically-crop-the-pics-from-the-thresholded-images-using-opencv',\n",
       "   'title': 'How to dynamically crop the pics from the thresholded images using Opencv?',\n",
       "   'body': '<p><a href=\"https://i.sstatic.net/vL3ZI.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/vL3ZI.png\" alt=\"1\" /></a></p>\\n<p>So the I have Thresholded the Image using OpenCV\\nHers a piece of code</p>\\n<pre><code>X = [] # Image data\\ny = [] # Labels\\n\\n# Loops through imagepaths to load images and labels into arrays\\nfor path in imagepaths:\\n  img = cv.imread(path) # Reads image and returns np.array\\n  img = cv.cvtColor(img, cv.COLOR_BGR2GRAY) # Converts into the corret colorspace (GRAY)\\n  img_thresh = cv.threshold(img, 50, 225, cv.THRESH_BINARY)[1]  # Threshholds the grayscale image and the pixels below 50 are considered as black and above it are white\\n  img_thresh = cv.bitwise_not(img_thresh) \\n  X.append(img_thresh)\\n  \\n  # Processing label in image path\\n  category = path.split(&quot;\\\\\\\\&quot;)[4]\\n  label = int(category.split(&quot;_&quot;)[0][1]) # We need to convert 10_down to 00_down, or else it crashes\\n  y.append(label)\\n\\n# Turn X and y into np.array to speed up train_test_split\\nX = np.array(X, dtype=&quot;uint8&quot;)\\nX = X.reshape(len(imagepaths), 240, 640, 1) # Needed to reshape so CNN knows it\\'s different images\\ny = np.array(y)\\n\\nprint(&quot;Images loaded: &quot;, len(X))\\nprint(&quot;Labels loaded: &quot;, len(y))\\n\\nprint(y[0], imagepaths[0]) # Debugging\\n</code></pre>\\n<p>Can anyone please help me in dynamically croping the pic such that I crop the pics on the tips of the thresholded pics</p>\\n'},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'pyspark',\n",
       "    'amazon-emr',\n",
       "    'amazon-sagemaker'],\n",
       "   'owner': {'account_id': 19408127,\n",
       "    'reputation': 224,\n",
       "    'user_id': 14193101,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/fa6bd6c2e06c850c67d7f8276821a25f?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'Maokai',\n",
       "    'link': 'https://stackoverflow.com/users/14193101/maokai'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 258,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701953050,\n",
       "   'creation_date': 1700503859,\n",
       "   'last_edit_date': 1700524274,\n",
       "   'question_id': 77518065,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77518065/cant-train-sagemaker-model-using-recordio-protobuf-data-saved-in-s3',\n",
       "   'title': 'Can&#39;t train sagemaker model using recordio protobuf data saved in S3',\n",
       "   'body': '<p>I want to preprocess data in Amazon EMR using PySpark and train a machine learning model in SageMaker using pipe mode. The issue I am having now is saving data in S3 and feeding it to the model.</p>\\n<p>SageMaker models accept the application/x-recordio-protobuf type. Therefore, I have saved my data as:</p>\\n<pre><code>output_path = f&quot;s3://my_path/output_processed&quot;\\ndf_transformed.write.format(&quot;sagemaker&quot;).mode(&quot;overwrite&quot;).save(output_path)\\n</code></pre>\\n<p>Where <code>df_transformed</code> is a <code>pyspark dataframe</code>.</p>\\n<p>When I am trying to feed my data to the model:</p>\\n<pre><code>records = RecordSet(s3_data=train_path, s3_data_type=\\'S3Prefix\\', num_records=-1, feature_dim=50) rcf.fit(records)\\n</code></pre>\\n<p>I am getting this error:</p>\\n<pre><code>Failed. Reason: ClientError: Unable to read data channel \\'train\\'. Requested content-type is \\'application/x-recordio-protobuf\\'. Please verify the data matches the requested content-type. (caused by MXNetError)\\n</code></pre>\\n<p>Do you have any idea what I am doing wrong? And is it necessary to preprocess data separately in EMR and train in <code>SageMaker</code>, or can I just do everything in SageMaker? (Considering the cost).</p>\\n<p>Tutorial I followed: <a href=\"https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/\" rel=\"nofollow noreferrer\">https://aws.amazon.com/blogs/machine-learning/using-pipe-input-mode-for-amazon-sagemaker-algorithms/</a></p>\\n'},\n",
       "  {'tags': ['python', 'machine-learning', 'regression', 'pycaret'],\n",
       "   'owner': {'account_id': 9698749,\n",
       "    'reputation': 398,\n",
       "    'user_id': 7194375,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/f80994be58351c8c4e639742ad6779c6?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'AldegarRzvan',\n",
       "    'link': 'https://stackoverflow.com/users/7194375/aldegarr%c4%b1zvan'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 258,\n",
       "   'accepted_answer_id': 77521363,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700577882,\n",
       "   'creation_date': 1700501320,\n",
       "   'last_edit_date': 1700524305,\n",
       "   'question_id': 77517864,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77517864/running-setup-works-in-jupyter-lab-but-not-in-vscode-debug-environment-although',\n",
       "   'title': 'Running setup works in jupyter lab but not in vscode debug environment, although using same python path',\n",
       "   'body': \"<p>I am running this code in my jupyterlab and in vscode, it works in jupyterlab but does not in vscode. The error appears in the setup function of pycaret. This is my code:</p>\\n<pre><code>from pycaret.regression(.oop) import * #tried with and without oop\\nfrom pycaret.datasets import get_data\\n\\ndata = get_data('insurance')\\ns = setup(data, target = 'charges', session_id = 123)\\n</code></pre>\\n<p>Unfortunately I do not get any error message from pycaret. It just says:</p>\\n<pre><code>Error:\\n</code></pre>\\n<p>But if I breakpoint before and run pycarets setup code in &quot;Debug Console&quot;, I get the following error:</p>\\n<pre><code>Traceback (most recent call last):\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/gradio/__init__.py&quot;, line 3, in &lt;module&gt;\\n    import gradio._simple_templates\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/gradio/_simple_templates/__init__.py&quot;, line 1, in &lt;module&gt;\\n    from .simpledropdown import SimpleDropdown\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/gradio/_simple_templates/simpledropdown.py&quot;, line 6, in &lt;module&gt;\\n    from gradio.components.base import FormComponent\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/gradio/components/__init__.py&quot;, line 1, in &lt;module&gt;\\n    from gradio.components.annotated_image import AnnotatedImage\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/gradio/components/annotated_image.py&quot;, line 8, in &lt;module&gt;\\n    from gradio_client.documentation import document, set_documentation_group\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/gradio_client/__init__.py&quot;, line 1, in &lt;module&gt;\\n    from gradio_client.client import Client\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/gradio_client/client.py&quot;, line 22, in &lt;module&gt;\\n    import httpx\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/httpx/__init__.py&quot;, line 2, in &lt;module&gt;\\n    from ._api import delete, get, head, options, patch, post, put, request, stream\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/httpx/_api.py&quot;, line 4, in &lt;module&gt;\\n    from ._client import Client\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/httpx/_client.py&quot;, line 30, in &lt;module&gt;\\n    from ._transports.default import AsyncHTTPTransport, HTTPTransport\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/httpx/_transports/default.py&quot;, line 30, in &lt;module&gt;\\n    import httpcore\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/httpcore/__init__.py&quot;, line 1, in &lt;module&gt;\\n    from ._api import request, stream\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/httpcore/_api.py&quot;, line 5, in &lt;module&gt;\\n    from ._sync.connection_pool import ConnectionPool\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/httpcore/_sync/__init__.py&quot;, line 1, in &lt;module&gt;\\n    from .connection import HTTPConnection\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/httpcore/_sync/connection.py&quot;, line 12, in &lt;module&gt;\\n    from .._synchronization import Lock\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/httpcore/_synchronization.py&quot;, line 11, in &lt;module&gt;\\n    import trio\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/trio/__init__.py&quot;, line 20, in &lt;module&gt;\\n    from ._core import TASK_STATUS_IGNORED as TASK_STATUS_IGNORED  # isort: split\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/trio/_core/__init__.py&quot;, line 21, in &lt;module&gt;\\n    from ._local import RunVar, RunVarToken\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/trio/_core/_local.py&quot;, line 9, in &lt;module&gt;\\n    from . import _run\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/trio/_core/_run.py&quot;, line 51, in &lt;module&gt;\\n    from ._multierror import MultiError, concat_tb\\n  File &quot;/home/fx/.local/lib/python3.9/site-packages/trio/_core/_multierror.py&quot;, line 488, in &lt;module&gt;\\n    assert sys.excepthook is apport_python_hook.apport_excepthook\\nAssertionError\\n</code></pre>\\n<p>Also tried to upgrade, but no effect on the behavior:</p>\\n<pre><code>pip install --upgrade gradio httpx trio\\n</code></pre>\\n<p>I do use the same environment (python path is the same), pycaret is installed and can be loaded and displays also version, the version is the most recent one (3.2.0), I installed all dependencies.</p>\\n<pre><code>pip install pycaret[full]\\n</code></pre>\\n<p>Can anybody help me to solve this?</p>\\n\"},\n",
       "  {'tags': ['machine-learning',\n",
       "    'pytorch',\n",
       "    'conv-neural-network',\n",
       "    'deep-residual-networks'],\n",
       "   'owner': {'account_id': 20903243,\n",
       "    'reputation': 1,\n",
       "    'user_id': 15355856,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh4.googleusercontent.com/-kcXjI_iMksY/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclr-Y04aiEgXIN35afFg7kuVihPQw/s96-c/photo.jpg?sz=256',\n",
       "    'display_name': '2Razor007',\n",
       "    'link': 'https://stackoverflow.com/users/15355856/2razor007'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 52,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701017041,\n",
       "   'creation_date': 1700499869,\n",
       "   'last_edit_date': 1700504724,\n",
       "   'question_id': 77517714,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77517714/given-groups-1-weight-of-size-8-3-3-3-expected-input1-1000-28-28-to',\n",
       "   'title': 'Given groups=1, weight of size [8, 3, 3, 3], expected input[1, 1000, 28, 28] to have 3 channels, but got 1000 channels instead',\n",
       "   'body': \"<p>I tried to implement a Residual connection neural network and recreate Lenet-5, and can't set up architecture\\nHere is Residual connection block</p>\\n<pre><code>class ResidualBlock(torch.nn.Module): \\n\\n    def __init__(  \\n        self,\\n        input_c,  #    \\n        output_c,  #    \\n        kernel_size,  #  \\n        activation=torch.nn.ReLU,  #   \\n    ):\\n        super().__init__()\\n        #    \\n        self.activation = activation()\\n\\n        #   ,   h  w   \\n        #  (     )\\n        padding_size = (kernel_size - 1) // 2\\n\\n        #  \\n        self.conv = torch.nn.Conv2d(\\n            in_channels=input_c,\\n            out_channels=output_c,\\n            kernel_size=kernel_size,\\n            padding=padding_size,\\n            padding_mode=&quot;zeros&quot;,\\n        )\\n\\n        #        ,     \\n        #    1\\n        if input_c != output_c:\\n            self.correct_channels = torch.nn.Conv2d(\\n                in_channels=input_c,\\n                out_channels=output_c,\\n                kernel_size=1,\\n            )\\n        else:\\n            self.correct_channels = torch.nn.Identity()\\n</code></pre>\\n<p>Here's architecture</p>\\n<pre><code>class MyModel(torch.nn.Module):\\n    def __init__(self):\\n      super().__init__()\\n      \\n      self.conv1 = ResidualBlock(input_c=1, output_c=6, kernel_size=5,)\\n      self.maxpool1 = torch.nn.MaxPool2d(kernel_size=2)\\n      self.conv2 = ResidualBlock(input_c=6, output_c=16, kernel_size=5)\\n      self.maxpool2 = torch.nn.MaxPool2d(kernel_size=2)\\n      self.conv3 = ResidualBlock(input_c=16, output_c=120, kernel_size=5)\\n\\n      self.lin1 = torch.nn.Linear(in_features=120, out_features=84)\\n      self.relu1 = torch.nn.ReLU()\\n      self.lin2 = torch.nn.Linear(in_features=84, out_features=10)\\n      self.softmax = torch.nn.Softmax()\\n\\n    def forward(self, x):\\n      x = self.activation(self.conv(x) + self.correct_channels(x))\\n      x = self.maxpool1(x)\\n      x = self.activation(self.conv(x) + self.correct_channels(x))\\n      x = self.maxpool2(x)\\n      x = self.activation(self.conv(x) + self.correct_channels(x))\\n\\n      x = self.lin1(x)\\n      x = self.relu1(x)\\n      x = self.lin2(x)\\n      x = self.softmax(x)\\n\\n      return x\\nmodel = MyModel()\\n</code></pre>\\n<p>I tried to change the sizes of the input and output channels, but nothing worked</p>\\n\"},\n",
       "  {'tags': ['machine-learning', 'image-processing', 'ocr', 'mnist'],\n",
       "   'owner': {'account_id': 28672081,\n",
       "    'reputation': 1,\n",
       "    'user_id': 21954940,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/fc34a53f3fbd0538fb5b491d02b04fa4?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'kkwadk',\n",
       "    'link': 'https://stackoverflow.com/users/21954940/kkwadk'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 52,\n",
       "   'answer_count': 1,\n",
       "   'score': -1,\n",
       "   'last_activity_date': 1700593002,\n",
       "   'creation_date': 1700492296,\n",
       "   'last_edit_date': 1700593002,\n",
       "   'question_id': 77516882,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77516882/how-to-remove-borders-in-handwritten-mnist-data',\n",
       "   'title': 'how to remove borders in handwritten mnist data',\n",
       "   'body': '<p><a href=\"https://i.sstatic.net/Lhy4Q.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\\n<p>Like this image, I wanna remove the borders, but I dont know how to remove them,,\\nAnd how can I get an image with only values of 0 and 1 to have values of 0 to 255?</p>\\n<p><a href=\"https://i.sstatic.net/3HunO.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\\n<p>I tried:\\nI put the numbers in the image in the center.\\nCompute the horizontal and vertical projection of every image.\\nUse Gaussian, Contour</p>\\n'},\n",
       "  {'tags': ['python',\n",
       "    'opencv',\n",
       "    'machine-learning',\n",
       "    'image-processing',\n",
       "    'face-detection'],\n",
       "   'owner': {'account_id': 29948527,\n",
       "    'reputation': 11,\n",
       "    'user_id': 22951241,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/017ed0b8e8b9ea1cb59ad5c528e205ae?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'Milena Smoka',\n",
       "    'link': 'https://stackoverflow.com/users/22951241/milena-smo%c5%82ka'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 57,\n",
       "   'answer_count': 1,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1700491382,\n",
       "   'creation_date': 1700482404,\n",
       "   'last_edit_date': 1700491382,\n",
       "   'question_id': 77515830,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77515830/face-detection-using-haarcascades-throws-215assertion-failed-scalefactor-1',\n",
       "   'title': 'Face detection using HaarCascades throws (-215:Assertion failed) scaleFactor &gt; 1 &amp;&amp; _image.depth() == CV_8U in function &#39;detectMultiScale&#39;',\n",
       "   'body': \"<p>I have a problem while using HaarCascades to detect a face in the photo. I get this error:</p>\\n<pre><code>error: OpenCV(4.8.0) /io/opencv/modules/objdetect/src/cascadedetect.cpp:1389: error: (-215:Assertion failed) scaleFactor &gt; 1 &amp;&amp; _image.depth() == CV_8U in function 'detectMultiScale'\\n</code></pre>\\n<p>Here is my function:</p>\\n<pre><code>def face_detector(img):\\n\\n    color_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\\n\\n    face_classifier = cv2.CascadeClassifier(os.path.join(cv2.data.haarcascades, 'haarcascade_frontalface_default.xml'))\\n\\n    scale = 1.3\\n    faces = face_classifier.detectMultiScale(color_img, scale, 5)\\n\\n    if len(faces) == 0:\\n        while scale != 0.0:\\n            x = 0.1\\n            scale = scale - x\\n            faces = face_classifier.detectMultiScale(color_img, scale, 5)\\n            if len(faces) != 0:\\n                for (x, y, w, h) in faces:\\n                    cv2.rectangle(color_img, (x, y), (x+w, y+h), (0, 255, 255), 2)\\n                    roi = color_img[y:y+h, x:x+w]\\n\\n                    cv2_imshow(color_img)\\n                    cv2_imshow(roi)\\n\\n                    return color_img, roi\\n\\n    for (x, y, w, h) in faces:\\n        cv2.rectangle(color_img, (x, y), (x+w, y+h), (0, 255, 255), 2)\\n        roi = color_img[y:y+h, x:x+w]\\n\\n        cv2_imshow(color_img)\\n        cv2_imshow(roi)\\n\\n        return color_img, roi\\n\\n    return color_img, None\\n\\n</code></pre>\\n<p>It happens only sometimes (not for all images). What can I do?</p>\\n\"},\n",
       "  {'tags': ['python', 'machine-learning', 'keras', 'deep-learning'],\n",
       "   'owner': {'account_id': 29946590,\n",
       "    'reputation': 1,\n",
       "    'user_id': 22949739,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/ACg8ocKq0SJGaQ2kwuXtuFjan75aZQmW-brB0n-q5ss5fR7My4E=k-s256',\n",
       "    'display_name': 'cybercalv',\n",
       "    'link': 'https://stackoverflow.com/users/22949739/cybercalv'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 26,\n",
       "   'answer_count': 1,\n",
       "   'score': -2,\n",
       "   'last_activity_date': 1700560767,\n",
       "   'creation_date': 1700468234,\n",
       "   'last_edit_date': 1700524899,\n",
       "   'question_id': 77514332,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77514332/infinity-looping-while-training-dataset-validation-data-dan-training-data-in-dee',\n",
       "   'title': 'infinity looping while training dataset validation_data dan training_data in deep learning',\n",
       "   'body': '<p>I try to input <code>validation_data</code> parameter in <code>model.fit</code> like this:</p>\\n<pre><code>val_generator = data_generator(validation, mapping, features, tokenizer, max_length, vocab_size, batch_size)\\n\\ntrain_generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\\n\\nhistory = model.fit(train_generator,\\n                    epochs=40,\\n                    steps_per_epoch=steps,\\n                    callbacks = callback,\\n                    validation_data = val_generator,\\n                    verbose=1 )\\n</code></pre>\\n<p>and i get a problem is infinity looping. i wanna show val_loss dan val_accuracy from this code as output.</p>\\n<p>i add validation_data = val_generator in code. The result is infinity looping.\\nI want to show val_loss dan val_accuracy from my code.\\ncan u guys help me fix it? please.</p>\\n'},\n",
       "  {'tags': ['python', 'machine-learning', 'google-colaboratory'],\n",
       "   'owner': {'account_id': 27086939,\n",
       "    'reputation': 1,\n",
       "    'user_id': 20635299,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/d7669fde8776d3a477a5b13653167516?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'studentwithproblem',\n",
       "    'link': 'https://stackoverflow.com/users/20635299/studentwithproblem'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 141,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700463425,\n",
       "   'creation_date': 1700400094,\n",
       "   'last_edit_date': 1700463425,\n",
       "   'question_id': 77510907,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77510907/google-colab-modulenotfounderror',\n",
       "   'title': 'Google Colab &quot;ModuleNotFoundError&quot;',\n",
       "   'body': \"<p>Since yesterday I have been trying to run a git project in Google Collab. However, I have the problem that it does not recognize the modules. However, the modules are stored in Drive. With another account without the Pro version, everything worked perfectly.</p>\\n<p>2023-11-19 13:11:41.722619: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\\n2023-11-19 13:11:41.722685: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\\n2023-11-19 13:11:41.722737: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\\n2023-11-19 13:11:43.215387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\\nTraceback (most recent call last):\\nFile &quot;/content/drive/MyDrive/apbench/poisons_generate.py&quot;, line 13, in \\nfrom attack import ar_poisons, dc_poisons, em_poisons, hypo_poisons, lsp_poisons, ops_poisons, rem_poisons, tap_poisons, ntga_poisons\\nFile &quot;/content/drive/MyDrive/apbench/attack/ntga_poisons.py&quot;, line 6, in \\nfrom data.ntga_base.attacks.projected_gradient_descent import projected_gradient_descent\\nModuleNotFoundError: No module named 'data'</p>\\n\"},\n",
       "  {'tags': ['python', 'tensorflow', 'machine-learning', 'loss-function'],\n",
       "   'owner': {'account_id': 27068947,\n",
       "    'reputation': 38,\n",
       "    'user_id': 20619877,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/07291f95c66a3824dc5961388191448b?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'Takeraparterer',\n",
       "    'link': 'https://stackoverflow.com/users/20619877/takeraparterer'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 51,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700365925,\n",
       "   'creation_date': 1700365925,\n",
       "   'question_id': 77509544,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77509544/tensorflow-ml-problem-valueerror-no-gradients-provided-for-any-variable',\n",
       "   'title': 'Tensorflow ML problem: ValueError: No gradients provided for any variable',\n",
       "   'body': \"<p>I am attempting to create an agent to explore a game, and reward it for finding novel viewpoints.\\nI am using TensorFlow for the Machine Learning part. My code is:</p>\\n<pre><code>import vgamepad as vg\\nimport time\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nfrom PIL import Image, ImageChops\\nimport tensorflow as tf\\nimport pyautogui\\ngamepad = vg.VX360Gamepad()\\nimages = []\\n\\ndef updatecontrols(inputs):\\n    cam = (inputs[0],inputs[1])\\n    walk = (inputs[2],inputs[3])\\n    gamepad.right_joystick_float(x_value_float=cam[0], y_value_float=cam[1])\\n    gamepad.left_joystick_float(x_value_float=walk[0], y_value_float=walk[1])\\n    gamepad.update()\\nsize = 256\\nmodel = tf.keras.models.Sequential([\\n  tf.keras.layers.Rescaling(1./255, input_shape=(size, size, 3)),\\n  tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\\n  tf.keras.layers.MaxPooling2D(),\\n  tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\\n  tf.keras.layers.MaxPooling2D(),\\n  tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\\n  tf.keras.layers.MaxPooling2D(),\\n  tf.keras.layers.Flatten(),\\n  tf.keras.layers.Dense(128),\\n  tf.keras.layers.Dense(128),\\n  tf.keras.layers.Dense(128),\\n  tf.keras.layers.Dense(4),\\n])\\ndmax = 10000000\\ndef translate(value, leftMin, leftMax, rightMin, rightMax):\\n    # Figure out how 'wide' each range is\\n    leftSpan = leftMax - leftMin\\n    rightSpan = rightMax - rightMin\\n\\n    # Convert the left range into a 0-1 range (float)\\n    valueScaled = float(value - leftMin) / float(leftSpan)\\n\\n    # Convert the 0-1 range into a value in the right range.\\n    return rightMin + (valueScaled * rightSpan)\\n\\ndef get_novelty(image1):\\n    nov = [0]\\n    for image in images:\\n        diff = ImageChops.difference(image1, image)\\n        \\n        # Convert the difference image to grayscale\\n        diff = diff.convert('L')\\n        \\n        # Get the numpy array of pixel values\\n        diff_array = np.array(diff)\\n        \\n        # Calculate the total pixel-wise difference\\n        total_difference = np.sum(diff_array)\\n        nov.append(total_difference)\\n    return sum(nov)/len(nov)\\n# Take a screenshot and resize it to (size, size)\\n\\n\\noptimizer = tf.keras.optimizers.SGD()\\n\\n\\n\\nepochs = 2000\\nfor epoch in range(epochs):\\n    time.sleep(0.1)\\n    image = pyautogui.screenshot().resize((size, size))\\n    image_array = np.expand_dims(np.asarray(image), axis=0)\\n    # Iterate over the batches of the dataset.\\n    images.append(image)\\n    images = images[-50:]\\n    updatecontrols(model.predict(image_array)[0])\\n    # Open a GradientTape to record the operations run\\n    # during the forward pass, which enables auto-differentiation.\\n    with tf.GradientTape() as tape:\\n        nov = get_novelty(image)\\n        loss = tf.constant(translate(nov,0,dmax,1,0))\\n    # Use the gradient tape to automatically retrieve\\n    # the gradients of the trainable variables with respect to the loss.\\n    grads = tape.gradient(loss, model.trainable_weights)\\n    # Run one step of gradient descent by updating\\n    # the value of the variables to minimize the loss.\\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\\n</code></pre>\\n<p>When I run it, I get a problem on the last line:\\n<code>ValueError: No gradients provided for any variable: (['conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0'],). Provided </code>grads_and_vars<code> is ((None, &lt;tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 16) dtype=float32, numpy= ...</code></p>\\n<p>I am fairly certain the problem is in the calculation of the model's loss:</p>\\n<pre><code>nov = get_novelty(image)\\nloss = tf.constant(translate(nov,0,dmax,1,0))\\n</code></pre>\\n<p>Where I calculate the novelty and then remap it so higher variation leads to lower loss, but am unsure how to fix it. If it worked, it would not give an error.</p>\\n\"},\n",
       "  {'tags': ['python',\n",
       "    'tensorflow',\n",
       "    'machine-learning',\n",
       "    'keras',\n",
       "    'deep-learning'],\n",
       "   'owner': {'account_id': 29933494,\n",
       "    'reputation': 11,\n",
       "    'user_id': 22939794,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/149f4913efad05650001f13803e2e584?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'Alexander Peev',\n",
       "    'link': 'https://stackoverflow.com/users/22939794/alexander-peev'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 46,\n",
       "   'answer_count': 0,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1700331121,\n",
       "   'creation_date': 1700309707,\n",
       "   'last_edit_date': 1700331121,\n",
       "   'question_id': 77506839,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77506839/why-do-i-get-a-notfounderror-when-trying-to-save-my-model-with-model-savemy-m',\n",
       "   'title': 'Why do I get a NotFoundError when trying to save my model with: model.save(&#39;my_model&#39;, save_format=&#39;tf&#39;)',\n",
       "   'body': \"<p>I am using the Tensorflow Keras Subclassing API to Build Dynamic Models, but when I try to save my model I get a NotFoundError.</p>\\n<pre><code>class WideAndDeepModel(tf.keras.Model):\\n    def __init__(self, units=30, activation=&quot;relu&quot;, **kwargs):\\n        super().__init__(**kwargs)  # needed to support naming the model\\n        self.norm_layer_wide = tf.keras.layers.Normalization()\\n        self.norm_layer_deep = tf.keras.layers.Normalization()\\n        self.hidden1 = tf.keras.layers.Dense(units, activation=activation)\\n        self.hidden2 = tf.keras.layers.Dense(units, activation=activation)\\n        self.main_output = tf.keras.layers.Dense(1)\\n        self.aux_output = tf.keras.layers.Dense(1)\\n        \\n    def call(self, inputs):\\n        input_wide, input_deep = inputs\\n        norm_wide = self.norm_layer_wide(input_wide)\\n        norm_deep = self.norm_layer_deep(input_deep)\\n        hidden1 = self.hidden1(norm_deep)\\n        hidden2 = self.hidden2(hidden1)\\n        concat = tf.keras.layers.concatenate([norm_wide, hidden2])\\n        output = self.main_output(concat)\\n        aux_output = self.aux_output(hidden2)\\n        return output, aux_output\\n\\ntf.random.set_seed(42)  # extra code  just for reproducibility\\nmodel = WideAndDeepModel(30, activation=&quot;relu&quot;, name=&quot;my_cool_model&quot;)\\n\\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\\nmodel.compile(loss=&quot;mse&quot;, loss_weights=[0.9, 0.1], optimizer=optimizer,\\n              metrics=[&quot;RootMeanSquaredError&quot;])\\nmodel.norm_layer_wide.adapt(X_train_wide)\\nmodel.norm_layer_deep.adapt(X_train_deep)\\nhistory = model.fit(\\n    (X_train_wide, X_train_deep), (y_train, y_train), epochs=10,\\n    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)))\\neval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))\\nweighted_sum_of_losses, main_loss, aux_loss, main_rmse, aux_rmse = eval_results\\ny_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))\\n\\n</code></pre>\\n<p>but when I try to save my model, using:</p>\\n<pre><code>model.save('my_model', save_format='tf')\\n</code></pre>\\n<p>I get this error:</p>\\n<pre><code>---------------------------------------------------------------------------\\nNotFoundError                             Traceback (most recent call last)\\nCell In[94], line 1\\n----&gt; 1 model.save('my_model', save_format='tf')\\n\\nFile ~\\\\anaconda3\\\\Lib\\\\site-packages\\\\keras\\\\src\\\\utils\\\\traceback_utils.py:70, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)\\n     67     filtered_tb = _process_traceback_frames(e.__traceback__)\\n     68     # To get the full stack trace, call:\\n     69     # `tf.debugging.disable_traceback_filtering()`\\n---&gt; 70     raise e.with_traceback(filtered_tb) from None\\n     71 finally:\\n     72     del filtered_tb\\n\\nFile ~\\\\anaconda3\\\\Lib\\\\site-packages\\\\tensorflow\\\\python\\\\eager\\\\execute.py:53, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\\n     51 try:\\n     52   ctx.ensure_initialized()\\n---&gt; 53   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\\n     54                                       inputs, attrs, num_outputs)\\n     55 except core._NotOkStatusException as e:\\n     56   if name is not None:\\n\\nNotFoundError: {{function_node __wrapped__SaveV2_dtypes_43_device_/job:localhost/replica:0/task:0/device:CPU:0}} Failed to rename: my_model\\\\variables\\\\variables_temp/part-00000-of-00001.data-00000-of-00001.tempstate16756848698604293682 to: my_model\\\\variables\\\\variables_temp/part-00000-of-00001.data-00000-of-00001 : The system cannot find the path specified.\\n; No such process [Op:SaveV2]\\n</code></pre>\\n\"},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'scikit-learn',\n",
       "    'artificial-intelligence',\n",
       "    'loss-function'],\n",
       "   'owner': {'account_id': 15698254,\n",
       "    'reputation': 83,\n",
       "    'user_id': 13559394,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/9505f604409c4aa5c8a20390c8ba19f3?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'Phillip Feldman ',\n",
       "    'link': 'https://stackoverflow.com/users/13559394/phillip-feldman'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 451,\n",
       "   'accepted_answer_id': 77508767,\n",
       "   'answer_count': 1,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1700342768,\n",
       "   'creation_date': 1700280533,\n",
       "   'question_id': 77505666,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77505666/how-do-i-implement-a-custom-error-function-in-a-sklearn-classifier',\n",
       "   'title': 'How do I implement a custom error function in a sklearn classifier?',\n",
       "   'body': \"<p>I want to treat errors from overestimates and underestimates differently (like The Price is Right) during model training. I don't want to rewrite the entire MLP, regression, DecisionTree, etc. algorithms in sklearn just to implement my custom cost function and relevant derivative. Is there a way for me to define a function any classifier can use to override the default? This is an example of what I'm looking for:</p>\\n<pre><code>def myCustomError(y_preds,y_actuals):\\n\\n    #Calculate The price is right style error\\n\\n    return #not MSE\\n\\n\\nfrom sklearn import #Classifier\\n\\n\\nc = #Classifier(loss=myCustomError)\\n</code></pre>\\n<p>If I can't do this in sklearn but I have to use tensorflow or some other libraries, please let me know.</p>\\n\"},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'deep-learning',\n",
       "    'pytorch',\n",
       "    'data-augmentation'],\n",
       "   'owner': {'account_id': 29930699,\n",
       "    'reputation': 1,\n",
       "    'user_id': 22937619,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/ACg8ocLJ-t5hmOjdWrRoE-B2ZZrGVBv3qx-vK2AIYISWQBY=k-s256',\n",
       "    'display_name': 'Prem Gorde',\n",
       "    'link': 'https://stackoverflow.com/users/22937619/prem-gorde'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 1000,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701252970,\n",
       "   'creation_date': 1700265580,\n",
       "   'last_edit_date': 1700334034,\n",
       "   'question_id': 77505170,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77505170/applying-monai-transforms-to-sequences-of-images-uniformly',\n",
       "   'title': 'Applying MONAI transforms to sequences of images uniformly',\n",
       "   'body': \"<p>I am working with a medical dataset in which each datapoint is a sequence of 10 images. I want to apply MONAI transforms (Rand2DElastic, RandRotate, RandZoom, RandGaussianNoise) to these sequences for augmentation. These transforms should be applied randomly to each sequence, however each image in a given sequence should have the exact same transforms for consistency purposes. Is there already existing functionality to do so? If there is not, is there a good way of doing it?\\nData is stored as an np array of shape (n, 10, 3, 128, 128)\\nThanks!</p>\\n<pre><code>images = [Image.open(os.path.join(self.root_dir, image_path)) for image_path in sequence_path]\\nimages = [np.transpose(np.array(image), (2,0,1)) for image in images] \\n\\n        if self.augment:\\n            augment_transforms = Compose([\\n                Rand2DElastic(prob=0.6, spacing=(30, 30), magnitude_range=(0.1, 0.3)),\\n                RandRotate(range_x=np.pi / 60, prob=0.2, keep_size=True),\\n                RandZoom(min_zoom=0.8, max_zoom=1.5, prob=0.6),\\n                RandGaussianNoise(prob=0.5, mean=0, std=0.01)\\n            ])\\n            images = augment_transforms({&quot;image&quot;: np.array(images)})[&quot;image&quot;]\\n            print(images)\\n</code></pre>\\n<p>This is what I tried so far, but didn't give results I needed</p>\\n\"},\n",
       "  {'tags': ['python', 'java', 'machine-learning', 'data-modeling', 'h2o.ai'],\n",
       "   'owner': {'account_id': 25519170,\n",
       "    'reputation': 9,\n",
       "    'user_id': 19309439,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/fcc1d26cb31c5501161b8ee5deb0a8c4?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'lvfmc85',\n",
       "    'link': 'https://stackoverflow.com/users/19309439/lvfmc85'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 622,\n",
       "   'answer_count': 1,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1700523219,\n",
       "   'creation_date': 1700255828,\n",
       "   'last_edit_date': 1700266082,\n",
       "   'question_id': 77504686,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77504686/load-and-run-h2o-mojo-model-without-h2o-cluster',\n",
       "   'title': 'Load and run h2o MOJO model without H2O cluster',\n",
       "   'body': '<p>I have several models generated in h2o on Python. I saved them as mojo files (.zip and h2o-genmodel.jar) using the code:</p>\\n<pre><code>H2O_Model.download_mojo(path=Model_Path, get_genmodel_jar=True)\\n</code></pre>\\n<p>Inside the zip files, there are 2 folders (&quot;domains&quot; and &quot;experimental&quot;) and a model.ini file, which has info regarding the models.</p>\\n<p>I need to run those models in an old raspberry pi 2 to make predictions on some data provided inside CSV files. I can do it by running the models inside the h2o cluster using the following code:</p>\\n<pre><code>#load H2O Model\\nh2o.init()\\n\\n# Download the MOJO\\nmojo_path = model_mojo\\nimported_model = h2o.import_mojo(mojo_path)\\n\\n# Run Model\\nnew_observations = h2o.H2OFrame(df_prever)\\npredictions = imported_model.predict(new_observations)\\n\\n# Save Results\\npredictions = predictions.as_data_frame(predictions)\\ndata_results = pd.concat([df_prever, predictions], axis=1)\\ndata_results = data_results.rename(columns={&quot;predict&quot; : &quot;Predicted_Values&quot;}) \\ndata_results = data_results.infer_objects()\\ndata_results.to_csv(&quot;./Data_Results.csv&quot;, sep=delimiter, decimal=decimal_sep, index=False)\\n</code></pre>\\n<p>And I ended up with a CSV with the prediction results.</p>\\n<p>The problem is that the cluster takes a long time to start in a Raspberry Pi 2. I read that running the models without a cluster is possible, but I couldn\\'t do it.</p>\\n<p>I tried to run the models without starting the cluster with no success. I even tried to let a cluster open at the boot, but it didn\\'t work, and every time I run a script to make predictions, the cluster starts and ends when the script finalizes.</p>\\n<p>Can anyone please help me with that? My models are sorted; I have deep learning, XGboost, StackedEnsemble, GBM, etc.. I have regression, multi-class, and binary classification models, so a flexible way to deal with different models is much appreciated.</p>\\n<p>Below is the complete code I use to load the models with the h2o cluster.</p>\\n<pre><code>import pandas as pd\\nfrom pandas import read_csv\\nimport h2o\\n\\n# User defined Variables\\n\\n# Data file to make predictions\\ncsv_file = &quot;Data.csv&quot;\\n\\n# CSV parameters\\ndelimiter=&quot;;&quot;\\ndecimal_sep=&quot;,&quot;\\nencoding=&quot;ascii&quot;\\n\\n# Model zip file\\nmodel_mojo=&quot;DeepLearning_grid_1_AutoML_1_20231116_204416_model_581.zip&quot;\\n\\n# Load data\\ndf_prever = read_csv(filepath_or_buffer = csv_file, sep=delimiter, decimal=decimal_sep, encoding=encoding)\\ndf_prever = df_prever.reset_index(drop=True)\\n\\n#load H2O Model\\nh2o.init()\\n\\n# Import the MOJO file\\nmojo_path = model_mojo\\nimported_model = h2o.import_mojo(mojo_path)\\n\\n# Run Model\\nnew_observations = h2o.H2OFrame(df_prever)\\npredictions = imported_model.predict(new_observations)\\n\\n# Save Results\\npredictions = predictions.as_data_frame(predictions)\\ndata_results = pd.concat([df_prever, predictions], axis=1)\\ndata_results = data_results.rename(columns={&quot;predict&quot; : &quot;Predicted_Values&quot;}) \\ndata_results = data_results.infer_objects()\\ndata_results.to_csv(&quot;./Data_Results.csv&quot;, sep=delimiter, decimal=decimal_sep, index=False)\\n\\n# Close H2O\\nh2o.cluster().shutdown()\\n</code></pre>\\n<p>I tried to call the model in Java (probably I did it wrong) using the h2o-genmodel.jar and the zip file, but it asks for a &quot;main&quot; file I don\\'t have. I tried to let a cluster open.\\nI read the following documentation:</p>\\n<p><a href=\"https://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html#step-2-compile-and-run-the-mojo\" rel=\"nofollow noreferrer\">h2o-docs/productionizing</a>\\n<a href=\"https://docs.h2o.ai/h2o/latest-stable/h2o-docs/mojo-capabilities.html\" rel=\"nofollow noreferrer\">h2o-docs/mojo-capabilities</a>\\n<a href=\"https://docs.h2o.ai/h2o/latest-stable/h2o-genmodel/javadoc/index.html\" rel=\"nofollow noreferrer\">h2o-genmodel/javadoc</a>\\n<a href=\"https://github.com/h2oai/h2o-tutorials/tree/f67765bc6c68c2058d4b2786d1bbc627d3b70539/tutorials%2Fmojo-resource\" rel=\"nofollow noreferrer\">Tutorial/mojo-resource</a>\\n<a href=\"https://docs.seldon.io/projects/seldon-core/en/latest/examples/h2o_mojo.html\" rel=\"nofollow noreferrer\">examples/h2o_mojo</a></p>\\n'},\n",
       "  {'tags': ['tensorflow', 'machine-learning', 'debugging'],\n",
       "   'owner': {'account_id': 6211782,\n",
       "    'reputation': 1,\n",
       "    'user_id': 4837869,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/58831102fc7ada347909042341ff0567?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'solveForX',\n",
       "    'link': 'https://stackoverflow.com/users/4837869/solveforx'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 72,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700242998,\n",
       "   'creation_date': 1700242998,\n",
       "   'question_id': 77503636,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77503636/identify-bottleneck-while-training-model-using-tensorflow2',\n",
       "   'title': 'Identify bottleneck while training model using tensorflow2',\n",
       "   'body': \"<p>I'm new to Tensorflow. I would like to identify which part of my model training is taking time: whether it's the data load or forward pass through the model.<br />\\nIn pytorch, I could just comment the forward pass,  loss computation and the backward pass, which are under the for loop for the number of epochs, and get an idea of how much time data loader is taking.\\nHowever,  in TF. I'm pretty much stuck or hit a wall with model.fit.generator.</p>\\n<p>Any idea how I can achieve a similar bottleneck analysis?</p>\\n\"},\n",
       "  {'tags': ['machine-learning', 'pytorch', 'gradient-descent', 'autograd'],\n",
       "   'owner': {'account_id': 20095910,\n",
       "    'reputation': 2581,\n",
       "    'user_id': 14735451,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://i.sstatic.net/BlY6t.jpg?s=256',\n",
       "    'display_name': 'Penguin',\n",
       "    'link': 'https://stackoverflow.com/users/14735451/penguin'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 499,\n",
       "   'answer_count': 0,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1700697255,\n",
       "   'creation_date': 1700237271,\n",
       "   'last_edit_date': 1700697255,\n",
       "   'question_id': 77503056,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77503056/how-to-modify-parameters-that-require-gradients-in-meta-learning',\n",
       "   'title': 'How to modify parameters that require gradients in meta learning?',\n",
       "   'body': \"<p>I have a neural network that is trained to output learning rates:</p>\\n<pre><code>import torch \\nimport torch.nn as nn\\nimport torch.optim as optim\\n\\ncriterion = nn.MSELoss()\\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\\n\\nclass Meta_Model(nn.Module):\\n    def __init__(self):\\n        super(Meta_Model, self).__init__()\\n\\n        self.fc1 = nn.Linear(1,32)\\n        self.fc2 = nn.Linear(32,32)\\n        self.fc3 = nn.Linear(32,32)\\n        self.fc4 = nn.Linear(32,1)\\n\\n        self.lky = nn.LeakyReLU(0.1)\\n\\n    def forward(self, x):\\n        x = self.lky(self.fc1(x))\\n        x = self.lky(self.fc2(x))\\n        x = self.lky(self.fc3(x))\\n        x = self.fc4(x)\\n        return x # x should be some learning rate\\n\\nmeta_model = Meta_Model().to(device)\\nmeta_model_opt = optim.Adam(meta_model.parameters(), lr=1e-1)\\n</code></pre>\\n<p>I have some inputs and a function I'm trying to learn:</p>\\n<pre><code>input_tensor = torch.rand(1000,1) # some inputs\\nlabel_tensor = 2 * input_tensor # function to learn\\n</code></pre>\\n<p>I'm trying to update <strong>one trainable parameter</strong> to solve this function:</p>\\n<pre><code>meta_model_epochs = 10\\nw_epochs = 5\\n\\nfor _ in range(meta_model_epochs):\\n    torch.manual_seed(42) # reset seed for reproducibility\\n    w1 = torch.rand(1, requires_grad=True) # reset **trainable weight**\\n    weight_opt = optim.SGD([w1], lr=1e-1) # reset weight optimizer\\n    meta_loss = 0 # reset meta loss\\n    for _ in range(w_epochs):\\n        predicted_tensor = w1 * input_tensor \\n        loss = criterion(predicted_tensor, label_tensor)\\n        meta_loss += loss # add to meta loss\\n        meta_model_output = meta_model(loss.detach().unsqueeze(0)) # input to the meta model is the loss\\n        weight_opt.zero_grad()\\n        loss.backward(retain_graph=True) # get grads\\n\\n        w1 = w1 - meta_model_output * w1.grad # step --&gt; this is the issue\\n    \\n    meta_model_opt.zero_grad()\\n    meta_loss.backward()\\n    meta_model_opt.step()\\n    print('meta_loss', meta_loss.item())\\n</code></pre>\\n<p>So the setting is that the meta model should learn to output the optimal learning rate to update the trainable parameter <code>w1</code> based on the current loss.</p>\\n<p>The issue is that I'm getting <code>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1]] is at version 2; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).</code></p>\\n<p>I also tried replacing the update step with  <code>w1.data = w1.data - meta_model_output * w1.grad # step</code>, which resolves the issue, but then the meta model is not updating (i.e., the loss stays the same)</p>\\n<p><strong>Update 1:</strong></p>\\n<p>Tried @VonC idea of computing the updated value of w1 (using a clone of w1: w1_updated_value) and setting it as the data of w1:</p>\\n<pre><code>w1_clone = w1.clone()\\nw1_clone = w1_clone - meta_model_output * w1.grad # step\\nw1.data = w1_clone\\n</code></pre>\\n<p>While this removes the error, it results in the same issue of the meta model is not updating (i.e., the loss stays the same).</p>\\n<p><strong>Update 2:</strong>\\nAfter lots of readings on buffers and updating leaf tensors I got a solution that updates a whole network:</p>\\n<pre><code>import torch \\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.optim as optim\\nimport functools \\nimport math \\n\\n\\ncriterion = nn.MSELoss()\\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\\n\\nclass Meta_Model(nn.Module):\\n    def __init__(self):\\n        super(Meta_Model, self).__init__()\\n\\n        self.fc1 = nn.Linear(1,32)\\n        self.fc2 = nn.Linear(32,32)\\n        self.fc3 = nn.Linear(32,32)\\n        self.fc4 = nn.Linear(32,1)\\n\\n        self.lky = nn.LeakyReLU(0.1)\\n\\n    def forward(self, x):\\n        x = self.lky(self.fc1(x))\\n        x = self.lky(self.fc2(x))\\n        x = self.lky(self.fc3(x))\\n        x = self.fc4(x)\\n        return x # x should be some learning rate\\n\\nmeta_model = Meta_Model().to(device)\\nmeta_model_opt = optim.Adam(meta_model.parameters(), lr=1e-1)\\ninput_tensor = torch.rand(1000,1) # some inputs\\nlabel_tensor = 2 * input_tensor # function to learn\\nmeta_model_epochs = 10\\nw_epochs = 5\\n\\n############## the new solution\\ndef rsetattr(obj, attr, val):\\n    pre, _, post = attr.rpartition('.')\\n    return setattr(rgetattr(obj, pre) if pre else obj, post, val)\\n\\n# using wonder's beautiful simplification: https://stackoverflow.com/questions/31174295/getattr-and-setattr-on-nested-objects/31174427?noredirect=1#comment86638618_31174427\\n\\ndef rgetattr(obj, attr, *args):\\n    def _getattr(obj, attr):\\n        return getattr(obj, attr, *args)\\n    return functools.reduce(_getattr, [obj] + attr.split('.'))\\n\\nclass MetaLinear(nn.Module):\\n    def __init__(self, in_features, out_features, bias=True):\\n        super().__init__()\\n        # Initialize weights and biases to zero\\n        # The line below is nearly identical to &quot;self.weight = ...&quot;, but we get all of the added PyTorch features.\\n        self.register_buffer('weight', torch.zeros(out_features, in_features, requires_grad=True))\\n        if bias:\\n            self.register_buffer('bias', torch.zeros(out_features, requires_grad=True))\\n        else:\\n            self.bias = None\\n        \\n        # Fancy initialization from https://discuss.pytorch.org/t/how-are-layer-weights-and-biases-initialized-by-default/13073\\n        stdv = 2. / math.sqrt(self.weight.size(1))\\n        self.weight.data.uniform_(-stdv, stdv)\\n        if self.bias is not None:\\n            self.bias.data.uniform_(-stdv, stdv)\\n            \\n    def forward(self, x):\\n        return F.linear(x, self.weight, self.bias)    \\n\\nclass Weights_network(nn.Module):\\n    def __init__(self):\\n        super(Weights_network, self).__init__()\\n\\n        self.fc1 = MetaLinear(1,8, bias=True)\\n        self.fc2 = MetaLinear(8,8, bias=True)\\n        self.fc3 = MetaLinear(8,8, bias=True)\\n        self.fc4 = MetaLinear(8,1, bias=True)\\n\\n    def forward(self, x):\\n        x = torch.relu(self.fc1(x))\\n        x = torch.relu(self.fc2(x))\\n        x = torch.relu(self.fc3(x))\\n        x = torch.relu(self.fc4(x))\\n        return x\\n\\nclass CustomeOptimizer():\\n    def __init__(self, model):\\n        self.named_buffers = model.named_buffers()\\n        self.model = model\\n\\n    def zero_grad(self):\\n        for name, param in self.named_buffers:\\n            if param.grad:\\n                param.grad.zero_()\\n\\n    def step(self, meta_output_lr): \\n        for name, param in self.model.named_buffers():\\n            clipping_value = 1e-2\\n            clipped_gradient = torch.clip(param.grad.detach().clone(), min = -clipping_value, max = clipping_value)\\n            new_param = (param.clone() - meta_output_lr.to(device) * clipped_gradient)            \\n            new_param.retain_grad()\\n            rsetattr(self.model, name, new_param)        \\n##############\\n\\nfor _ in range(meta_model_epochs): # meta training loop\\n    meta_loss = 0 # meta loss\\n    torch.manual_seed(42)\\n    w1 = Weights_network().to(device) # reset network\\n    # maintain grad\\n    for name, param in w1.named_buffers():\\n        param.retain_grad()     \\n    weight_opt = CustomeOptimizer(w1) # reset optimizer\\n    for _ in range(w_epochs): # weights training loop\\n        predicted_tensor = w1(input_tensor)\\n        loss = criterion(predicted_tensor, label_tensor)\\n        meta_loss += loss\\n        meta_model_output = meta_model(loss.detach().unsqueeze(0)).to(device)\\n        weight_opt.zero_grad()\\n        loss.backward(retain_graph=True)\\n        weight_opt.step(meta_model_output)\\n\\n    meta_model_opt.zero_grad()\\n    meta_loss.backward()\\n    meta_model_opt.step()\\n    print('meta_loss', meta_loss.item())\\n&gt;&gt;&gt; \\nmeta_loss 0.9203591346740723\\nmeta_loss 0.4630056917667389\\nmeta_loss 5.195590972900391\\nmeta_loss 0.44623494148254395\\nmeta_loss 0.45180386304855347\\nmeta_loss 0.5693209767341614\\nmeta_loss 0.43741166591644287\\nmeta_loss 0.5331400632858276\\nmeta_loss 0.5698808431625366\\nmeta_loss 0.4637502133846283\\n</code></pre>\\n<p>Though, 1) I'm not sure why this works as the update step seems similar to what I tried. 2) Since I don't understand why this works I can't figure out how to use this on my initial setup where I have a simple parameter <code>w1</code> that is not a whole network</p>\\n<p><strong>Update 3:</strong> Tried @VonC suggestion on wrapping my tensor of weights as a buffer:</p>\\n<pre><code>import torch \\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport functools \\n\\ncriterion = nn.MSELoss()\\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\\n\\nclass Meta_Model(nn.Module):\\n    def __init__(self):\\n        super(Meta_Model, self).__init__()\\n\\n        self.fc1 = nn.Linear(1,32)\\n        self.fc2 = nn.Linear(32,32)\\n        self.fc3 = nn.Linear(32,32)\\n        self.fc4 = nn.Linear(32,1)\\n\\n        self.lky = nn.LeakyReLU(0.1)\\n\\n    def forward(self, x):\\n        x = self.lky(self.fc1(x))\\n        x = self.lky(self.fc2(x))\\n        x = self.lky(self.fc3(x))\\n        x = self.fc4(x)\\n        return x # x should be some learning rate\\n\\nclass Weights(nn.Module):\\n    def __init__(self):\\n        super(Weights, self).__init__()\\n        self.register_buffer('w1', torch.rand(1, requires_grad=True))\\n\\nmeta_model = Meta_Model().to(device)\\nmeta_model_opt = optim.Adam(meta_model.parameters(), lr=1e-1)\\ninput_tensor = torch.rand(1000,1) # some inputs\\nlabel_tensor = 2 * input_tensor # function to learn\\nmeta_model_epochs = 10\\nw_epochs = 5\\n\\nfor _ in range(meta_model_epochs):\\n    torch.manual_seed(42)\\n    w1 = Weights().w1\\n    weight_opt = optim.SGD([w1], lr=0)  # Set learning rate to zero\\n    meta_loss = 0\\n    for _ in range(w_epochs):\\n        predicted_tensor = w1 * input_tensor\\n        loss = criterion(predicted_tensor, label_tensor)\\n        meta_loss += loss\\n        meta_model_output = meta_model(loss.detach().unsqueeze(0))\\n        weight_opt.zero_grad()\\n        loss.backward(retain_graph=True)\\n        # Compute updated value of w1\\n        w1_updated = w1.clone() - meta_model_output * w1.grad\\n        w1_updated.retain_grad()\\n        # Use w1_updated as input to the next iteration\\n        w1 = w1_updated\\n\\n    meta_model_opt.zero_grad()\\n    meta_loss.backward()\\n    meta_model_opt.step()\\n    print('meta_loss', meta_loss.item())\\n</code></pre>\\n<p>But I'm still getting the inplace operation runtime error.</p>\\n\"},\n",
       "  {'tags': ['python',\n",
       "    'numpy',\n",
       "    'tensorflow',\n",
       "    'machine-learning',\n",
       "    'scikit-learn'],\n",
       "   'owner': {'account_id': 24833125,\n",
       "    'reputation': 13,\n",
       "    'user_id': 18712101,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/250fce3c200a77adf2029809ed7a3b77?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'jeongwon bahk',\n",
       "    'link': 'https://stackoverflow.com/users/18712101/jeongwon-bahk'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 46,\n",
       "   'accepted_answer_id': 77509089,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700350904,\n",
       "   'creation_date': 1700227968,\n",
       "   'question_id': 77502062,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77502062/validation-extraction-using-intel-image-classification',\n",
       "   'title': 'validation extraction using intel image classification',\n",
       "   'body': '<p>I\\'m trying to make a machine learning model that operates validation extraction to distinguish whether it\\'s bright or dark.</p>\\n<p>I used the dataset &quot;<a href=\"https://www.kaggle.com/datasets/puneet6060/intel-image-classification\" rel=\"nofollow noreferrer\">intel image classification</a>&quot; from kaggle.</p>\\n<pre><code>import os\\nimport cv2\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score\\n\\n# read image and change to HSV colorspace to extract validation\\ndef extract_brightness(image_path):\\n    image = cv2.imread(image_path)\\n    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\\n    brightness = hsv_image[:,:,2]  # extract validation\\n    return brightness.flatten()\\n\\n# image file directory\\ndirectory = \\'D:\\\\intel_image_classification\\\\seg_train\\\\seg_train/buildings\\' \\n\\nimage_files = []\\n# repeat every file in directory\\nfor filename in os.listdir(directory):\\n    if filename.endswith(\\'.jpg\\') or filename.endswith(\\'.png\\') or filename.endswith(\\'.jpeg\\'):\\n        # jpg, png, jpeg only added in list\\n        file_path = os.path.join(directory, filename)\\n        image_files.append(file_path)\\n\\n\\n\\n# extract validation from each file image\\nbrightness_data = []\\nfor file in image_files:\\n    brightness = extract_brightness(file)\\n    brightness_data.append(brightness)\\nbinarybright_data = brightness_data.copy()\\n# change brightness to bright=1 dark=0 for model training\\nfor i in range(len(binarybright_data)):\\n    if binarybright_data[i].all() &lt; 127.5: #&lt;------ First problem\\n        binarybright_data[i] = 0\\n    else:\\n        binarybright_data[i] = 1\\n\\n\\n# data\\nX = np.array(brightness_data, dtype=object)\\ny = np.array(binarybright_data)  # label of images (0: dark, 1: bright)\\n\\n# divide data to train and test\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# choose machine learning model\\nmodel = LogisticRegression()\\nmodel.fit(X_train, y_train)\\n\\n# prediction using test data\\ny_pred = model.predict(X_test)\\n\\n# accuracy evaluation\\naccuracy = accuracy_score(y_test, y_pred)\\nprint(&quot;Accuracy:&quot;, accuracy)\\n</code></pre>\\n<h1>First Problem</h1>\\n<p>I used 127.5 (which is half of 255) to train whether it\\'s bright or dark.</p>\\n<p>I know it can cause inaccuracy, but there are 14k images only for training so I couldn\\'t label all those images.</p>\\n<p>I would be grateful if you suggest me any other way to label images.</p>\\n<pre><code>for i in range(len(binarybright_data)):\\n    if binarybright_data[i].all() &lt; 127.5: #&lt;------ First problem\\n        binarybright_data[i] = 0\\n    else:\\n        binarybright_data[i] = 1\\n</code></pre>\\n<p>if I don\\'t put \\'.all()\\' method at the end, I get error which is</p>\\n<blockquote>\\n<p>ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()</p>\\n</blockquote>\\n<p>I can\\'t understand why this error comes up even\\n<strong>data is composed of numbers not boolean.</strong></p>\\n<h1>Second Problem</h1>\\n<p>if i use\\'.all()\\' method, then this two errors comes out.</p>\\n<blockquote>\\n<p>TypeError: only size-1 arrays can be converted to Python scalars</p>\\n</blockquote>\\n<blockquote>\\n<p>ValueError: setting an array element with a sequence.</p>\\n</blockquote>\\n<h1></h1>\\n<p>I really appreciate for your help in advance.</p>\\n<p>Hope you have a great day.</p>\\n'},\n",
       "  {'tags': ['python', 'pandas', 'machine-learning', 'scikit-learn'],\n",
       "   'owner': {'account_id': 29215604,\n",
       "    'reputation': 58,\n",
       "    'user_id': 22382498,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://i.sstatic.net/7mnaQ.jpg?s=256',\n",
       "    'display_name': 'Beitian Ma',\n",
       "    'link': 'https://stackoverflow.com/users/22382498/beitian-ma'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 59,\n",
       "   'accepted_answer_id': 77502039,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700360885,\n",
       "   'creation_date': 1700222437,\n",
       "   'last_edit_date': 1700360885,\n",
       "   'question_id': 77501487,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77501487/a-recursive-sample-splitting-scheme-with-grid-searching',\n",
       "   'title': 'A recursive sample splitting scheme (with grid searching)',\n",
       "   'body': \"<p>I have a data panel, there are several samples in each cross-section, for example</p>\\n<pre><code>import pandas as pd\\nimport numpy as np\\n\\ndates = [&quot;2018-01-01&quot;, &quot;2019-01-01&quot;, &quot;2020-01-01&quot;, &quot;2021-01-01&quot;, &quot;2022-01-01&quot;] * 2\\ndates.sort()\\nsamples = [1, 2] * 5\\ndf = pd.DataFrame(\\n    {\\n        &quot;dates&quot;: dates,\\n        &quot;samples&quot;: samples\\n    }\\n)\\n</code></pre>\\n<p>I want to create a cross-validation generator, in which I do validation for 3 times:</p>\\n<ol>\\n<li>The first time, samples in <code>[&quot;2018-01-01&quot;, &quot;2019-01-01&quot;]</code> are the training samples, and in <code>[&quot;2020-01-01&quot;]</code> are the validation samples;</li>\\n<li>The second time, samples in <code>[&quot;2018-01-01&quot;, &quot;2019-01-01&quot;, &quot;2020-01-01&quot;]</code> are the training samples, and in <code>[&quot;2021-01-01&quot;]</code> are the validation samples;</li>\\n<li>The last time, samples in <code>[&quot;2018-01-01&quot;, &quot;2019-01-01&quot;, &quot;2020-01-01&quot;, &quot;2021-01-01&quot;]</code> are the training samples, and in <code>[&quot;2022-01-01&quot;]</code> are the validation samples.</li>\\n</ol>\\n<p>Briefly, the training set recursively increases while the validation set keeps a constant length.</p>\\n<p>I had thought about <code>PredefinedSplit()</code> function from <code>sklearn.model_selection</code>, but the problems are:</p>\\n<ol>\\n<li>As you see, I didn't include all samples (either in the testing set or the validation set) each time;</li>\\n<li><code>[&quot;2020-01-01&quot;]</code> is in training set in the first and second time but not the first time validation.</li>\\n</ol>\\n<p>This makes <code>PredefinedSplit()</code> powerless.</p>\\n<p><strong>My question is</strong>: how to customise this splitting scheme? It is better to keep it in <code>sklearn</code> as I want to pass this splitting scheme into <code>GridSearchCV()</code> for grid searching?</p>\\n\"},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'lstm',\n",
       "    'cross-validation',\n",
       "    'optuna'],\n",
       "   'owner': {'account_id': 16926669,\n",
       "    'reputation': 2342,\n",
       "    'user_id': 12242085,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/d073a2a2f38d392c678ac04b1b6c3c18?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'dingaro',\n",
       "    'link': 'https://stackoverflow.com/users/12242085/dingaro'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 168,\n",
       "   'answer_count': 0,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1700272257,\n",
       "   'creation_date': 1700212224,\n",
       "   'last_edit_date': 1700272257,\n",
       "   'question_id': 77500442,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77500442/how-to-add-cross-validation-to-optuna-function-to-tune-hyperparameters-for-lstm',\n",
       "   'title': 'How to add cross validation to Optuna function to tune hyperparameters for LSTM?',\n",
       "   'body': '<p>I have code to tune hyperparameters in LSTM. How can I:</p>\\n<ol>\\n<li>add cross validation based on 5 folds on training dataset</li>\\n<li>print avg <code>AUC</code> from each iteration from training dataset divided on 5 folds</li>\\n<li>print <code>AUC</code> from test dataset (of course do not divide test dataset on folds):</li>\\n</ol>\\n<pre><code>def objective(trial):\\n    start_time = time.time()\\n    model = create_model(trial)\\n    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, verbose=0)\\n\\n    y_pred = model.predict(X_test)\\n    auc = roc_auc_score(y_test, y_pred)\\n    \\n    end_time = time.time()\\n    elapsed_time = end_time - start_time\\n\\n    print(&quot;iteration no:&quot;, trial.number)\\n    print(&quot;AUC:&quot;, auc)\\n    print(&quot;hyperparameters:&quot;, trial.params)\\n    print(&quot;time:&quot;, elapsed_time, &quot;sec&quot;)\\n\\n    return auc \\n</code></pre>\\n<p>How can I do that in Python?</p>\\n'},\n",
       "  {'tags': ['machine-learning',\n",
       "    'deep-learning',\n",
       "    'pytorch',\n",
       "    'neural-network',\n",
       "    'conv-neural-network'],\n",
       "   'owner': {'account_id': 27120770,\n",
       "    'reputation': 1,\n",
       "    'user_id': 20664512,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/5ba5c38f3489e64d3bbceb889c779ff6?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'Amir',\n",
       "    'link': 'https://stackoverflow.com/users/20664512/amir'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 127,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700213691,\n",
       "   'creation_date': 1700212162,\n",
       "   'last_edit_date': 1700212274,\n",
       "   'question_id': 77500436,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77500436/runtimeerror-mat1-and-mat2-shapes-cannot-be-multiplied-64x1400-and-200x100',\n",
       "   'title': 'RuntimeError: mat1 and mat2 shapes cannot be multiplied (64x1400 and 200x100)',\n",
       "   'body': \"<p>I am working on a multivariate time series forecasting problem using the household_power_consumption.txt dataset and implementing a 1D CNN model. The data is loaded with a batch size of 64, a sequence length of 50, and 7 features. The preprocessing is done. and model code is provided in the below Please help me what is going wrong? with the dimension mismatch of the model.</p>\\n<pre><code>class CNN_ForecastNet(nn.Module):\\n    def __init__(self):\\n        super(CNN_ForecastNet,self).__init__()\\n        self.conv1d = nn.Conv1d(50,200,kernel_size=1)\\n        self.relu = nn.ReLU(inplace=True)\\n        self.drop_out = nn.Dropout(0.5)\\n        self.max_pooling = nn.MaxPool1d(1)\\n        self.fc1 = nn.Linear(200,100)\\n        self.fc2 = nn.Linear(100,1)\\n        \\n    def forward(self,x):\\n        x = self.conv1d(x)\\n        x = self.relu(x)\\n        x = self.drop_out(x)\\n        x = self.max_pooling(x)\\n        x = x.view(x.size(0),-1)\\n        x = self.fc1(x)\\n        x = self.relu(x)\\n        x = self.fc2(x)\\n        \\n        return x\\n    \\n    \\nmodel = CNN_ForecastNet()\\ntrain_losses = []\\nvalid_losses = []\\ndef Train():\\n    \\n    running_loss = .0\\n    \\n    model.train()\\n    \\n    for idx, (inputs,labels) in enumerate(train_loader):\\n        optimizer.zero_grad()\\n        preds = model(inputs.float())\\n        loss = criterion(preds,labels)\\n        loss.backward()\\n        optimizer.step()\\n        running_loss += loss\\n        \\n    train_loss = running_loss/len(train_loader)\\n    train_losses.append(train_loss.detach().numpy())\\n    \\n    print(f'train_loss {train_loss}')\\n    \\ndef Valid():\\n    running_loss = .0\\n    \\n    model.eval()\\n    \\n    with torch.no_grad():\\n        for idx, (inputs, labels) in enumerate(test_loader):\\n            optimizer.zero_grad()\\n            preds = model(inputs.float())\\n            loss = criterion(preds,labels)\\n            running_loss += loss\\n            \\n        valid_loss = running_loss/len(test_loader)\\n        valid_losses.append(valid_loss.detach().numpy())\\n        print(f'valid_loss {valid_loss}')\\n\\nepochs = 10\\nfor epoch in range(epochs):\\n  if epoch % 2==0:\\n\\n    print('epochs {}/{}'.format(epoch+1,epochs))\\n    Train()\\n    Valid()\\n    gc.collect()\\n</code></pre>\\n<p>the error is raised like below</p>\\n<pre><code>Traceback (most recent call last):\\n\\n  Cell In[15], line 73\\n    Train()\\n\\n  Cell In[15], line 41 in Train\\n    preds = model(inputs.float())\\n\\n  File ~\\\\anaconda3\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py:1501 in _call_impl\\n    return forward_call(*args, **kwargs)\\n\\n  Cell In[15], line 17 in forward\\n    x = self.fc1(x)\\n\\n  File ~\\\\anaconda3\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\module.py:1501 in _call_impl\\n    return forward_call(*args, **kwargs)\\n\\n  File ~\\\\anaconda3\\\\Lib\\\\site-packages\\\\torch\\\\nn\\\\modules\\\\linear.py:114 in forward\\n    return F.linear(input, self.weight, self.bias)\\n\\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (64x1400 and 200x100)\\n</code></pre>\\n\"},\n",
       "  {'tags': ['python',\n",
       "    'tensorflow',\n",
       "    'machine-learning',\n",
       "    'keras',\n",
       "    'deep-learning'],\n",
       "   'owner': {'account_id': 7238529,\n",
       "    'reputation': 135,\n",
       "    'user_id': 5522848,\n",
       "    'user_type': 'registered',\n",
       "    'accept_rate': 71,\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/0fd66680ea0950176ca57de8ae4591ba?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'this_is_david',\n",
       "    'link': 'https://stackoverflow.com/users/5522848/this-is-david'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 100,\n",
       "   'accepted_answer_id': 77500050,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700215867,\n",
       "   'creation_date': 1700198508,\n",
       "   'last_edit_date': 1700215867,\n",
       "   'question_id': 77499496,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77499496/incompatible-shapes-when-training-keras-with-custom-loss-function',\n",
       "   'title': 'Incompatible shapes when training Keras with custom loss function',\n",
       "   'body': \"<p>When running the code below I am receiving incompatible shapes from Keras. I have seen several similar questions regarding custom loss functions but none with incompatible shapes. Is this issue arising from my custom loss itself or something deeper in Keras?</p>\\n<p>tensorflow==2.13.0</p>\\n<pre><code>import numpy as np\\nimport pandas as pd\\nfrom tensorflow.keras.layers import Dense\\nfrom tensorflow.keras.models import Sequential\\n\\nN = 1000\\ndf = pd.DataFrame({\\n    'Feature1': np.random.normal(loc=0, scale=1, size=N),\\n    'Feature2': np.random.normal(loc=1, scale=2, size=N),\\n    'Label': np.random.choice([0, 1], size=N)\\n})\\n\\ndf_train = df.sample(frac = 0.80, random_state = 42)\\ndf_test = df[~df.index.isin(df_train.index)]\\nprint(f&quot;df_train.shape = {df_train.shape}&quot;)\\nprint(f&quot;df_test.shape = {df_test.shape}&quot;)\\n\\nX_train, y_train = df_train[['Feature1', 'Feature2']], df_train['Label']\\nX_test, y_test = df_test[['Feature1', 'Feature2']], df_test['Label']\\n\\ndef my_loss(data, y_pred):\\n    y_true = data[:, 0]\\n    amount = data[:, 1]\\n    amount_true = amount * y_true\\n    amount_pred = amount * y_pred\\n    error = amount_pred - amount_true\\n    return sum(error)\\n\\ny_train_plus_amt = np.append(y_train.values.reshape(-1, 1),\\n    X_train['Feature1'].values.reshape(-1, 1), axis = 1)\\n\\nM = Sequential()\\nM.add(Dense(16, input_shape=(X_train.shape[1],), activation = 'relu'))\\nM.compile(optimizer='adam', loss = my_loss, run_eagerly = True)\\nM.fit(X_train, y_train_plus_amt, epochs=10, batch_size=64)\\n\\n\\nTraceback (most recent call last):\\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\\n  File &quot;/Users/venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py&quot;, line 70, in error_handler\\n    raise e.with_traceback(filtered_tb) from None\\n  File &quot;&lt;stdin&gt;&quot;, line 5, in my_loss\\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [64] vs. [64,16] [Op:Mul] name: \\n</code></pre>\\n\"},\n",
       "  {'tags': ['machine-learning',\n",
       "    'pytorch',\n",
       "    'nlp',\n",
       "    'huggingface-transformers',\n",
       "    'huggingface'],\n",
       "   'owner': {'account_id': 1589784,\n",
       "    'reputation': 5516,\n",
       "    'user_id': 1601580,\n",
       "    'user_type': 'registered',\n",
       "    'accept_rate': 47,\n",
       "    'profile_image': 'https://i.sstatic.net/2B6rV.png?s=256',\n",
       "    'display_name': 'Charlie Parker',\n",
       "    'link': 'https://stackoverflow.com/users/1601580/charlie-parker'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 1876,\n",
       "   'answer_count': 1,\n",
       "   'score': 2,\n",
       "   'last_activity_date': 1702642845,\n",
       "   'creation_date': 1700190956,\n",
       "   'last_edit_date': 1701499715,\n",
       "   'question_id': 77499162,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77499162/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-offic',\n",
       "   'title': 'How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model?',\n",
       "   'body': '<p>I want to reinitialize the weights of a LLaMA v2 model I\\'m using/downloading. I went through all the documentation and the source code from their Hugging\\xa0 Face code:</p>\\n<ul>\\n<li><a href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L721\" rel=\"nofollow noreferrer\">https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L721</a></li>\\n<li><a href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L1154\" rel=\"nofollow noreferrer\">https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L1154</a></li>\\n<li><a href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L809\" rel=\"nofollow noreferrer\">https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L809</a></li>\\n<li><a href=\"https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L721\" rel=\"nofollow noreferrer\">https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L721</a></li>\\n<li>docs <a href=\"https://huggingface.co/docs/transformers/main/model_doc/llama#transformers.LlamaModel\" rel=\"nofollow noreferrer\">https://huggingface.co/docs/transformers/main/model_doc/llama#transformers.LlamaModel</a></li>\\n<li>and papers where neither of the two mentions how the initialized the models <strong>exactly</strong> or any of the layers llamav1 <a href=\"https://arxiv.org/pdf/2302.13971.pdf\" rel=\"nofollow noreferrer\">https://arxiv.org/pdf/2302.13971.pdf</a> and llamav2 <a href=\"https://arxiv.org/pdf/2307.09288.pdf\" rel=\"nofollow noreferrer\">https://arxiv.org/pdf/2307.09288.pdf</a> (maybe due to trade secrets?)</li>\\n</ul>\\n<p>I tried the very simple test of going through the modules/parameters and reinitializing according to how their code suggests and printing if the weights norm changed. It never changed, so I don\\'t know if some mutation protection is going on in <a href=\"https://en.wikipedia.org/wiki/PyTorch\" rel=\"nofollow noreferrer\">PyTorch</a> Hugging\\xa0 Face models. Is there something I might be doing wrong?</p>\\n<pre class=\"lang-py prettyprint-override\"><code>import torch\\nfrom transformers import AutoModelForCausalLM, AutoConfig\\nimport torch.nn as nn\\n\\ndef main_reinit_model():\\n    &quot;&quot;&quot;\\n    ref: https://stackoverflow.com/questions/76971761/how-to-adapt-llama-v2-model-to-less-than-7b-parameters\\n    ref: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L721\\n    ref: https://chat.openai.com/c/977d0cb0-b819-48ac-be5c-6e482ad5e518\\n    &quot;&quot;&quot;\\n    print(\\'Starting to reinitialize the model...\\')\\n    # Load the pretrained LLaMA v2 config\\n    config = AutoConfig.from_pretrained(&quot;meta-llama/Llama-2-7b-hf&quot;)\\n    # print(f\\'config: {config} {type(config)}\\')\\n    # Print the original number of parameters\\n    model = AutoModelForCausalLM.from_config(config)\\n    # put model on device cuda\\n    model = model.to(\\'cuda\\')\\n    # print the model\\'s device\\n    print(f\\'{model.device=}\\')\\n    # print(f\\'{model=}\\')\\n    # print(&quot;Original number of parameters:&quot;, sum(p.numel() for p in model.parameters()))\\n    # go through all parameters and compute the l1 norm and sum it then print it\\n    norm_model = sum(p.norm(1) for p in model.parameters())\\n    # loop through modules of model and reinitialize weights with normal_mean, 0.02\\n    print(f\\'{norm_model=}\\')\\n    &quot;&quot;&quot;\\n    go through model and print all laters\\n    &quot;&quot;&quot;\\n    # model.init_weights()  # didn\\'t work\\n    # model._init_weights(module)  # didn\\'t work needs module\\n    # for name, param in model.named_parameters():\\n    #     model._init_weights(param)\\n    # model.post_init()\\n    reinitialize_weights(model)\\n    # model._initialize_weights(module)  # didn\\'t work needs module\\n    # for name, param in model.named_parameters():\\n    #     print(f\\'{name=} {param.shape=}\\')\\n    norm_model = sum(p.norm(1) for p in model.parameters())\\n    print(f\\'{norm_model=}\\')\\n\\ndef reinitialize_weights(model) -&gt; None:\\n    for module in model.modules():\\n        if isinstance(module, nn.Linear):\\n            nn.init.normal_(module.weight, mean=0, std=0.02)\\n            if module.bias is not None:\\n                nn.init.constant_(module.bias, 0)\\n\\ndef _init_weights(self, module):\\n    std = self.config.initializer_range\\n    if isinstance(module, nn.Linear):\\n        module.weight.data.normal_(mean=100.0, std=std)\\n        if module.bias is not None:\\n            module.bias.data.zero_()\\n    elif isinstance(module, nn.Embedding):\\n        module.weight.data.normal_(mean=0.0, std=std)\\n        if module.padding_idx is not None:\\n            module.weight.data[module.padding_idx].zero_()\\n\\ndef main_generate_smaller_model():\\n    &quot;&quot;&quot;\\n    ref: https://stackoverflow.com/questions/76971761/how-to-adapt-llama-v2-model-to-less-than-7b-parameters\\n    &quot;&quot;&quot;\\n    print(\\'Starting to reinitialize the model...\\')\\n    # Load the pretrained LLaMA v2 config\\n    config = AutoConfig.from_pretrained(&quot;meta-llama/Llama-2-7b-hf&quot;)\\n    print(f\\'config: {config} {type(config)}\\')\\n    # Print the original number of parameters\\n    model = AutoModelForCausalLM.from_config(config)\\n    print(&quot;Original number of parameters:&quot;, sum(p.numel() for p in model.parameters()))\\n\\n    # Modify the config to reduce size\\n    config.hidden_size = 2048\\n    config.num_hidden_layers = 12\\n\\n    # Create new smaller model from modified config\\n    smaller_model = AutoModelForCausalLM.from_config(config)\\n    print(&quot;New number of parameters:&quot;, sum(p.numel() for p in smaller_model.parameters()))\\n\\nif __name__ == \\'__main__\\':\\n    import time\\n    start = time.time()\\n    # main_generate_smaller_model()\\n    main_reinit_model()\\n    print(\\'Done!\\\\a\\\\a\\\\a\\')\\n</code></pre>\\n<p>And the output never showed the weight norms changed:</p>\\n<pre><code>Starting to reinitialize the model...\\nmodel.device=device(type=\\'cuda\\', index=0)\\nnorm_model=tensor(1.0779e+08, device=\\'cuda:0\\', grad_fn=&lt;AddBackward0&gt;)\\nnorm_model=tensor(1.0779e+08, device=\\'cuda:0\\', grad_fn=&lt;AddBackward0&gt;)\\nDone!\\n</code></pre>\\n<p>What am I doing wrong? I need to know how to reinitialize the weights in the <strong>proper/correct way</strong> according to LLaMA. What exact init method and values should I use?</p>\\n<hr />\\n<p>Related</p>\\n<ul>\\n<li>related blog: <a href=\"https://blog.briankitano.com/llama-from-scratch/\" rel=\"nofollow noreferrer\">https://blog.briankitano.com/llama-from-scratch/</a></li>\\n<li>Hugging Face question pre-train: <a href=\"https://discuss.huggingface.co/t/can-i-pretrain-llama-from-scratch/37821/7\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/can-i-pretrain-llama-from-scratch/37821/7</a></li>\\n<li>Hugging Face Discord: <a href=\"https://discord.com/channels/879548962464493619/1174911090254172231/1174911090254172231\" rel=\"nofollow noreferrer\">https://discord.com/channels/879548962464493619/1174911090254172231/1174911090254172231</a></li>\\n<li>Hugging Face question reinit: <a href=\"https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547\" rel=\"nofollow noreferrer\">https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547</a></li>\\n<li>related: <a href=\"https://stackoverflow.com/questions/76971761/how-to-adapt-llama-v2-model-to-less-than-7b-parameters\">How to adapt LLaMA v2 model to less than 7B parameters?</a></li>\\n</ul>\\n'},\n",
       "  {'tags': ['tensorflow',\n",
       "    'machine-learning',\n",
       "    'keras',\n",
       "    'keras-tuner',\n",
       "    'tensorflow-addons'],\n",
       "   'owner': {'account_id': 29922790,\n",
       "    'reputation': 1,\n",
       "    'user_id': 22931525,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/b57d4578a8b66d7e77fde562fe100ff9?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'user22931525',\n",
       "    'link': 'https://stackoverflow.com/users/22931525/user22931525'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 397,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1719008163,\n",
       "   'creation_date': 1700185374,\n",
       "   'question_id': 77498936,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77498936/having-trouble-using-f1-score-as-a-metric-in-keras-tuner',\n",
       "   'title': 'Having Trouble Using F1 Score as a Metric in Keras-Tuner',\n",
       "   'body': \"<p>I want to optimize the f1-score for a binary image classification model using keras-tuner. I know the default F1 Score metric is removed for keras, so I tried using Tensorflow Addons' F1Score() class, but it is giving me a KeyError since from what I understand, keras-tuner does not recognize f1-score as a metric.</p>\\n<p>I tried using Tensorflow Addons' F1Score() class as a metric, but that doesn't seem to work.</p>\\n<pre><code>def model_builder(hp):\\n    model = tf.keras.Sequential()\\n    model.add(tf.keras.layers.Rescaling(scale=255))\\n    model.add(tf.keras.layers.TimeDistributed(net))\\n    model.add(tf.keras.layers.Dense(units=hp.Int(\\n        'units', min_value=32, max_value=512, step=32), activation='relu'))\\n    model.add(tf.keras.layers.GlobalAveragePooling3D())\\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\\n\\n    custom_optimizer = keras.optimizers.Adam(\\n        learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]),\\n        beta_1=hp.Choice('beta_1', values=[0.9, 0.99, 0.999]),\\n        beta_2=hp.Choice('beta_2', values=[0.999, 0.9999]),\\n        epsilon=hp.Float('epsilon', min_value=1e-10, max_value=1e-7)\\n    )\\n\\n    # Define metrics\\n    #metrics = [tf.keras.metrics.AUC(), tf.keras.metrics.Recall(), tf.keras.metrics.Precision(), tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalseNegatives(), tf.keras.metrics.FalsePositives()]\\n\\n\\n    # Running with SGD optimizer\\n    model.compile(optimizer='sgd',\\n                  loss=keras.losses.binary_crossentropy, metrics=tfa.metrics.F1Score(num_classes=1, average='macro',threshold=0.5))\\n    \\n    \\n    return model\\n\\n# Initialize the tuner\\ntuner = RandomSearch(\\n    model_builder,\\n    # understand 'objective' should be converted to binary\\n    objective=Objective(tfa.metrics.F1Score(num_classes=1, average='macro',threshold=0.5), direction=max),\\n    max_trials=10,  # Adjust the number of trials as needed\\n    directory='test_directory/logs'\\n)\\n\\n# Start the tuning process\\ntuner.search(train_ds, epochs=10, validation_data=(\\n    val_ds), callbacks=combined)\\n</code></pre>\\n<p>This is the error that my code is outputting:</p>\\n<pre><code>RuntimeError                              Traceback (most recent call last)\\n 3 combined = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\\n      5 # Start the tuning process\\n----&gt; 6 tuner.search(train_ds, epochs=10, validation_data=(\\n      7     val_ds), callbacks=combined)\\n\\nFile ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\keras_tuner\\\\src\\\\engine\\\\base_tuner.py:234, in BaseTuner.search(self, *fit_args, **fit_kwargs)\\n    232     self.on_trial_begin(trial)\\n    233     self._try_run_and_update_trial(trial, *fit_args, **fit_kwargs)\\n--&gt; 234     self.on_trial_end(trial)\\n    235 self.on_search_end()\\n\\nFile ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\keras_tuner\\\\src\\\\engine\\\\base_tuner.py:338, in BaseTuner.on_trial_end(self, trial)\\n    332 def on_trial_end(self, trial):\\n    333     &quot;&quot;&quot;Called at the end of a trial.\\n    334 \\n    335     Args:\\n    336         trial: A `Trial` instance.\\n    337     &quot;&quot;&quot;\\n--&gt; 338     self.oracle.end_trial(trial)\\n    339     self.save()\\n\\nFile ~\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\keras_tuner\\\\src\\\\engine\\\\oracle.py:108, in synchronized.&lt;locals&gt;.wrapped_func(*args, **kwargs)\\n...\\n  File &quot;C:\\\\Users\\\\name_here\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\keras_tuner\\\\src\\\\engine\\\\objective.py&quot;, line 59, in get_value\\n    return logs[self.name]\\n           ~~~~^^^^^^^^^^^\\nKeyError: &lt;tensorflow_addons.metrics.f_scores.F1Score object at 0x000001C8709D6710&gt;\\n</code></pre>\\n<p>I am wondering if there is a workaround to obtain the f1-score from keras' Tuner class. Thank you.</p>\\n\"},\n",
       "  {'tags': ['python', 'numpy', 'tensorflow', 'machine-learning', 'tf-agent'],\n",
       "   'owner': {'account_id': 24192822,\n",
       "    'reputation': 11,\n",
       "    'user_id': 18151820,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/e6db3369e32070dadf5551282fd56c17?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'laezZ_boi',\n",
       "    'link': 'https://stackoverflow.com/users/18151820/laezz-boi'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 57,\n",
       "   'accepted_answer_id': 77551854,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1701004936,\n",
       "   'creation_date': 1700146917,\n",
       "   'question_id': 77495908,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77495908/tf-agent-shape-of-reward-step-type-and-discount',\n",
       "   'title': 'tf agent shape of reward, step_type and discount',\n",
       "   'body': '<p>I am trying to train an agent with tf agent based on the code in <a href=\"https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial\" rel=\"nofollow noreferrer\">this tutorial</a>. I am currently customizing the py_environment for my own use. Other than the code related to the environment, the rest of the code is exactly the same as it is in tutorial.</p>\\n<pre class=\"lang-py prettyprint-override\"><code>def compute_avg_return(environment, policy, num_episodes=10):\\n\\n  total_return = 0.0\\n  for _ in range(num_episodes):\\n\\n    time_step = environment.reset()\\n    episode_return = 0.0\\n\\n    while not time_step.is_last():\\n      action_step = policy.action(time_step) # &lt;----- error on this line\\n      time_step = environment.step(action_step.action)\\n      episode_return += time_step.reward\\n    total_return += episode_return\\n\\n  avg_return = total_return / num_episodes\\n  return avg_return.numpy()[0]\\n\\ncompute_avg_return(eval_env, random_policy, num_eval_episodes)\\n</code></pre>\\n<p>I got the following error when runnning the above code for the first time:</p>\\n<pre><code>ValueError: Received a mix of batched and unbatched Tensors, or Tensors are not compatible with Specs.  num_outer_dims: 1.\\nSaw tensor_shapes:\\n   TimeStep(\\n{\\'discount\\': TensorShape([1]),\\n \\'observation\\': TensorShape([1, 50, 30]),\\n \\'reward\\': TensorShape([1]),\\n \\'step_type\\': TensorShape([1])})\\nAnd spec_shapes:\\n   TimeStep(\\n{\\'discount\\': TensorShape([]),\\n \\'observation\\': TensorShape([1, 50, 30]),\\n \\'reward\\': TensorShape([]),\\n \\'step_type\\': TensorShape([])})\\n</code></pre>\\n<p>From the error log, my shape for observation is correct, but still it says that they are not compatible. So I think the problem is on the shape of \\'discount\\', \\'reward\\' and \\'step_type\\'?</p>\\n<p>But what should I do about it? I can\\'t find anything that show me how to define/alter the shape of these attributes.</p>\\n'},\n",
       "  {'tags': ['flutter', 'machine-learning', 'model'],\n",
       "   'owner': {'account_id': 23260955,\n",
       "    'reputation': 19,\n",
       "    'user_id': 17347215,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a-/AOh14Gg2vXmOw8yjKPcJTXJPyecRghd4rZJtmF6lHpDtSA=k-s256',\n",
       "    'display_name': 'EASY',\n",
       "    'link': 'https://stackoverflow.com/users/17347215/easy'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 66,\n",
       "   'answer_count': 0,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1700140259,\n",
       "   'creation_date': 1700132912,\n",
       "   'last_edit_date': 1700140259,\n",
       "   'question_id': 77494306,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77494306/how-to-use-kobert-model-on-flutter',\n",
       "   'title': 'how to use kobert model on flutter?',\n",
       "   'body': \"<p>I customized kobert model using this code.</p>\\n<pre><code>model = BertForSequenceClassification.from_pretrained('skt/kobert-base-v1', num_labels=6)\\n</code></pre>\\n<p>and I fine-tuned this &quot;model&quot; with my custom train data. Then, I save model with this code.</p>\\n<pre><code>model.save_pretrained('.')\\n</code></pre>\\n<p>I have this files: 'config.json' and 'model.safetensors'.</p>\\n<p>I wanna load my custom kobertmodel on Flutter.</p>\\n<p>How can I do with this files?</p>\\n\"},\n",
       "  {'tags': ['machine-learning', 'knime'],\n",
       "   'owner': {'account_id': 24082596,\n",
       "    'reputation': 1,\n",
       "    'user_id': 18057065,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/a01ca19ee48cd8aceb51ad7a0dc28bb6?s=256&d=identicon&r=PG',\n",
       "    'display_name': 'Harsha',\n",
       "    'link': 'https://stackoverflow.com/users/18057065/harsha'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 1246,\n",
       "   'answer_count': 1,\n",
       "   'score': -1,\n",
       "   'last_activity_date': 1711468985,\n",
       "   'creation_date': 1700128220,\n",
       "   'last_edit_date': 1711468985,\n",
       "   'question_id': 77493779,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77493779/data-preprocessing-to-both-train-and-test-sets',\n",
       "   'title': 'Data preprocessing to both train and test sets?',\n",
       "   'body': \"<p>Need help in understanding the step of data preprocessing while building predictive models.</p>\\n<p>Believe is the scenario:</p>\\n<p>I built decision tree,</p>\\n<p>I Combined files from multiple csvs ; did some steps like creating multiple categories for target variable and also converted numeric variables to categorical and added some new calculated columns , made imputation for input variables</p>\\n<p>Then did partitioning 70 training and 30 test</p>\\n<p>And then I Joined the test 70 percent partitioning to decision tree learner node in knime And joined 30 percent output from partitioning node to a decission tree predictor node and  I got 100 percent accuracy this is wired.</p>\\n<p>After digging I found that all the data preprocessing I did (adding extra calculated columns, imputation, converting categorical columns from numerical) has to be done prior to data partition to get train and test sets.</p>\\n<p>My doubt is if I do it after splitting how can the predictor node know about my changes because I created new target variable column based on numeric column in the dataset. When I do it on test data how can I even use the target variable which doesn't even exist in predictor node.</p>\\n<p>And also read somewhere that all the steps I did before, I must do it on test data as well. Is it really the case? I feel this is too tedious for replicating every data cleaning step on test data.</p>\\n<p>Even if do the cleaning after splitting what difference does it really make.</p>\\n<p>I'm an absolute beginner please help me out and if you could provide answers specific to knime platform it would be much helpful. Thanks in advance</p>\\n<p>Describe everything in the question.</p>\\n\"},\n",
       "  {'tags': ['tensorflow', 'machine-learning', 'keras'],\n",
       "   'owner': {'account_id': 29540437,\n",
       "    'reputation': 1,\n",
       "    'user_id': 22639242,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/ACg8ocJ--0xXHlQRVgqNi2r4wtPcdgPZoFLEGai1H3jWvnPH=k-s256',\n",
       "    'display_name': 'Frederic Strand',\n",
       "    'link': 'https://stackoverflow.com/users/22639242/frederic-strand'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 308,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700127607,\n",
       "   'creation_date': 1700127607,\n",
       "   'question_id': 77493714,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77493714/please-initialize-bidirectional-layer-with-a-tf-keras-layers-layer-instance',\n",
       "   'title': 'Please initialize `Bidirectional` layer with a `tf.keras.layers.Layer` instance',\n",
       "   'body': '<p>I am trying to recreate the SkimLit paper (<a href=\"https://arxiv.org/abs/1612.05251\" rel=\"nofollow noreferrer\">https://arxiv.org/abs/1612.05251</a>) and I am experiencing trouble when trying to Concatenate both token and character level embedding, but also with the line numbers and total line numbers. I\\'ve seen others encountering this problem, but their solutions doesn\\'t work for me. Here\\'s an example: <a href=\"https://github.com/keras-team/tf-keras/issues/556\" rel=\"nofollow noreferrer\">https://github.com/keras-team/tf-keras/issues/556</a>. I would be extremely grateful if anyone could help a lost soul.</p>\\n<p>This is just one hot encoding the data and prefetching it so it\\'ll run faster.</p>\\n<pre><code># Combine chars and tokens into a dataset\\ntrain_token_char_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars))\\ntrain_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\\ntrain_char_token_dataset = tf.data.Dataset.zip((train_token_char_data, train_char_token_labels))\\n\\n# Prefetch and batch train data\\n\\ntrain_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\\n\\n# Combine chars and tokens into a dataset\\n\\nval_token_char_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars))\\nval_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\\nval_char_token_dataset = tf.data.Dataset.zip((val_token_char_data, val_char_token_labels))\\n\\n# Prefetch and batch train data\\n\\nval_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\\\\`\\n\\n# Use tensorflow to create one-hot-encoded tensors of our &quot;line_number&quot; column\\ntrain_line_numbers_one_hot = tf.one_hot(train_df[&quot;line_number&quot;].to_numpy(), depth=20)\\nval_line_numbers_one_hot = tf.one_hot(val_df[&quot;line_number&quot;].to_numpy(), depth=20)\\ntest_line_numbers_one_hot = tf.one_hot(test_df[&quot;line_number&quot;].to_numpy(), depth=20)\\ntrain_line_numbers_one_hot[:5], train_line_numbers_one_hot.shape\\n\\n# Use tensorflow to create one-hot encoded tensors of our &quot;total_lines&quot; feature\\ntrain_total_lines_one_hot = tf.one_hot(train_df[&quot;total_lines&quot;].to_numpy(), depth=20)\\nval_total_lines_one_hot = tf.one_hot(val_df[&quot;total_lines&quot;].to_numpy(), depth=20)\\ntest_total_lines_one_hot = tf.one_hot(test_df[&quot;total_lines&quot;].to_numpy(), depth=20)\\ntrain_total_lines_one_hot[:2], train_total_lines_one_hot.shape`\\n</code></pre>\\n<p>Now for the actual model:</p>\\n<pre><code># 1. Token inputs\\ntoken_inputs = layers.Input(shape=[], dtype=tf.string, name=&quot;token_inputs&quot;)\\ntoken_embeddings = tf_hub_embedding_layer(token_inputs)\\ntoken_outputs = layers.Dense(128, activation=&quot;relu&quot;)(token_embeddings)\\ntoken_model = tf.keras.Model(token_inputs,token_outputs)\\n\\n# 2. Char inputs\\nchar_inputs = layers.Input(shape=(1,), dtype=tf.string, name=&quot;char_inputs&quot;)\\nchar_vectors = char_vectorizer(char_inputs)\\nchar_embedding = char_embed(char_vectors)\\nchar_bi_lstm = layers.Bidirectional(layers.LSTM(24)(char_embeddings))\\nchar_model = tf.keras.Model(char_inputs, char_bi_lstm)\\n\\n# 3. Line number feature\\nline_number_inputs = layers.Input(shape=[], dtype=tf.float32, name=&quot;line_number_inputs&quot;)\\nx = layers.Dense(32, activation=&quot;relu&quot;)(line_numbers_inputs)\\nline_number_model = tf.keras.Model(line_numbers_inputs, x)\\n\\n# 4. Total line number feature\\ntotal_lines_inputs = layers.Input(shape=(20,), dtype=tf.float32, name=&quot;total_lines_inputs&quot;)\\ny = layers.Dense(32, activation=&quot;relu&quot;)(total_lines_inputs)\\ntotal_lines_model = tf.keras.Model(total_lines_inputs, y)\\n\\n# 5. Combine token and char embeddings into a hybrid embedding\\ncombined_embeddings = layers.Concatenate(name=&quot;char_token_hybrid_embedding&quot;)([token_model.output,\\n                                                                             char_model.output])\\n\\nz = layers.Dense(256, activation=&quot;relu&quot;)(combined_embeddings)\\nz = layers.Dropout(0.5)(z)\\n\\n# 6. Combine positional embedding with combined token and char embeddings\\ntribrid_embeddings = layers.Concatenate(name=&quot;char_token_tribrid_embedding&quot;)([line_number_model.output,\\n total_lines_model.ouput,\\n z])\\n\\n# 7. Create output layer\\noutput_layer = layers.Dense(5, activation=&quot;softmax&quot;, name=&quot;output_layer&quot;)(tribrid_embeddings)\\n\\n# 8. Put together model with all kinds of inputs\\nmodel_5 = tf.keras.Model(inputs=[line_number_model.input,\\n                                total_line_model.input,\\n                                token_model.input,\\n                                char_model.input],\\n                        outputs=output_layer)\\n\\n</code></pre>\\n<p>I am getting this error:</p>\\n<pre><code>---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\nCell In[123], line 11\\n      9 char_vectors = char_vectorizer(char_inputs)\\n     10 char_embedding = char_embed(char_vectors)\\n---&gt; 11 char_bi_lstm = layers.Bidirectional(layers.LSTM(24)(char_embeddings))\\n     12 char_model = tf.keras.Model(char_inputs, char_bi_lstm)\\n     14 # 3. Line number feature\\n\\nFile ~/Desktop/skimLit/env/lib/python3.10/site-packages/keras/src/layers/rnn/bidirectional.py:118, in Bidirectional.__init__(self, layer, merge_mode, weights, backward_layer, **kwargs)\\n    109 def __init__(\\n    110     self,\\n    111     layer,\\n   (...)\\n    115     **kwargs,\\n    116 ):\\n    117     if not isinstance(layer, Layer):\\n--&gt; 118         raise ValueError(\\n    119             &quot;Please initialize `Bidirectional` layer with a &quot;\\n    120             f&quot;`tf.keras.layers.Layer` instance. Received: {layer}&quot;\\n    121         )\\n    122     if backward_layer is not None and not isinstance(backward_layer, Layer):\\n    123         raise ValueError(\\n    124             &quot;`backward_layer` need to be a `tf.keras.layers.Layer` &quot;\\n    125             f&quot;instance. Received: {backward_layer}&quot;\\n    126         )\\n\\nValueError: Please initialize `Bidirectional` layer with a `tf.keras.layers.Layer` instance. Received: KerasTensor(type_spec=TensorSpec(shape=(None, 24), dtype=tf.float32, name=None), name=\\'lstm_19/PartitionedCall:0\\', description=&quot;created by layer \\'lstm_19\\'&quot;)\\n</code></pre>\\n'},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'openai-api',\n",
       "    'langchain',\n",
       "    'amazon-bedrock'],\n",
       "   'owner': {'account_id': 22772285,\n",
       "    'reputation': 5,\n",
       "    'user_id': 16929498,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a-/AOh14GjqLwcJzRFhetOPL5gimRec0ErzP88ksks2hKDN=k-s256',\n",
       "    'display_name': 'PlanetX',\n",
       "    'link': 'https://stackoverflow.com/users/16929498/planetx'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 370,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700258988,\n",
       "   'creation_date': 1700126269,\n",
       "   'question_id': 77493561,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77493561/contextualcompressionretriever-get-relevant-documents-is-throwing-error-when-t',\n",
       "   'title': 'ContextualCompressionRetriever.get_relevant_documents() is throwing Error when trying to get relevant documents',\n",
       "   'body': \"<p>So, I have created a compression_retriever as such</p>\\n<pre><code>retriever=db.as_retriever(search_kwargs=search_kwargs)\\ncompressor = LLMChainFilter.from_llm(llm)\\ncompression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\\n</code></pre>\\n<p>Here llm used is:</p>\\n<pre><code>llm = Bedrock(model_id=&quot;anthropic.claude-v2&quot;, client=bedrock_client)\\n</code></pre>\\n<p>Now when I am trying to retrieve the relevant documents using 'answer' in this way</p>\\n<pre><code>compressed_docs = compression_retriever.get_relevant_documents(answer)\\n</code></pre>\\n<p>I am getting an error saying:</p>\\n<pre><code>An error occurred BooleanOutputParser expected output value to either be YES or NO\\n</code></pre>\\n<p>But, with the set the LLM as OpenAI 3.5 it works fine.</p>\\n<p>I tried going through the documentations but no solution seems to pop up.</p>\\n\"},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'deep-learning',\n",
       "    'nlp',\n",
       "    'google-colaboratory'],\n",
       "   'owner': {'account_id': 24196984,\n",
       "    'reputation': 31,\n",
       "    'user_id': 18155359,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a-/AOh14GiIL5HoZY6dupKwEbcuysQ33vInk90HpENPdDezFg=k-s256',\n",
       "    'display_name': 'Bilal Khan',\n",
       "    'link': 'https://stackoverflow.com/users/18155359/bilal-khan'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 9369,\n",
       "   'answer_count': 2,\n",
       "   'score': 3,\n",
       "   'last_activity_date': 1727799967,\n",
       "   'creation_date': 1700071714,\n",
       "   'last_edit_date': 1700088240,\n",
       "   'question_id': 77490008,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77490008/cannot-import-name-typealiastype-from-typing-extensions-usr-local-lib-pyth',\n",
       "   'title': 'cannot import name &#39;TypeAliasType&#39; from &#39;typing_extensions&#39; (/usr/local/lib/python3.10/dist-packages/typing_extensions.py)',\n",
       "   'body': \"<p>I using elevenlab api for voice cloning in google colab. Here is my code</p>\\n<pre><code>import elevenlabs\\n\\nfrom elevenlabs import set_api_key\\n\\nset_api_key(&quot;*****************&quot;)\\n</code></pre>\\n<p>It gives me this error:</p>\\n<blockquote>\\n<p>ImportError: cannot import name 'TypeAliasType' from 'typing_extensions' (/usr/local/lib/python3.10/dist-packages/typing_extensions.py</p>\\n</blockquote>\\n<p>I have changed the version of typing_extensions but it does not work for me.</p>\\n<p>I have changed the version of typing_extension as gpt suggest me to change the version of it. But it was still not working. when I install the new version of typing_extention. Then it gives me this error:</p>\\n<blockquote>\\n<p>ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\\ntensorflow-probability 0.22.0 requires typing-extensions&lt;4.6.0, but you have typing-extensions 4.8.0 which is incompatible.</p>\\n</blockquote>\\n<p>along with installation</p>\\n\"},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'scikit-learn',\n",
       "    'knn',\n",
       "    'gridsearchcv'],\n",
       "   'owner': {'account_id': 29843776,\n",
       "    'reputation': 1,\n",
       "    'user_id': 22920890,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/ACg8ocICEAwHrQQwZn2T575V0KZhxPWVRWU-Jnegbp6_tEwg=k-s256',\n",
       "    'display_name': 'Kousha Zhiyani',\n",
       "    'link': 'https://stackoverflow.com/users/22920890/kousha-zhiyani'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 77,\n",
       "   'answer_count': 1,\n",
       "   'score': -1,\n",
       "   'last_activity_date': 1700072311,\n",
       "   'creation_date': 1700068634,\n",
       "   'last_edit_date': 1700068738,\n",
       "   'question_id': 77489734,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77489734/why-does-manual-search-with-for-result-better-than-using-gridsearchcv',\n",
       "   'title': 'Why does manual search with &quot;for&quot; result better than using GridsearchCV?',\n",
       "   'body': '<p>I read several places to use the GridsearchCV algorithm to find the best hyperparameters, but when in the knn algorithm, I set the number of neighbors to three and use train_test_spilt, it shows a difference of 7057, and when I use GridsearchCV to find the best neighbor, the number is 4. when I use 4, show the difference is 11473\\nHow to increase the difference with the best numbers of neighbor?\\ncode:</p>\\n<pre><code>import pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.neighbors import KNeighborsRegressor\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.metrics import mean_squared_error\\nfrom math import sqrt\\n\\nfile_data = pd.read_csv(r&quot;C:\\\\Users\\\\Ashil-Rayan\\\\Desktop\\\\Data\\\\RankHome\\\\housePricelimit2.csv&quot;, header=None)\\ndata = file_data.drop(6, axis=1)\\ntarget = file_data[6]\\n\\nz = []\\nX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\\n\\n#find best neighbor with for\\n\\nfor i in range(1,10):\\n    knn = KNeighborsRegressor(n_neighbors=i,  weights=&quot;distance&quot;,)\\n    knn.fit(X_train, y_train)\\n    predicts = knn.predict(X_test)\\n    result = mean_squared_error(y_test, predicts)\\n    result = sqrt(result)\\n    score = knn.score(X_test, y_test)\\n    z.append((i, result, score))\\nx = []\\ny = []\\nfor i in z:\\n    x.append(i[0])\\n    y.append(i[1])\\nplt.scatter(x, y)\\nplt.xlabel(&quot;n_neighbors&quot;)\\nplt.ylabel(&quot;mean_squared_error&quot;)\\nplt.show()\\nprint(z[1])\\nprint(z[3])\\n#use GridsearchCV\\n\\nparametrs = {&quot;n_neighbors&quot; : range(1, 50),\\n             &quot;weights&quot; : [&quot;uniform&quot;, &quot;distance&quot;]}\\nGrid = GridSearchCV(KNeighborsRegressor(), parametrs)\\nGrid.fit(X_train, y_train)\\nprint(Grid.best_params_)\\n# print(knn.predict([new_data]))\\nprint(knn.score(X_test, y_test))\\n\\n</code></pre>\\n<p><a href=\"https://i.sstatic.net/8txez.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\\n<p><a href=\"https://i.sstatic.net/lWdI6.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\\n'},\n",
       "  {'tags': ['python',\n",
       "    'tensorflow',\n",
       "    'machine-learning',\n",
       "    'keras',\n",
       "    'tensorflow2.0'],\n",
       "   'owner': {'account_id': 9224014,\n",
       "    'reputation': 173,\n",
       "    'user_id': 6852382,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://i.sstatic.net/03TD1.png?s=256',\n",
       "    'display_name': 'Sime',\n",
       "    'link': 'https://stackoverflow.com/users/6852382/sime'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 188,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700068848,\n",
       "   'creation_date': 1700048004,\n",
       "   'last_edit_date': 1700068848,\n",
       "   'question_id': 77487364,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77487364/replace-tfa-metrics-f1score-from-tf-addon-by-tf-keras-metrics-f1score',\n",
       "   'title': 'Replace tfa.metrics.F1Score from tf addon by tf.keras.metrics.F1Score',\n",
       "   'body': '<p>I have an old Kera model compiled with a custom model with <a href=\"https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/F1Score\" rel=\"nofollow noreferrer\">F1Score</a> from Tensorflow Addon, as is deprecated now, I would like to migrate to <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/metrics/F1Score\" rel=\"nofollow noreferrer\">tf.keras.metrics.F1Score</a> from the model saved.</p>\\n<p>This is what I used now when I load the model:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>   custom_objects = {\\n            \\'F1Score\\': F1Score(num_classes=4, threshold=0.5)\\n   }\\n\\n</code></pre>\\n<p>I tried to replace it by the new function, but I get this message:</p>\\n<pre><code>ValueError: Unable to restore custom object of class &quot;Addons&gt;F1Score&quot; (type _tf_keras_metric). Please make sure that this class is included in the `custom_objects` arg when calling `load_model()`. Also, check that the class implements `get_config` and `from_config`.\\n\\nComplete metadata: {\\'class_name\\': \\'Addons&gt;F1Score\\', \\'name\\': \\'f1_score\\', \\'dtype\\': \\'float32\\', \\'config\\': {\\'name\\': \\'f1_score\\', \\'dtype\\': \\'float32\\', \\'num_classes\\': 4, \\'average\\': None, \\'threshold\\': 0.5}, \\'shared_object_id\\': 32}\\n</code></pre>\\n<p>How can I do for migrating to the new function ?</p>\\n'},\n",
       "  {'tags': ['python', 'machine-learning'],\n",
       "   'owner': {'account_id': 29553781,\n",
       "    'reputation': 7,\n",
       "    'user_id': 22649823,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://i.sstatic.net/0oedc.jpg?s=256',\n",
       "    'display_name': 'Abhijit',\n",
       "    'link': 'https://stackoverflow.com/users/22649823/abhijit'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 100,\n",
       "   'closed_date': 1711494619,\n",
       "   'accepted_answer_id': 77484693,\n",
       "   'answer_count': 1,\n",
       "   'score': -3,\n",
       "   'last_activity_date': 1711469002,\n",
       "   'creation_date': 1700009985,\n",
       "   'last_edit_date': 1711469002,\n",
       "   'question_id': 77484650,\n",
       "   'link': 'https://stackoverflow.com/questions/77484650/how-to-use-to-different-dataset-and-train-it-with-same-model',\n",
       "   'closed_reason': 'Needs more focus',\n",
       "   'title': 'How to use to different dataset and train it with same model',\n",
       "   'body': \"<p>I have two dataset of two different type of ddos attack how do I merge this dataset in one large set to work with? I am trying to train deep learning model on cic ddos 2019 dataset which contains 7 different CSV files of different ddos methods</p>\\n<p>I tried to merged them together but it didn't</p>\\n<p>How do I tarin model based on these 7 different datasets for an ids any solution please suggest</p>\\n\"},\n",
       "  {'tags': ['python', 'tensorflow', 'machine-learning', 'keras'],\n",
       "   'owner': {'account_id': 19700768,\n",
       "    'reputation': 13,\n",
       "    'user_id': 14423582,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/-T6nCBJnJPH8/AAAAAAAAAAI/AAAAAAAAAVQ/AMZuucmPWGpvBOhdil4Gyl45dACje8k6DA/photo.jpg?sz=256',\n",
       "    'display_name': 'Ali a',\n",
       "    'link': 'https://stackoverflow.com/users/14423582/ali-a'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 454,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700042978,\n",
       "   'creation_date': 1700002817,\n",
       "   'last_edit_date': 1700025690,\n",
       "   'question_id': 77484331,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77484331/purpose-of-batch-size-in-image-augmentation-in-tensorflow-keras',\n",
       "   'title': 'purpose of Batch_size in image augmentation in tensorflow keras?',\n",
       "   'body': '<p>I am trying to do image augmentation using keras and tensorflow. However, i couldn\\'t understand the impact of batch_size parameter on the augmented data. I gave 1 image as input, code ran for 3 times, my output were only 3 images (code attached below). I have tried to change batch_size and iterations multiple times, even chatgpt says, i should have 6 images (3 iterations, 2 batch_size) as output in this case.</p>\\n<p>Anyhow, my idea was output image (for one image input) =  batch_size  * 1 (for each iteration)\\nso if batch_size isn\\'t a multiplier per image input, then what is its purpose? or what am i missing?</p>\\n<p>further info regarding code:\\n<a href=\"https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\" rel=\"nofollow noreferrer\">https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html</a></p>\\n<pre><code>from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\\n\\ndatagen = ImageDataGenerator(\\n        rotation_range=40,\\n        width_shift_range=0.2,\\n        height_shift_range=0.2,\\n        shear_range=0.2,\\n        zoom_range=0.2,\\n        horizontal_flip=True,\\n        fill_mode=\\'nearest\\')\\n\\nimg = load_img(\\'/content/library.png\\')  \\nx = img_to_array(img)  \\nx = x.reshape((1,) + x.shape)  \\n\\ni = 0\\nfor batch in datagen.flow(x, batch_size=2,\\n                          save_to_dir=\\'libra\\', save_prefix=\\'lib\\', save_format=\\'jpeg\\'):\\n    i += 1\\n    if i &gt; 2:\\n        break\\n</code></pre>\\n'},\n",
       "  {'tags': ['machine-learning', 'scikit-learn', 'confusion-matrix'],\n",
       "   'owner': {'account_id': 29901109,\n",
       "    'reputation': 1,\n",
       "    'user_id': 22914841,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/ACg8ocKgd0KrT7MiaLD-4uRA8zawuQoxtjQcKZlzg1kujg0q=k-s256',\n",
       "    'display_name': 'Tanner Tolman',\n",
       "    'link': 'https://stackoverflow.com/users/22914841/tanner-tolman'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 96,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1699996026,\n",
       "   'creation_date': 1699974162,\n",
       "   'last_edit_date': 1699996005,\n",
       "   'question_id': 77481666,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77481666/deprecated-confusion-matrix-method',\n",
       "   'title': 'Deprecated confusion_matrix method',\n",
       "   'body': '<p>I\\'m working through a Udemy course and it appears that an aspect of it has not been edited to reflect recent updates. I\\'m attempting to create a Confusion Matrix for a classification problem using Naive Bayes, but I\\'m having trouble getting the material I have to work with the updated functions.</p>\\n<p>The data set is on airline reviews, and I\\'ve trimmed it down to just the text of the review and the labelled sentiment(positive, negative, neutral). Here is the relevant code, with the error I\\'m receiving when just changing the method rather than my arguments. The problem arises in the &quot;report&quot; method.</p>\\n<pre class=\"lang-py prettyprint-override\"><code>from sklearn.model_selection import train_test_split\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,classification_report\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\\ntfidf = TfidfVectorizer(stop_words=\\'english\\')\\ntfidf.fit(X_train)\\nX_train_tfidf = tfidf.transform(X_train)\\nX_test_tfidf = tfidf.transform(X_test)\\n\\nnb = MultinomialNB()\\nnb.fit(X_train_tfidf,y_train)\\n\\ndef report(model):\\n    preds = model.predict(X_test_tfidf)\\n    print(classification_report(y_test,preds))\\n    plot_confusion_matrix(model,X_test_tfidf,y_test)\\n\\nprint(&quot;NB MODEL&quot;)\\nreport(nb)\\n</code></pre>\\n<p>Output:</p>\\n<pre><code>NB MODEL\\n              precision    recall  f1-score   support\\n\\n    negative       0.66      0.99      0.79      1817\\n     neutral       0.79      0.15      0.26       628\\n    positive       0.89      0.14      0.24       483\\n\\n    accuracy                           0.67      2928\\n   macro avg       0.78      0.43      0.43      2928\\nweighted avg       0.73      0.67      0.59      2928\\n\\n\\nTypeError: too many positional arguments\\n</code></pre>\\n<p>Tried to create a confusion matrix using new method, but am receiving an error.</p>\\n'},\n",
       "  {'tags': ['tensorflow', 'machine-learning', 'tf.keras'],\n",
       "   'owner': {'account_id': 6521840,\n",
       "    'reputation': 3,\n",
       "    'user_id': 5045821,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/cd487e18d1fe329f94c8348ae8ab9165?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'Daniel ',\n",
       "    'link': 'https://stackoverflow.com/users/5045821/daniel'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 94,\n",
       "   'accepted_answer_id': 77522408,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700565689,\n",
       "   'creation_date': 1699956845,\n",
       "   'question_id': 77479765,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77479765/padding-the-output-of-a-list-of-2d-arrays-given-by-tf-split-efficiently',\n",
       "   'title': 'Padding the output of a list of 2d arrays given by tf.split efficiently',\n",
       "   'body': \"<p>I am trying to make a physics inspired machine learning model, but I am coming across a strange problem while writing the cost function.</p>\\n<p>The network takes in a vector of length N and outputs a vector of length N, so in essence a multidimensional regression problem.</p>\\n<p>The issue is that irregular subsets of this vector need to mantain a norm of 1, which is part of the cost function. That is to say, the output of the ML model needs to split the vector and take the norm of each split section.\\nLet's say the vector is of length 100, this is split into sub-tensors of length\\n50 30 25 5.\\nthe problem is that it is also a time series, so actually we have an output list of these shapes:\\n[(100,50) (100,30) (100,25) (100,5)].\\nTo efficiently compute the norm without a loop I need to pad the second axis with 0's so that the tensors can be combined and the norms computed in one loop, but this requires an efficient way to transform this into\\n[(100,50), (100,50), (100,50), (100,50)] so that I can stack the dataframes and run the norm computation without a loop.</p>\\n<p>It might be that I am overthinking it and that a loop is fine.</p>\\n<p>At the moment I have this, where the inputs are of shape (100,100) and norm_list contains the lengths of the parts that have to have the norm computed. The reason for the strange splitting is because the actual values are complex, but represented as pairs of reals.</p>\\n<pre><code>def prop_loss_full(y_true,y_pred):\\n    y_truer, y_truec = f2rr_full(y_true)\\n    y_predr, y_predc = f2rr_full(y_pred)\\n    diff = math_ops.squared_difference(y_pred,y_true)\\n    split_real = tf.split(y_predr,norm_list,1)\\n    split_imag = tf.split(y_predc,norm_list,1)\\n    norm = math_ops.reduce_sum((math_ops.square(split_real[0])+math_ops.square(split_imag[0])),axis=1)\\n\\n    for i in range(1,len(split_real)):\\n        norm+=math_ops.reduce_sum((math_ops.square(split_real[i])+math_ops.square(split_imag[i])),axis=1)\\n\\n    return norm+ks.MSE(y_true, y_pred)\\nprop_loss_full(data,data)\\n</code></pre>\\n<p>Any help would be appreciated! When I use tf.pad_sequence I get an error - I assume it doesn't like seuqences of 2d tensors. I have looked far and wide, including ragged tensors and other such features, but have not managed to get them to work.</p>\\n\"},\n",
       "  {'tags': ['machine-learning', 'artificial-intelligence', 'stable-diffusion'],\n",
       "   'owner': {'account_id': 17382214,\n",
       "    'reputation': 1,\n",
       "    'user_id': 12594199,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a-/AAuE7mC0YqVSIsVu9XmMYFH74KXucLHx1tfVn8miOr0Z=k-s256',\n",
       "    'display_name': 'Gaurish Trivedi',\n",
       "    'link': 'https://stackoverflow.com/users/12594199/gaurish-trivedi'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 1777,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700120391,\n",
       "   'creation_date': 1699956218,\n",
       "   'last_edit_date': 1699996657,\n",
       "   'question_id': 77479694,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77479694/adding-lora-to-checkpoint-model',\n",
       "   'title': 'Adding LoRA to Checkpoint Model',\n",
       "   'body': \"<p>I'm still new to the world of Stable Diffusion. I'm trying to LoRA weights to an original model. The approach I came up with is this to get my job done:</p>\\n<pre><code>for i in range(0, 12): \\n model_model[f&quot;cond_stage_model.transformer.text_model.encoder.layers.{i}.mlp.fc1.weight&quot;] +=\\n model_lora[f&quot;lora_te_text_model_encoder_layers_{i}_mlp_fc1.alpha&quot;] *\\ntorch.matmul( \\nmodel_lora[f&quot;lora_te_text_model_encoder_layers_{i}_mlp_fc1.lora_up.weight&quot;].float(), \\nmodel_lora[f&quot;lora_te_text_model_encoder_layers_{i}_mlp_fc1.lora_down.weight&quot;].float()\\n)\\n</code></pre>\\n<p>However, I'm unable to map these LoRA keys to the checkpoint weights as I'm not able to find a similar key in the original model:</p>\\n<pre><code>lora_unet_down_blocks_0_attentions_0_proj_in.alpha\\nlora_unet_down_blocks_0_attentions_0_proj_in.lora_down.weight\\nlora_unet_down_blocks_0_attentions_0_proj_in.lora_up.weight\\n</code></pre>\\n<p>The closest I came to finding a key similar to the above keys are these:</p>\\n<pre><code>model.diffusion_model.input_blocks.0.0.weight\\nmodel.diffusion_model.input_blocks.1.1.proj_in.weight\\n</code></pre>\\n<p>I have made a Stable Diffusion program using the internet and I want to be able to add LoRAs to it in order to create more realistic images for a project purpose. I code be wrong with the way I add LoRAs.</p>\\n\"},\n",
       "  {'tags': ['python',\n",
       "    'tensorflow',\n",
       "    'machine-learning',\n",
       "    'keras',\n",
       "    'keras-tuner'],\n",
       "   'owner': {'account_id': 29898480,\n",
       "    'reputation': 21,\n",
       "    'user_id': 22912757,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/ACg8ocLMU2nXwVlTzYgihFR3Z08cuVoEz240yFMKFtDiP7JqHs4=k-s256',\n",
       "    'display_name': 'ibnfaisalc',\n",
       "    'link': 'https://stackoverflow.com/users/22912757/ibnfaisalc'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 438,\n",
       "   'answer_count': 0,\n",
       "   'score': 2,\n",
       "   'last_activity_date': 1699951946,\n",
       "   'creation_date': 1699951946,\n",
       "   'question_id': 77479267,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77479267/keras-tuner-parallel-processing-using-multi-core-cpu',\n",
       "   'title': 'Keras-Tuner parallel processing using multi-core CPU',\n",
       "   'body': '<p>I am currently running a hyperparameter search in Keras-Tuner using GridSearch. I wanted to get some clarification on whether or not Keras-Tuner can utilize multiple CPU cores/threads to speed up the process of Hyperparameter tuning. I know there is documentation online about utilizing multiple GPUs via tf.distribute, but I am specifically interested in utilizing my Threadripper PRO 3975WX and NVIDIA RTX A6000.</p>\\n<p>To verify, when I run the top command in the terminal during the hyperparameter search, it only shows one process for Python running. I know that when using alternatives, such as scikit, configuring the n_jobs parameter before a search creates multiple python processes running together.</p>\\n<p>Thanks</p>\\n'},\n",
       "  {'tags': ['python', 'machine-learning', 'anaconda', 'conda'],\n",
       "   'owner': {'account_id': 29898067,\n",
       "    'reputation': 31,\n",
       "    'user_id': 22912428,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/faf842537e9f7ad9ac1858a69f230754?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'kobebryan9',\n",
       "    'link': 'https://stackoverflow.com/users/22912428/kobebryan9'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 5167,\n",
       "   'answer_count': 3,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1743463863,\n",
       "   'creation_date': 1699948838,\n",
       "   'last_edit_date': 1743378701,\n",
       "   'question_id': 77479005,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77479005/not-able-to-download-mmcv-1-3-0-and-build-wheels',\n",
       "   'title': 'Not able to download mmcv 1.3.0 and build wheels',\n",
       "   'body': '<p>When I try to install <code>mmcv-full==1.3.0</code>, it couldn\\'t be downloaded and build the wheel (where I have updated to wheel already)</p>\\n<p><a href=\"https://i.sstatic.net/dGOBV.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/dGOBV.png\" alt=\"enter image description here\" /></a></p>\\n<p>error Could not build wheels for mmcv-full, which is required to install pyproject.toml-based projects\\n<a href=\"https://i.sstatic.net/Z0bRh.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/Z0bRh.png\" alt=\"enter image description here\" /></a></p>\\n<p>But then when I try to use\\n<code>mim install mmcv-full</code></p>\\n<p>error message:</p>\\n<pre><code>RROR: Failed building wheel for mmcv-full\\n  Running setup.py clean for mmcv-full\\nFailed to build mmcv-full\\nERROR: Could not build wheels for mmcv-full, which is required to install pyproject.toml-based projects\\n</code></pre>\\n<p>The latest version of <code>mmcv-full</code> could be downloaded, but as the repo I am trying to clone needs to use <code>mmcv version 1.3.0</code>.\\nI am using <code>Windows 11</code> and would like to know how should I download the version.</p>\\n'},\n",
       "  {'tags': ['python', 'machine-learning', 'pytorch', 'local'],\n",
       "   'owner': {'account_id': 29897541,\n",
       "    'reputation': 11,\n",
       "    'user_id': 22912035,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/9875c285a9add114bcfed976a3cf7894?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'Hasith',\n",
       "    'link': 'https://stackoverflow.com/users/22912035/hasith'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 201,\n",
       "   'answer_count': 1,\n",
       "   'score': 1,\n",
       "   'last_activity_date': 1711735760,\n",
       "   'creation_date': 1699946068,\n",
       "   'last_edit_date': 1699960856,\n",
       "   'question_id': 77478747,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77478747/pytorch-installation',\n",
       "   'title': 'Pytorch installation',\n",
       "   'body': '<p>I am trying to install Pytorch in python 3.12.0 and cuda 12.1 in windows 11 ? But I get the error</p>\\n<p>ERROR: could not find a version that satisfies the requirement torch(from version: None)\\nERROR: No matching distribution found for torch</p>\\n<p>I installed the nvidia cuda 12.1 as well</p>\\n<p>I tried to install Pytorch using the Pytorch website but it isnt working and is giving me the error\\nERROR: could not find a version that satisfies the requirement torch(from version: None)\\nERROR: No matching distribution found for torch</p>\\n'},\n",
       "  {'tags': ['python',\n",
       "    'machine-learning',\n",
       "    'deep-learning',\n",
       "    'pytorch',\n",
       "    'computer-vision'],\n",
       "   'owner': {'account_id': 29837780,\n",
       "    'reputation': 13,\n",
       "    'user_id': 22866802,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/ACg8ocLNDlBj5UjNYpaLOqGpQuL9FqxzKoJD0ZZT_wc802jf=k-s256',\n",
       "    'display_name': 'Rishi Suman',\n",
       "    'link': 'https://stackoverflow.com/users/22866802/rishi-suman'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 73,\n",
       "   'answer_count': 0,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1699916271,\n",
       "   'creation_date': 1699903473,\n",
       "   'last_edit_date': 1699916271,\n",
       "   'question_id': 77476359,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77476359/typeerror-img-should-be-pil-image-got-class-dict',\n",
       "   'title': 'TypeError: img should be PIL Image. Got &lt;class &#39;dict&#39;&gt;',\n",
       "   'body': \"<p>I am trying to train my model on my local GPU and it gives an error while the same code runs properly on Google Colab.</p>\\n<pre><code>from datasets import load_dataset\\ndataset = load_dataset(&quot;tglcourse/lsun_church_train&quot;, cache_dir='dataset')\\n\\nimage_size = 256\\nchannels = 3\\nbatch_size = 1\\n\\nfrom torchvision import transforms\\nfrom torch.utils.data import DataLoader\\n\\n# define image transformations (e.g. using torchvision)\\ntransform = Compose([\\n            # transforms.RandomHorizontalFlip(),\\n            Resize(image_size),\\n            CenterCrop(image_size),\\n            transforms.ToTensor(),\\n            transforms.Lambda(lambda t: (t * 2) - 1)\\n])\\n\\n# define function\\ndef transforms(examples):\\n   examples[&quot;pixel_values&quot;] = [transform(image) for image in examples[&quot;image&quot;]]\\n   del examples[&quot;image&quot;]\\n\\n   return examples\\n\\ntransformed_dataset = dataset.with_transform(transforms).remove_columns(&quot;label&quot;)\\n\\n# create dataloader\\ndataloader = DataLoader(transformed_dataset[&quot;train&quot;], batch_size=batch_size, shuffle=True)\\n\\n# print(type(dataloader))\\nbatch = next(iter(dataloader))\\n</code></pre>\\n<p>It gives the error on the last line <code>batch = next(iter(dataloader))</code></p>\\n<p>It says</p>\\n<pre><code>TypeError: img should be PIL Image. Got &lt;class 'dict'&gt;\\n</code></pre>\\n<p>Full Traceback:</p>\\n<pre><code>Traceback (most recent call last):\\n  File &quot;diff_lsun_church.py&quot;, line 486, in &lt;module&gt;\\n    batch = next(iter(dataloader))\\n  File &quot;/home1/rishi_suman/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py&quot;, line 521, in __next__\\n    data = self._next_data()\\n  File &quot;/home1/rishi_suman/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py&quot;, line 561, in _next_data\\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\\n  File &quot;/home1/rishi_suman/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 49, in fetch\\n    data = [self.dataset[idx] for idx in possibly_batched_index]\\n  File &quot;/home1/rishi_suman/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 49, in &lt;listcomp&gt;\\n    data = [self.dataset[idx] for idx in possibly_batched_index]\\n  File &quot;/home1/rishi_suman/.local/lib/python3.6/site-packages/datasets/arrow_dataset.py&quot;, line 2166, in __getitem__\\n    key,\\n  File &quot;/home1/rishi_suman/.local/lib/python3.6/site-packages/datasets/arrow_dataset.py&quot;, line 2151, in _getitem\\n    pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\\n  File &quot;/home1/rishi_suman/.local/lib/python3.6/site-packages/datasets/formatting/formatting.py&quot;, line 532, in format_table\\n    return formatter(pa_table, query_type=query_type)\\n  File &quot;/home1/rishi_suman/.local/lib/python3.6/site-packages/datasets/formatting/formatting.py&quot;, line 281, in __call__\\n    return self.format_row(pa_table)\\n  File &quot;/home1/rishi_suman/.local/lib/python3.6/site-packages/datasets/formatting/formatting.py&quot;, line 387, in format_row\\n    formatted_batch = self.format_batch(pa_table)\\n  File &quot;/home1/rishi_suman/.local/lib/python3.6/site-packages/datasets/formatting/formatting.py&quot;, line 418, in format_batch\\n    return self.transform(batch)\\n  File &quot;diff_lsun_church.py&quot;, line 475, in transforms\\n    examples[&quot;pixel_values&quot;] = [transform(image) for image in examples[&quot;image&quot;]]\\n  File &quot;diff_lsun_church.py&quot;, line 475, in &lt;listcomp&gt;\\n    examples[&quot;pixel_values&quot;] = [transform(image) for image in examples[&quot;image&quot;]]\\n  File &quot;/home1/rishi_suman/.local/lib/python3.6/site-packages/torchvision/transforms/transforms.py&quot;, line 61, in __call__\\n    img = t(img)\\n  File &quot;/home1/rishi_suman/.local/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 1102, in _call_impl\\n    return forward_call(*input, **kwargs)\\n  File &quot;/home1/rishi_suman/.local/lib/python3.6/site-packages/torchvision/transforms/transforms.py&quot;, line 304, in forward\\n    return F.resize(img, self.size, self.interpolation, self.max_size, self.antialias)\\n  File &quot;/home1/rishi_suman/.local/lib/python3.6/site-packages/torchvision/transforms/functional.py&quot;, line 419, in resize\\n    return F_pil.resize(img, size=size, interpolation=pil_interpolation, max_size=max_size)\\n  File &quot;/home1/rishi_suman/.local/lib/python3.6/site-packages/torchvision/transforms/functional_pil.py&quot;, line 233, in resize\\n    raise TypeError('img should be PIL Image. Got {}'.format(type(img)))\\nTypeError: img should be PIL Image. Got &lt;class 'dict'&gt;\\n</code></pre>\\n\"},\n",
       "  {'tags': ['machine-learning',\n",
       "    'pyspark',\n",
       "    'random-forest',\n",
       "    'decision-tree',\n",
       "    'h2o'],\n",
       "   'owner': {'account_id': 168917,\n",
       "    'reputation': 849,\n",
       "    'user_id': 394667,\n",
       "    'user_type': 'registered',\n",
       "    'accept_rate': 62,\n",
       "    'profile_image': 'https://www.gravatar.com/avatar/b7018c9544e5f9429b57048c9eda8338?s=256&d=identicon&r=PG&f=y&so-version=2',\n",
       "    'display_name': 'Vijay Kansal',\n",
       "    'link': 'https://stackoverflow.com/users/394667/vijay-kansal'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 48,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700055555,\n",
       "   'creation_date': 1699898233,\n",
       "   'last_edit_date': 1699916435,\n",
       "   'question_id': 77475890,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77475890/meaning-of-predvalue-field-in-h2o-random-forest-model',\n",
       "   'title': 'Meaning of predValue field in H2O Random forest model',\n",
       "   'body': \"<p>I built an <code>H2ORandomForestEstimator</code> model using <code>train()</code> method on my spark dataframe with the target column containing values 0 or 1. I downloaded and printed its mojo files using <code>model.download_mojo(MOJO_ZIP_PATH)</code> and <code>h2o.print_mojo(MOJO_ZIP_PATH, tree_index=tree_ind)</code> functions respectively. A partial such output tree is shown below.</p>\\n<p>As can be seen, leaf nodes have a field named <code>predValue</code> containing a value between 0 and 1. What is the meaning of this <code>predValue</code> field? Does it mean that the target variable is likely to contain the value contained in <code>predValue</code> field if the input variables happen to meet this root to leaf path when <code>predict()</code> is called on them?</p>\\n<p>Moreover, I want to preprocess the output model of <code>H2ORandomForestEstimator</code> and filter only those rules (root to leaf paths) for which my model will predict 1. Is there a way to filter such rules by parsing the mojo files without actually running the <code>predict()</code> function on input variables? <code>predValue</code> field in the output mojo files looked promising to solve this problem but I could not figure out its co-relation with the output variable. Can it be used to figure out the top-N rules?</p>\\n<pre><code>'trees': [{\\n    'root': {\\n        'nodeNumber': 0,\\n        'weight': 18319.0,\\n        'colId': 169,\\n        'colName': 'pkg_items_gl_product_group_desc_1.gl_electronics',\\n        'leftward': True,\\n        'isCategorical': False,\\n        'inclusiveNa': True,\\n        'splitValue': 0.5,\\n        'rightChild': {\\n            'nodeNumber': 25,\\n            'weight': 462.0,\\n            'predValue': 0.9935065\\n        },\\n        'leftChild': {\\n            'nodeNumber': 1,\\n            'weight': 17857.0,\\n            'colId': 0,\\n            'colName': 'pkg_attr_total_pkg_price',\\n            'leftward': True,\\n            'isCategorical': False,\\n            'inclusiveNa': True,\\n            'splitValue': 186.52805,\\n            'rightChild': {\\n                'nodeNumber': 26,\\n                'weight': 201.0,\\n                'predValue': 0.9900498\\n            },\\n            'leftChild': {\\n                'nodeNumber': 3,\\n                'weight': 13184.0,\\n                'colId': 149,\\n                'colName': 'pkg_items_gl_product_group_desc_1.gl_automotive',\\n                'leftward': True,\\n                'isCategorical': False,\\n                'inclusiveNa': True,\\n                'splitValue': 0.5,\\n                'rightChild': {\\n                    'nodeNumber': 27,\\n                    'weight': 312.0,\\n                    'predValue': 0.99038464\\n                },\\n</code></pre>\\n\"},\n",
       "  {'tags': ['tensorflow',\n",
       "    'machine-learning',\n",
       "    'deep-learning',\n",
       "    'neural-network'],\n",
       "   'owner': {'account_id': 27969917,\n",
       "    'reputation': 13,\n",
       "    'user_id': 21359845,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://lh3.googleusercontent.com/a/AGNmyxYN1v4SwVL1wIcbcAFuRIi1NPI4UeY8N-Zneanr=k-s256',\n",
       "    'display_name': 'Justin Jonany',\n",
       "    'link': 'https://stackoverflow.com/users/21359845/justin-jonany'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 77,\n",
       "   'accepted_answer_id': 77479501,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1711469180,\n",
       "   'creation_date': 1699892570,\n",
       "   'last_edit_date': 1711469180,\n",
       "   'question_id': 77475341,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77475341/confusion-with-tensorflows-sequential-dense',\n",
       "   'title': 'Confusion with tensorflow&#39;s Sequential Dense',\n",
       "   'body': \"<p>I'm working on a regression probem using Tensorflow, and have created two models with slight differences in their first Dense layer.</p>\\n<h1>The Models</h1>\\n<pre><code># Create some regression data\\nX_regression = tf.range(0, 1000, 5)\\ny_regression = tf.range(100, 1100, 5) # Y = X+ 100\\n\\n# Split regression data into training and test sets\\nX_reg_train = X_regression[:150]\\nX_reg_test = X_regression[150:]\\ny_reg_train = y_regression[:150]\\ny_reg_test = y_regression[150:]\\n</code></pre>\\n<p><strong>Model 1</strong></p>\\n<pre><code># Setup random seed\\ntf.random.set_seed(42)\\n\\n\\nmodel_1_reg = tf.keras.Sequential([\\n    tf.keras.layers.Dense(100),\\n    tf.keras.layers.Dense(10),\\n    tf.keras.layers.Dense(1)\\n])\\n\\n\\nmodel_1_reg.compile(loss=tf.keras.losses.mae,\\n                    optimizer=tf.keras.optimizers.Adam(),\\n                    metrics=['mae'])\\n\\nmodel_1_reg.fit(tf.expand_dims(X_reg_train, axis=-1), y_reg_train, epochs=100)\\n</code></pre>\\n<p><strong>Model 2</strong></p>\\n<pre><code># Setup random seed\\ntf.random.set_seed(42)\\n\\n\\nmodel_2_reg = tf.keras.Sequential([\\n    tf.keras.layers.Dense(100, input_shape=(None, 1)),\\n    tf.keras.layers.Dense(10),\\n    tf.keras.layers.Dense(1)\\n])\\n\\n\\nmodel_2_reg.compile(loss=tf.keras.losses.mae,\\n                    optimizer=tf.keras.optimizers.Adam(),\\n                    metrics=['mae'])\\n\\nmodel_2_reg.fit(tf.expand_dims(X_reg_train, axis=-1), y_reg_train, epochs=100)\\n</code></pre>\\n<p>I'm confused about whether I should add the <code>input_shape</code> or not. <strong>Model 1</strong>'s input shape becomes <code>(None, 1)</code> and <strong>Model 2</strong>'s input becomes <code>(None, None, 1)</code>.</p>\\n<p>Both of them run, but perform differently.</p>\\n<p><strong>Model 2</strong> makes sense since we're inputting an array, but if I think about it, does that mean I only have a single node in the input layer? Since I'm giving it a whole ndarray instead of the instances it self. <strong>Model 1</strong> makes sense too since I want to give each number into it.</p>\\n<p>So, which one makes sense more? Or what case should I use each model for?\\nAlso, for <strong>model 2</strong>'s fit why does doing</p>\\n<pre><code>tf.expand_dims(X_reg_train, axis=-1)\\n</code></pre>\\n<p>for the X of</p>\\n<pre><code>model_2_reg.fit(tf.expand_dims(X_reg_train, axis=-1), y_reg_train, epochs=100\\n</code></pre>\\n<p>work? I thought we're suppose to put it in as a batch or like an array of the data so it should be inside an <code>ndarray</code>?</p>\\n\"},\n",
       "  {'tags': ['machine-learning',\n",
       "    'keras',\n",
       "    'neural-network',\n",
       "    'concatenation',\n",
       "    'tensorflow2.0'],\n",
       "   'owner': {'account_id': 28447680,\n",
       "    'reputation': 1,\n",
       "    'user_id': 21766180,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://i.sstatic.net/915SW.jpg?s=256',\n",
       "    'display_name': 'Anthony Belessis',\n",
       "    'link': 'https://stackoverflow.com/users/21766180/anthony-belessis'},\n",
       "   'is_answered': False,\n",
       "   'view_count': 44,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700061785,\n",
       "   'creation_date': 1699874582,\n",
       "   'last_edit_date': 1699907191,\n",
       "   'question_id': 77473519,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77473519/how-can-i-have-two-distinct-inputs-in-keras-layer',\n",
       "   'title': 'How can I have two distinct inputs in keras layer?',\n",
       "   'body': '<p>I am trying to replicate the neural networtk architecture of the <strong>ExpertModel</strong> in AI4Animation (<a href=\"https://github.com/sebastianstarke/AI4Animation/tree/master/AI4Animation/SIGGRAPH_2020/DeepLearning/Models/ExpertModel\" rel=\"nofollow noreferrer\">Github</a>) in TensorFlow 2.</p>\\n<p>I have two neural networks with identical (dense) layers, the only difference being that the first (let\\'s call it the gating network) uses its output to modulate the weights of the second network (by blending the weights). Note that the second network has a distinct input. So I need to have two inputs for the second network, let\\'s call them &quot;<strong>Input</strong>&quot; and &quot;<strong>weight_blend</strong>&quot;. I also need to be able to backpropegate to both x and back to the input of the gating network.</p>\\n<p>Basically what I want is this:</p>\\n<p><img src=\"https://i.sstatic.net/QPal2.png\" alt=\"\" /></p>\\n<p>I first tried to just pass <code>weight_blend</code> as an attribute to some layers. I could define the models of the two networks just fine, however I couldn\\'t connect the two  since keras didn\\'t see weight_blend as an input variable (I think). This makes sense to me, so I tried the following:</p>\\n<p>My second attempt, I tried to pass weight_blend as a second input alongside the main input x. However this turned out to be worse. My gating model is defined just fine, however when I pass the weight_blend variable to the second network, backpropegation fails altogether, not even for the second network\\'s distinct input.</p>\\n<p>This is how I call the custom dense layer where the weight blending happens:</p>\\n<pre><code>def call(self, inputs, *args, **kwargs):\\n\\n    x = inputs[0]\\n    weight_blend = inputs[1]\\n\\n    # get_NNweight/bias just return the initialized weight based on the weight_blend coefficient\\n    self.w = tf.Variable(initial_value=self.experts.get_NNweight(weight_blend, self.batch_size),\\n                         trainable=True)\\n    self.b = tf.Variable(initial_value=self.experts.get_NNbias(weight_blend, self.batch_size),\\n                         trainable=True)\\n\\n    x = tf.matmul(self.w, x) + self.b\\n\\n    acti = self.activation\\n    if acti != 0:\\n        if acti == tf.nn.softmax:\\n            x = acti(x, axis=1)\\n        else:\\n            x = acti(x)\\n    return x\\n</code></pre>\\n<p>This is the gating network (I just pass an initialized version of the weights here):</p>\\n<p><img src=\"https://i.sstatic.net/dsNmd.png\" alt=\"\" /></p>\\n<p>I am very reluctant to use a Concatenate layer, because I am using my two inputs for completely different things, plus the inputs have very different shapes. How can I merge the two networks so I can define the whole thing as one trainable model by just calculating the loss of the main network\\'s output?</p>\\n'},\n",
       "  {'tags': ['machine-learning',\n",
       "    'reinforcement-learning',\n",
       "    'openai-gym',\n",
       "    'stable-baselines',\n",
       "    'multi-agent'],\n",
       "   'owner': {'account_id': 21388141,\n",
       "    'reputation': 21,\n",
       "    'user_id': 22196655,\n",
       "    'user_type': 'registered',\n",
       "    'profile_image': 'https://i.sstatic.net/Hagat.jpg?s=256',\n",
       "    'display_name': 'Hamza',\n",
       "    'link': 'https://stackoverflow.com/users/22196655/hamza'},\n",
       "   'is_answered': True,\n",
       "   'view_count': 86,\n",
       "   'accepted_answer_id': 77488546,\n",
       "   'answer_count': 1,\n",
       "   'score': 0,\n",
       "   'last_activity_date': 1700058424,\n",
       "   'creation_date': 1699866878,\n",
       "   'last_edit_date': 1699882910,\n",
       "   'question_id': 77472802,\n",
       "   'content_license': 'CC BY-SA 4.0',\n",
       "   'link': 'https://stackoverflow.com/questions/77472802/ppo-boid-agent-not-learning',\n",
       "   'title': 'PPO Boid agent not learning',\n",
       "   'body': '<p>I have  a custom Boid flocking environment in OpenAI Gym using PPO from StableBaselines3. I wanted it to achieve flocking similar to <a href=\"https://drive.google.com/file/d/171TRU9uN4XGxpYidldbQ1Swsf6iZp4om/view?usp=drive_link\" rel=\"nofollow noreferrer\">Reynold\\'s model (Video)</a> or close enough, but it ISN\\'T learning.</p>\\n<p><a href=\"https://i.sstatic.net/ktuRb.png\" rel=\"nofollow noreferrer\">Reynold\\'s model</a></p>\\n<p><a href=\"https://drive.google.com/file/d/1ltWM2rDwZU36ipZTDBqLHlCalKdQFk55/view?usp=drive_link\" rel=\"nofollow noreferrer\">My Code</a></p>\\n<p>My results after 100000 timesteps of training:</p>\\n<p><a href=\"https://drive.google.com/file/d/1jAlGrGmpt2nUspBtoZcN7yJLHFQe4CAy/view?usp=drive_link\" rel=\"nofollow noreferrer\">My boids</a></p>\\n<p><a href=\"https://i.sstatic.net/3HcX5.png\" rel=\"nofollow noreferrer\">TensorBoard</a></p>\\n<p>I have adjusted the <code>calculate_reward</code> my model uses to be similar in reward, to encourage Reynold\\'s model like behavior, but  can\\'t see any apparent improvement.</p>\\n'}],\n",
       " 'has_more': True,\n",
       " 'quota_max': 300,\n",
       " 'quota_remaining': 299}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e885db-9074-48b8-becd-2466950ae62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>is_answered</th>\n",
       "      <th>view_count</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>score</th>\n",
       "      <th>last_activity_date</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>last_edit_date</th>\n",
       "      <th>question_id</th>\n",
       "      <th>content_license</th>\n",
       "      <th>...</th>\n",
       "      <th>owner.reputation</th>\n",
       "      <th>owner.user_id</th>\n",
       "      <th>owner.user_type</th>\n",
       "      <th>owner.accept_rate</th>\n",
       "      <th>owner.profile_image</th>\n",
       "      <th>owner.display_name</th>\n",
       "      <th>owner.link</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>closed_date</th>\n",
       "      <th>closed_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[pandas, tensorflow, machine-learning, keras]</td>\n",
       "      <td>False</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1701328929</td>\n",
       "      <td>1701262288</td>\n",
       "      <td>1.701329e+09</td>\n",
       "      <td>77571304</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>661859.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>75.0</td>\n",
       "      <td>https://www.gravatar.com/avatar/50d463f2dfeefb...</td>\n",
       "      <td>Nitul</td>\n",
       "      <td>https://stackoverflow.com/users/661859/nitul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[python, pandas, machine-learning, scikit-lear...</td>\n",
       "      <td>False</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1701357398</td>\n",
       "      <td>1701259008</td>\n",
       "      <td>1.701357e+09</td>\n",
       "      <td>77570948</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22994216.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://lh3.googleusercontent.com/a/ACg8ocKoKQ...</td>\n",
       "      <td>Aditya Shandilya</td>\n",
       "      <td>https://stackoverflow.com/users/22994216/adity...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[machine-learning, large-language-model, llama...</td>\n",
       "      <td>False</td>\n",
       "      <td>343</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1701258961</td>\n",
       "      <td>1701258961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77570944</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>9726166.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.gravatar.com/avatar/2d6c683e736f53...</td>\n",
       "      <td>Muhammad Burhan</td>\n",
       "      <td>https://stackoverflow.com/users/9726166/muhamm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[python, machine-learning, linear-regression, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1701299067</td>\n",
       "      <td>1701248098</td>\n",
       "      <td>1.701299e+09</td>\n",
       "      <td>77569740</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22805791.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://lh3.googleusercontent.com/a/ACg8ocLzlY...</td>\n",
       "      <td>Sagnik Taraphdar</td>\n",
       "      <td>https://stackoverflow.com/users/22805791/sagni...</td>\n",
       "      <td>77570643.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[python, tensorflow, machine-learning, keras, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>158</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1701427187</td>\n",
       "      <td>1701228576</td>\n",
       "      <td>1.701288e+09</td>\n",
       "      <td>77568420</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>13711721.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.gravatar.com/avatar/f7ae41fe88116c...</td>\n",
       "      <td>prynet</td>\n",
       "      <td>https://stackoverflow.com/users/13711721/prynet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[python, machine-learning, deep-learning, pyto...</td>\n",
       "      <td>False</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1699916271</td>\n",
       "      <td>1699903473</td>\n",
       "      <td>1.699916e+09</td>\n",
       "      <td>77476359</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22866802.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://lh3.googleusercontent.com/a/ACg8ocLNDl...</td>\n",
       "      <td>Rishi Suman</td>\n",
       "      <td>https://stackoverflow.com/users/22866802/rishi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[machine-learning, pyspark, random-forest, dec...</td>\n",
       "      <td>False</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1700055555</td>\n",
       "      <td>1699898233</td>\n",
       "      <td>1.699916e+09</td>\n",
       "      <td>77475890</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>849.0</td>\n",
       "      <td>394667.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>62.0</td>\n",
       "      <td>https://www.gravatar.com/avatar/b7018c9544e5f9...</td>\n",
       "      <td>Vijay Kansal</td>\n",
       "      <td>https://stackoverflow.com/users/394667/vijay-k...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[tensorflow, machine-learning, deep-learning, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1711469180</td>\n",
       "      <td>1699892570</td>\n",
       "      <td>1.711469e+09</td>\n",
       "      <td>77475341</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21359845.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://lh3.googleusercontent.com/a/AGNmyxYN1v...</td>\n",
       "      <td>Justin Jonany</td>\n",
       "      <td>https://stackoverflow.com/users/21359845/justi...</td>\n",
       "      <td>77479501.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>[machine-learning, keras, neural-network, conc...</td>\n",
       "      <td>False</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1700061785</td>\n",
       "      <td>1699874582</td>\n",
       "      <td>1.699907e+09</td>\n",
       "      <td>77473519</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21766180.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.sstatic.net/915SW.jpg?s=256</td>\n",
       "      <td>Anthony Belessis</td>\n",
       "      <td>https://stackoverflow.com/users/21766180/antho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>[machine-learning, reinforcement-learning, ope...</td>\n",
       "      <td>True</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1700058424</td>\n",
       "      <td>1699866878</td>\n",
       "      <td>1.699883e+09</td>\n",
       "      <td>77472802</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22196655.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.sstatic.net/Hagat.jpg?s=256</td>\n",
       "      <td>Hamza</td>\n",
       "      <td>https://stackoverflow.com/users/22196655/hamza</td>\n",
       "      <td>77488546.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tags  is_answered  \\\n",
       "0       [pandas, tensorflow, machine-learning, keras]        False   \n",
       "1   [python, pandas, machine-learning, scikit-lear...        False   \n",
       "2   [machine-learning, large-language-model, llama...        False   \n",
       "3   [python, machine-learning, linear-regression, ...         True   \n",
       "4   [python, tensorflow, machine-learning, keras, ...        False   \n",
       "..                                                ...          ...   \n",
       "95  [python, machine-learning, deep-learning, pyto...        False   \n",
       "96  [machine-learning, pyspark, random-forest, dec...        False   \n",
       "97  [tensorflow, machine-learning, deep-learning, ...         True   \n",
       "98  [machine-learning, keras, neural-network, conc...        False   \n",
       "99  [machine-learning, reinforcement-learning, ope...         True   \n",
       "\n",
       "    view_count  answer_count  score  last_activity_date  creation_date  \\\n",
       "0           43             0      0          1701328929     1701262288   \n",
       "1           76             0      0          1701357398     1701259008   \n",
       "2          343             0      1          1701258961     1701258961   \n",
       "3          113             1      0          1701299067     1701248098   \n",
       "4          158             1      0          1701427187     1701228576   \n",
       "..         ...           ...    ...                 ...            ...   \n",
       "95          73             0      0          1699916271     1699903473   \n",
       "96          48             1      0          1700055555     1699898233   \n",
       "97          77             1      0          1711469180     1699892570   \n",
       "98          44             1      0          1700061785     1699874582   \n",
       "99          86             1      0          1700058424     1699866878   \n",
       "\n",
       "    last_edit_date  question_id content_license  ... owner.reputation  \\\n",
       "0     1.701329e+09     77571304    CC BY-SA 4.0  ...           1035.0   \n",
       "1     1.701357e+09     77570948    CC BY-SA 4.0  ...              1.0   \n",
       "2              NaN     77570944    CC BY-SA 4.0  ...             48.0   \n",
       "3     1.701299e+09     77569740    CC BY-SA 4.0  ...             23.0   \n",
       "4     1.701288e+09     77568420    CC BY-SA 4.0  ...             51.0   \n",
       "..             ...          ...             ...  ...              ...   \n",
       "95    1.699916e+09     77476359    CC BY-SA 4.0  ...             13.0   \n",
       "96    1.699916e+09     77475890    CC BY-SA 4.0  ...            849.0   \n",
       "97    1.711469e+09     77475341    CC BY-SA 4.0  ...             13.0   \n",
       "98    1.699907e+09     77473519    CC BY-SA 4.0  ...              1.0   \n",
       "99    1.699883e+09     77472802    CC BY-SA 4.0  ...             21.0   \n",
       "\n",
       "   owner.user_id owner.user_type  owner.accept_rate  \\\n",
       "0       661859.0      registered               75.0   \n",
       "1     22994216.0      registered                NaN   \n",
       "2      9726166.0      registered                NaN   \n",
       "3     22805791.0      registered                NaN   \n",
       "4     13711721.0      registered                NaN   \n",
       "..           ...             ...                ...   \n",
       "95    22866802.0      registered                NaN   \n",
       "96      394667.0      registered               62.0   \n",
       "97    21359845.0      registered                NaN   \n",
       "98    21766180.0      registered                NaN   \n",
       "99    22196655.0      registered                NaN   \n",
       "\n",
       "                                  owner.profile_image  owner.display_name  \\\n",
       "0   https://www.gravatar.com/avatar/50d463f2dfeefb...               Nitul   \n",
       "1   https://lh3.googleusercontent.com/a/ACg8ocKoKQ...    Aditya Shandilya   \n",
       "2   https://www.gravatar.com/avatar/2d6c683e736f53...     Muhammad Burhan   \n",
       "3   https://lh3.googleusercontent.com/a/ACg8ocLzlY...    Sagnik Taraphdar   \n",
       "4   https://www.gravatar.com/avatar/f7ae41fe88116c...              prynet   \n",
       "..                                                ...                 ...   \n",
       "95  https://lh3.googleusercontent.com/a/ACg8ocLNDl...         Rishi Suman   \n",
       "96  https://www.gravatar.com/avatar/b7018c9544e5f9...        Vijay Kansal   \n",
       "97  https://lh3.googleusercontent.com/a/AGNmyxYN1v...       Justin Jonany   \n",
       "98              https://i.sstatic.net/915SW.jpg?s=256    Anthony Belessis   \n",
       "99              https://i.sstatic.net/Hagat.jpg?s=256               Hamza   \n",
       "\n",
       "                                           owner.link  accepted_answer_id  \\\n",
       "0        https://stackoverflow.com/users/661859/nitul                 NaN   \n",
       "1   https://stackoverflow.com/users/22994216/adity...                 NaN   \n",
       "2   https://stackoverflow.com/users/9726166/muhamm...                 NaN   \n",
       "3   https://stackoverflow.com/users/22805791/sagni...          77570643.0   \n",
       "4     https://stackoverflow.com/users/13711721/prynet                 NaN   \n",
       "..                                                ...                 ...   \n",
       "95  https://stackoverflow.com/users/22866802/rishi...                 NaN   \n",
       "96  https://stackoverflow.com/users/394667/vijay-k...                 NaN   \n",
       "97  https://stackoverflow.com/users/21359845/justi...          77479501.0   \n",
       "98  https://stackoverflow.com/users/21766180/antho...                 NaN   \n",
       "99     https://stackoverflow.com/users/22196655/hamza          77488546.0   \n",
       "\n",
       "   closed_date closed_reason  \n",
       "0          NaN           NaN  \n",
       "1          NaN           NaN  \n",
       "2          NaN           NaN  \n",
       "3          NaN           NaN  \n",
       "4          NaN           NaN  \n",
       "..         ...           ...  \n",
       "95         NaN           NaN  \n",
       "96         NaN           NaN  \n",
       "97         NaN           NaN  \n",
       "98         NaN           NaN  \n",
       "99         NaN           NaN  \n",
       "\n",
       "[100 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.json_normalize(response.json(), record_path=[\"items\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e147aa8f-376f-4d85-9c09-f479ef17e5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tags', 'is_answered', 'view_count', 'answer_count', 'score',\n",
       "       'last_activity_date', 'creation_date', 'last_edit_date', 'question_id',\n",
       "       'content_license', 'link', 'title', 'body', 'owner.account_id',\n",
       "       'owner.reputation', 'owner.user_id', 'owner.user_type',\n",
       "       'owner.accept_rate', 'owner.profile_image', 'owner.display_name',\n",
       "       'owner.link', 'accepted_answer_id', 'closed_date', 'closed_reason'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c0ac40-ccc6-4c44-8ba3-0187f8c44d0b",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af330050-f05d-4490-b58a-b5543c8fea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = []\n",
    "\n",
    "for page in range(1,26):  \n",
    "    response = requests.get(\n",
    "        \"https://api.stackexchange.com/2.3/questions\",\n",
    "        params={\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": \"machine_learning\",\n",
    "            \"site\": \"stackoverflow\",\n",
    "            \"pagesize\": 100,\n",
    "            \"page\": 25,\n",
    "            \"filter\": \"withbody\"  \n",
    "        }\n",
    "    )\n",
    "    items = response.json().get(\"items\", [])\n",
    "    all_items.extend(items)\n",
    "    \n",
    "    if not response.json().get(\"has_more\", False):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98240bf8-ab78-4ad9-887f-0b76edbf3969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>is_answered</th>\n",
       "      <th>view_count</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>score</th>\n",
       "      <th>last_activity_date</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>last_edit_date</th>\n",
       "      <th>question_id</th>\n",
       "      <th>content_license</th>\n",
       "      <th>...</th>\n",
       "      <th>owner.reputation</th>\n",
       "      <th>owner.user_id</th>\n",
       "      <th>owner.user_type</th>\n",
       "      <th>owner.accept_rate</th>\n",
       "      <th>owner.profile_image</th>\n",
       "      <th>owner.display_name</th>\n",
       "      <th>owner.link</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>closed_date</th>\n",
       "      <th>closed_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[pandas, tensorflow, machine-learning, keras]</td>\n",
       "      <td>False</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1701328929</td>\n",
       "      <td>1701262288</td>\n",
       "      <td>1.701329e+09</td>\n",
       "      <td>77571304</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>661859.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>75.0</td>\n",
       "      <td>https://www.gravatar.com/avatar/50d463f2dfeefb...</td>\n",
       "      <td>Nitul</td>\n",
       "      <td>https://stackoverflow.com/users/661859/nitul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[python, pandas, machine-learning, scikit-lear...</td>\n",
       "      <td>False</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1701357398</td>\n",
       "      <td>1701259008</td>\n",
       "      <td>1.701357e+09</td>\n",
       "      <td>77570948</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22994216.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://lh3.googleusercontent.com/a/ACg8ocKoKQ...</td>\n",
       "      <td>Aditya Shandilya</td>\n",
       "      <td>https://stackoverflow.com/users/22994216/adity...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[machine-learning, large-language-model, llama...</td>\n",
       "      <td>False</td>\n",
       "      <td>343</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1701258961</td>\n",
       "      <td>1701258961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77570944</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>48.0</td>\n",
       "      <td>9726166.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.gravatar.com/avatar/2d6c683e736f53...</td>\n",
       "      <td>Muhammad Burhan</td>\n",
       "      <td>https://stackoverflow.com/users/9726166/muhamm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[python, machine-learning, linear-regression, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1701299067</td>\n",
       "      <td>1701248098</td>\n",
       "      <td>1.701299e+09</td>\n",
       "      <td>77569740</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22805791.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://lh3.googleusercontent.com/a/ACg8ocLzlY...</td>\n",
       "      <td>Sagnik Taraphdar</td>\n",
       "      <td>https://stackoverflow.com/users/22805791/sagni...</td>\n",
       "      <td>77570643.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[python, tensorflow, machine-learning, keras, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>158</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1701427187</td>\n",
       "      <td>1701228576</td>\n",
       "      <td>1.701288e+09</td>\n",
       "      <td>77568420</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>13711721.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.gravatar.com/avatar/f7ae41fe88116c...</td>\n",
       "      <td>prynet</td>\n",
       "      <td>https://stackoverflow.com/users/13711721/prynet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>[python, machine-learning, deep-learning, pyto...</td>\n",
       "      <td>False</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1699916271</td>\n",
       "      <td>1699903473</td>\n",
       "      <td>1.699916e+09</td>\n",
       "      <td>77476359</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>22866802.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://lh3.googleusercontent.com/a/ACg8ocLNDl...</td>\n",
       "      <td>Rishi Suman</td>\n",
       "      <td>https://stackoverflow.com/users/22866802/rishi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>[machine-learning, pyspark, random-forest, dec...</td>\n",
       "      <td>False</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1700055555</td>\n",
       "      <td>1699898233</td>\n",
       "      <td>1.699916e+09</td>\n",
       "      <td>77475890</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>849.0</td>\n",
       "      <td>394667.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>62.0</td>\n",
       "      <td>https://www.gravatar.com/avatar/b7018c9544e5f9...</td>\n",
       "      <td>Vijay Kansal</td>\n",
       "      <td>https://stackoverflow.com/users/394667/vijay-k...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>[tensorflow, machine-learning, deep-learning, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1711469180</td>\n",
       "      <td>1699892570</td>\n",
       "      <td>1.711469e+09</td>\n",
       "      <td>77475341</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21359845.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://lh3.googleusercontent.com/a/AGNmyxYN1v...</td>\n",
       "      <td>Justin Jonany</td>\n",
       "      <td>https://stackoverflow.com/users/21359845/justi...</td>\n",
       "      <td>77479501.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>[machine-learning, keras, neural-network, conc...</td>\n",
       "      <td>False</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1700061785</td>\n",
       "      <td>1699874582</td>\n",
       "      <td>1.699907e+09</td>\n",
       "      <td>77473519</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21766180.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.sstatic.net/915SW.jpg?s=256</td>\n",
       "      <td>Anthony Belessis</td>\n",
       "      <td>https://stackoverflow.com/users/21766180/antho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>[machine-learning, reinforcement-learning, ope...</td>\n",
       "      <td>True</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1700058424</td>\n",
       "      <td>1699866878</td>\n",
       "      <td>1.699883e+09</td>\n",
       "      <td>77472802</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22196655.0</td>\n",
       "      <td>registered</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.sstatic.net/Hagat.jpg?s=256</td>\n",
       "      <td>Hamza</td>\n",
       "      <td>https://stackoverflow.com/users/22196655/hamza</td>\n",
       "      <td>77488546.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tags  is_answered  \\\n",
       "0         [pandas, tensorflow, machine-learning, keras]        False   \n",
       "1     [python, pandas, machine-learning, scikit-lear...        False   \n",
       "2     [machine-learning, large-language-model, llama...        False   \n",
       "3     [python, machine-learning, linear-regression, ...         True   \n",
       "4     [python, tensorflow, machine-learning, keras, ...        False   \n",
       "...                                                 ...          ...   \n",
       "2495  [python, machine-learning, deep-learning, pyto...        False   \n",
       "2496  [machine-learning, pyspark, random-forest, dec...        False   \n",
       "2497  [tensorflow, machine-learning, deep-learning, ...         True   \n",
       "2498  [machine-learning, keras, neural-network, conc...        False   \n",
       "2499  [machine-learning, reinforcement-learning, ope...         True   \n",
       "\n",
       "      view_count  answer_count  score  last_activity_date  creation_date  \\\n",
       "0             43             0      0          1701328929     1701262288   \n",
       "1             76             0      0          1701357398     1701259008   \n",
       "2            343             0      1          1701258961     1701258961   \n",
       "3            113             1      0          1701299067     1701248098   \n",
       "4            158             1      0          1701427187     1701228576   \n",
       "...          ...           ...    ...                 ...            ...   \n",
       "2495          73             0      0          1699916271     1699903473   \n",
       "2496          48             1      0          1700055555     1699898233   \n",
       "2497          77             1      0          1711469180     1699892570   \n",
       "2498          44             1      0          1700061785     1699874582   \n",
       "2499          86             1      0          1700058424     1699866878   \n",
       "\n",
       "      last_edit_date  question_id content_license  ... owner.reputation  \\\n",
       "0       1.701329e+09     77571304    CC BY-SA 4.0  ...           1035.0   \n",
       "1       1.701357e+09     77570948    CC BY-SA 4.0  ...              1.0   \n",
       "2                NaN     77570944    CC BY-SA 4.0  ...             48.0   \n",
       "3       1.701299e+09     77569740    CC BY-SA 4.0  ...             23.0   \n",
       "4       1.701288e+09     77568420    CC BY-SA 4.0  ...             51.0   \n",
       "...              ...          ...             ...  ...              ...   \n",
       "2495    1.699916e+09     77476359    CC BY-SA 4.0  ...             13.0   \n",
       "2496    1.699916e+09     77475890    CC BY-SA 4.0  ...            849.0   \n",
       "2497    1.711469e+09     77475341    CC BY-SA 4.0  ...             13.0   \n",
       "2498    1.699907e+09     77473519    CC BY-SA 4.0  ...              1.0   \n",
       "2499    1.699883e+09     77472802    CC BY-SA 4.0  ...             21.0   \n",
       "\n",
       "     owner.user_id owner.user_type  owner.accept_rate  \\\n",
       "0         661859.0      registered               75.0   \n",
       "1       22994216.0      registered                NaN   \n",
       "2        9726166.0      registered                NaN   \n",
       "3       22805791.0      registered                NaN   \n",
       "4       13711721.0      registered                NaN   \n",
       "...            ...             ...                ...   \n",
       "2495    22866802.0      registered                NaN   \n",
       "2496      394667.0      registered               62.0   \n",
       "2497    21359845.0      registered                NaN   \n",
       "2498    21766180.0      registered                NaN   \n",
       "2499    22196655.0      registered                NaN   \n",
       "\n",
       "                                    owner.profile_image  owner.display_name  \\\n",
       "0     https://www.gravatar.com/avatar/50d463f2dfeefb...               Nitul   \n",
       "1     https://lh3.googleusercontent.com/a/ACg8ocKoKQ...    Aditya Shandilya   \n",
       "2     https://www.gravatar.com/avatar/2d6c683e736f53...     Muhammad Burhan   \n",
       "3     https://lh3.googleusercontent.com/a/ACg8ocLzlY...    Sagnik Taraphdar   \n",
       "4     https://www.gravatar.com/avatar/f7ae41fe88116c...              prynet   \n",
       "...                                                 ...                 ...   \n",
       "2495  https://lh3.googleusercontent.com/a/ACg8ocLNDl...         Rishi Suman   \n",
       "2496  https://www.gravatar.com/avatar/b7018c9544e5f9...        Vijay Kansal   \n",
       "2497  https://lh3.googleusercontent.com/a/AGNmyxYN1v...       Justin Jonany   \n",
       "2498              https://i.sstatic.net/915SW.jpg?s=256    Anthony Belessis   \n",
       "2499              https://i.sstatic.net/Hagat.jpg?s=256               Hamza   \n",
       "\n",
       "                                             owner.link  accepted_answer_id  \\\n",
       "0          https://stackoverflow.com/users/661859/nitul                 NaN   \n",
       "1     https://stackoverflow.com/users/22994216/adity...                 NaN   \n",
       "2     https://stackoverflow.com/users/9726166/muhamm...                 NaN   \n",
       "3     https://stackoverflow.com/users/22805791/sagni...          77570643.0   \n",
       "4       https://stackoverflow.com/users/13711721/prynet                 NaN   \n",
       "...                                                 ...                 ...   \n",
       "2495  https://stackoverflow.com/users/22866802/rishi...                 NaN   \n",
       "2496  https://stackoverflow.com/users/394667/vijay-k...                 NaN   \n",
       "2497  https://stackoverflow.com/users/21359845/justi...          77479501.0   \n",
       "2498  https://stackoverflow.com/users/21766180/antho...                 NaN   \n",
       "2499     https://stackoverflow.com/users/22196655/hamza          77488546.0   \n",
       "\n",
       "     closed_date closed_reason  \n",
       "0            NaN           NaN  \n",
       "1            NaN           NaN  \n",
       "2            NaN           NaN  \n",
       "3            NaN           NaN  \n",
       "4            NaN           NaN  \n",
       "...          ...           ...  \n",
       "2495         NaN           NaN  \n",
       "2496         NaN           NaN  \n",
       "2497         NaN           NaN  \n",
       "2498         NaN           NaN  \n",
       "2499         NaN           NaN  \n",
       "\n",
       "[2500 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.json_normalize(all_items)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "774cad57-e287-4d31-bfd5-77cc70ce6bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tags', 'is_answered', 'view_count', 'answer_count', 'score',\n",
       "       'last_activity_date', 'creation_date', 'last_edit_date', 'question_id',\n",
       "       'content_license', 'link', 'title', 'body', 'owner.account_id',\n",
       "       'owner.reputation', 'owner.user_id', 'owner.user_type',\n",
       "       'owner.accept_rate', 'owner.profile_image', 'owner.display_name',\n",
       "       'owner.link', 'accepted_answer_id', 'closed_date', 'closed_reason'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "370339c3-1c33-4cce-9b63-0a26264e6f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tags', 'is_answered', 'view_count', 'answer_count', 'score',\n",
       "       'last_activity_date', 'creation_date', 'last_edit_date', 'question_id',\n",
       "       'content_license', 'link', 'title', 'body', 'owner.account_id',\n",
       "       'owner.reputation', 'owner.user_id', 'owner.user_type',\n",
       "       'owner.accept_rate', 'owner.profile_image', 'owner.display_name',\n",
       "       'owner.link', 'accepted_answer_id', 'closed_date', 'closed_reason'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed3216-d400-46db-a6d7-a7bf96397724",
   "metadata": {},
   "source": [
    "# Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78221e5d-91b4-4576-b3ab-687d77467c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = []\n",
    "\n",
    "for page in range(1,26):  \n",
    "    response = requests.get(\n",
    "        \"https://api.stackexchange.com/2.3/questions\",\n",
    "        params={\n",
    "            \"order\": \"desc\",\n",
    "            \"sort\": \"creation\",\n",
    "            \"tagged\": \"git\",\n",
    "            \"site\": \"stackoverflow\",\n",
    "            \"pagesize\": 100,\n",
    "            \"page\": 25,\n",
    "            \"filter\": \"withbody\"  \n",
    "        }\n",
    "    )\n",
    "    items = response.json().get(\"items\", [])\n",
    "    all_items.extend(items)\n",
    "    \n",
    "    if not response.json().get(\"has_more\", False):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6061aaf4-0553-4699-8a84-c3c9cdd36edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>is_answered</th>\n",
       "      <th>view_count</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>score</th>\n",
       "      <th>last_activity_date</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>question_id</th>\n",
       "      <th>content_license</th>\n",
       "      <th>...</th>\n",
       "      <th>owner.reputation</th>\n",
       "      <th>owner.user_id</th>\n",
       "      <th>owner.user_type</th>\n",
       "      <th>owner.profile_image</th>\n",
       "      <th>owner.display_name</th>\n",
       "      <th>owner.link</th>\n",
       "      <th>last_edit_date</th>\n",
       "      <th>owner.accept_rate</th>\n",
       "      <th>closed_date</th>\n",
       "      <th>closed_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[git, jupyter-notebook, gitattributes]</td>\n",
       "      <td>True</td>\n",
       "      <td>172</td>\n",
       "      <td>78475631.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1715658645</td>\n",
       "      <td>1715646718</td>\n",
       "      <td>78475266</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83</td>\n",
       "      <td>3861922</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://www.gravatar.com/avatar/bafcd1ef228844...</td>\n",
       "      <td>3eyes</td>\n",
       "      <td>https://stackoverflow.com/users/3861922/3eyes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[git]</td>\n",
       "      <td>True</td>\n",
       "      <td>35</td>\n",
       "      <td>78474955.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>1715637655</td>\n",
       "      <td>1715635782</td>\n",
       "      <td>78474876</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1417</td>\n",
       "      <td>2175604</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://www.gravatar.com/avatar/11c98698e4985b...</td>\n",
       "      <td>techie11</td>\n",
       "      <td>https://stackoverflow.com/users/2175604/techie11</td>\n",
       "      <td>1.715636e+09</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[git, azure-devops, tfvc]</td>\n",
       "      <td>False</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1715630634</td>\n",
       "      <td>1715629438</td>\n",
       "      <td>78474470</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>956</td>\n",
       "      <td>5557538</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://i.sstatic.net/wq8sW.jpg?s=256</td>\n",
       "      <td>tibx</td>\n",
       "      <td>https://stackoverflow.com/users/5557538/tibx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[python, git, gitlab, commit]</td>\n",
       "      <td>False</td>\n",
       "      <td>93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1721160367</td>\n",
       "      <td>1715624711</td>\n",
       "      <td>78474157</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>24475860</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://www.gravatar.com/avatar/dcb1aef45cf762...</td>\n",
       "      <td>Cole Burley</td>\n",
       "      <td>https://stackoverflow.com/users/24475860/cole-...</td>\n",
       "      <td>1.715629e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[git, visual-studio-code]</td>\n",
       "      <td>True</td>\n",
       "      <td>43</td>\n",
       "      <td>78473766.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1715619613</td>\n",
       "      <td>1715614640</td>\n",
       "      <td>78473365</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>349</td>\n",
       "      <td>17036260</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://lh3.googleusercontent.com/a-/AOh14Gh9y...</td>\n",
       "      <td>Waradu</td>\n",
       "      <td>https://stackoverflow.com/users/17036260/waradu</td>\n",
       "      <td>1.715615e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>[python, git, pip, python-import, pypi]</td>\n",
       "      <td>True</td>\n",
       "      <td>90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1714808321</td>\n",
       "      <td>1714796826</td>\n",
       "      <td>78427767</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>179</td>\n",
       "      <td>5005150</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://www.gravatar.com/avatar/a88517a8fab9ee...</td>\n",
       "      <td>abstract</td>\n",
       "      <td>https://stackoverflow.com/users/5005150/abstract</td>\n",
       "      <td>1.714808e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>[git, github, github-actions]</td>\n",
       "      <td>True</td>\n",
       "      <td>83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1714778452</td>\n",
       "      <td>1714766164</td>\n",
       "      <td>78426712</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2320</td>\n",
       "      <td>14357365</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://www.gravatar.com/avatar/b62371dd783830...</td>\n",
       "      <td>Jay</td>\n",
       "      <td>https://stackoverflow.com/users/14357365/jay</td>\n",
       "      <td>1.714777e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>[git, visual-studio-code]</td>\n",
       "      <td>False</td>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1714968644</td>\n",
       "      <td>1714760015</td>\n",
       "      <td>78426339</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>12892902</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://lh4.googleusercontent.com/-CtgSBxeS1ho...</td>\n",
       "      <td>Genesius Jaromsky</td>\n",
       "      <td>https://stackoverflow.com/users/12892902/genes...</td>\n",
       "      <td>1.714968e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>[git, repository]</td>\n",
       "      <td>False</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1714773684</td>\n",
       "      <td>1714745126</td>\n",
       "      <td>78425169</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>17652371</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://i.sstatic.net/wy1Dl.jpg?s=256</td>\n",
       "      <td>Abdullah Bin Rashid</td>\n",
       "      <td>https://stackoverflow.com/users/17652371/abdul...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>[git, makefile]</td>\n",
       "      <td>True</td>\n",
       "      <td>53</td>\n",
       "      <td>78426738.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1715063431</td>\n",
       "      <td>1714744618</td>\n",
       "      <td>78425124</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>20979095</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://i.sstatic.net/jmva7.png?s=256</td>\n",
       "      <td>Thesevs SCUTULATUS</td>\n",
       "      <td>https://stackoverflow.com/users/20979095/these...</td>\n",
       "      <td>1.715063e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows  24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         tags  is_answered  view_count  \\\n",
       "0      [git, jupyter-notebook, gitattributes]         True         172   \n",
       "1                                       [git]         True          35   \n",
       "2                   [git, azure-devops, tfvc]        False          92   \n",
       "3               [python, git, gitlab, commit]        False          93   \n",
       "4                   [git, visual-studio-code]         True          43   \n",
       "...                                       ...          ...         ...   \n",
       "2495  [python, git, pip, python-import, pypi]         True          90   \n",
       "2496            [git, github, github-actions]         True          83   \n",
       "2497                [git, visual-studio-code]        False          41   \n",
       "2498                        [git, repository]        False          42   \n",
       "2499                          [git, makefile]         True          53   \n",
       "\n",
       "      accepted_answer_id  answer_count  score  last_activity_date  \\\n",
       "0             78475631.0             1      1          1715658645   \n",
       "1             78474955.0             1     -2          1715637655   \n",
       "2                    NaN             1      0          1715630634   \n",
       "3                    NaN             1      2          1721160367   \n",
       "4             78473766.0             1      1          1715619613   \n",
       "...                  ...           ...    ...                 ...   \n",
       "2495                 NaN             1      0          1714808321   \n",
       "2496                 NaN             2      0          1714778452   \n",
       "2497                 NaN             1     -1          1714968644   \n",
       "2498                 NaN             1      0          1714773684   \n",
       "2499          78426738.0             2      0          1715063431   \n",
       "\n",
       "      creation_date  question_id content_license  ... owner.reputation  \\\n",
       "0        1715646718     78475266    CC BY-SA 4.0  ...               83   \n",
       "1        1715635782     78474876    CC BY-SA 4.0  ...             1417   \n",
       "2        1715629438     78474470    CC BY-SA 4.0  ...              956   \n",
       "3        1715624711     78474157    CC BY-SA 4.0  ...               21   \n",
       "4        1715614640     78473365    CC BY-SA 4.0  ...              349   \n",
       "...             ...          ...             ...  ...              ...   \n",
       "2495     1714796826     78427767    CC BY-SA 4.0  ...              179   \n",
       "2496     1714766164     78426712    CC BY-SA 4.0  ...             2320   \n",
       "2497     1714760015     78426339    CC BY-SA 4.0  ...                1   \n",
       "2498     1714745126     78425169    CC BY-SA 4.0  ...               11   \n",
       "2499     1714744618     78425124    CC BY-SA 4.0  ...                5   \n",
       "\n",
       "     owner.user_id owner.user_type  \\\n",
       "0          3861922      registered   \n",
       "1          2175604      registered   \n",
       "2          5557538      registered   \n",
       "3         24475860      registered   \n",
       "4         17036260      registered   \n",
       "...            ...             ...   \n",
       "2495       5005150      registered   \n",
       "2496      14357365      registered   \n",
       "2497      12892902      registered   \n",
       "2498      17652371      registered   \n",
       "2499      20979095      registered   \n",
       "\n",
       "                                    owner.profile_image   owner.display_name  \\\n",
       "0     https://www.gravatar.com/avatar/bafcd1ef228844...                3eyes   \n",
       "1     https://www.gravatar.com/avatar/11c98698e4985b...             techie11   \n",
       "2                 https://i.sstatic.net/wq8sW.jpg?s=256                 tibx   \n",
       "3     https://www.gravatar.com/avatar/dcb1aef45cf762...          Cole Burley   \n",
       "4     https://lh3.googleusercontent.com/a-/AOh14Gh9y...               Waradu   \n",
       "...                                                 ...                  ...   \n",
       "2495  https://www.gravatar.com/avatar/a88517a8fab9ee...             abstract   \n",
       "2496  https://www.gravatar.com/avatar/b62371dd783830...                  Jay   \n",
       "2497  https://lh4.googleusercontent.com/-CtgSBxeS1ho...    Genesius Jaromsky   \n",
       "2498              https://i.sstatic.net/wy1Dl.jpg?s=256  Abdullah Bin Rashid   \n",
       "2499              https://i.sstatic.net/jmva7.png?s=256   Thesevs SCUTULATUS   \n",
       "\n",
       "                                             owner.link last_edit_date  \\\n",
       "0         https://stackoverflow.com/users/3861922/3eyes            NaN   \n",
       "1      https://stackoverflow.com/users/2175604/techie11   1.715636e+09   \n",
       "2          https://stackoverflow.com/users/5557538/tibx            NaN   \n",
       "3     https://stackoverflow.com/users/24475860/cole-...   1.715629e+09   \n",
       "4       https://stackoverflow.com/users/17036260/waradu   1.715615e+09   \n",
       "...                                                 ...            ...   \n",
       "2495   https://stackoverflow.com/users/5005150/abstract   1.714808e+09   \n",
       "2496       https://stackoverflow.com/users/14357365/jay   1.714777e+09   \n",
       "2497  https://stackoverflow.com/users/12892902/genes...   1.714968e+09   \n",
       "2498  https://stackoverflow.com/users/17652371/abdul...            NaN   \n",
       "2499  https://stackoverflow.com/users/20979095/these...   1.715063e+09   \n",
       "\n",
       "     owner.accept_rate closed_date closed_reason  \n",
       "0                  NaN         NaN           NaN  \n",
       "1                 43.0         NaN           NaN  \n",
       "2                  NaN         NaN           NaN  \n",
       "3                  NaN         NaN           NaN  \n",
       "4                  NaN         NaN           NaN  \n",
       "...                ...         ...           ...  \n",
       "2495               NaN         NaN           NaN  \n",
       "2496               NaN         NaN           NaN  \n",
       "2497               NaN         NaN           NaN  \n",
       "2498               NaN         NaN           NaN  \n",
       "2499               NaN         NaN           NaN  \n",
       "\n",
       "[2500 rows x 24 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1= pd.json_normalize(all_items)\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ebd985e-a2c9-43af-9f50-3ab74347b957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tags', 'is_answered', 'view_count', 'accepted_answer_id',\n",
       "       'answer_count', 'score', 'last_activity_date', 'creation_date',\n",
       "       'question_id', 'content_license', 'link', 'title', 'body',\n",
       "       'owner.account_id', 'owner.reputation', 'owner.user_id',\n",
       "       'owner.user_type', 'owner.profile_image', 'owner.display_name',\n",
       "       'owner.link', 'last_edit_date', 'owner.accept_rate', 'closed_date',\n",
       "       'closed_reason'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7093e83d-8b8a-41c7-a58b-55b723f1ed50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[pandas, tensorflow, machine-learning, keras]</td>\n",
       "      <td>What is the difference between Sequential and ...</td>\n",
       "      <td>&lt;p&gt;What is the difference between Sequential a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[python, pandas, machine-learning, scikit-lear...</td>\n",
       "      <td>Including multiple dataset transformers in cus...</td>\n",
       "      <td>&lt;p&gt;Here is my custom transformer, meant to tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[machine-learning, large-language-model, llama...</td>\n",
       "      <td>Llama-2, Q4-Quantized model&amp;#39;s response tim...</td>\n",
       "      <td>&lt;p&gt;I am running quantized llama-2 model from &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[python, machine-learning, linear-regression, ...</td>\n",
       "      <td>Why is my implementation of linear regression ...</td>\n",
       "      <td>&lt;p&gt;I am trying to implement linear regression ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[python, tensorflow, machine-learning, keras, ...</td>\n",
       "      <td>Transfer learning using VGG16 MNIST Digits</td>\n",
       "      <td>&lt;p&gt;I am trying to perform transfer learning on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>[python, machine-learning, deep-learning, pyto...</td>\n",
       "      <td>TypeError: img should be PIL Image. Got &amp;lt;cl...</td>\n",
       "      <td>&lt;p&gt;I am trying to train my model on my local G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>[machine-learning, pyspark, random-forest, dec...</td>\n",
       "      <td>Meaning of predValue field in H2O Random fores...</td>\n",
       "      <td>&lt;p&gt;I built an &lt;code&gt;H2ORandomForestEstimator&lt;/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>[tensorflow, machine-learning, deep-learning, ...</td>\n",
       "      <td>Confusion with tensorflow&amp;#39;s Sequential Dense</td>\n",
       "      <td>&lt;p&gt;I'm working on a regression probem using Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>[machine-learning, keras, neural-network, conc...</td>\n",
       "      <td>How can I have two distinct inputs in keras la...</td>\n",
       "      <td>&lt;p&gt;I am trying to replicate the neural networt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>[machine-learning, reinforcement-learning, ope...</td>\n",
       "      <td>PPO Boid agent not learning</td>\n",
       "      <td>&lt;p&gt;I have  a custom Boid flocking environment ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tags  \\\n",
       "0         [pandas, tensorflow, machine-learning, keras]   \n",
       "1     [python, pandas, machine-learning, scikit-lear...   \n",
       "2     [machine-learning, large-language-model, llama...   \n",
       "3     [python, machine-learning, linear-regression, ...   \n",
       "4     [python, tensorflow, machine-learning, keras, ...   \n",
       "...                                                 ...   \n",
       "2495  [python, machine-learning, deep-learning, pyto...   \n",
       "2496  [machine-learning, pyspark, random-forest, dec...   \n",
       "2497  [tensorflow, machine-learning, deep-learning, ...   \n",
       "2498  [machine-learning, keras, neural-network, conc...   \n",
       "2499  [machine-learning, reinforcement-learning, ope...   \n",
       "\n",
       "                                                  title  \\\n",
       "0     What is the difference between Sequential and ...   \n",
       "1     Including multiple dataset transformers in cus...   \n",
       "2     Llama-2, Q4-Quantized model&#39;s response tim...   \n",
       "3     Why is my implementation of linear regression ...   \n",
       "4            Transfer learning using VGG16 MNIST Digits   \n",
       "...                                                 ...   \n",
       "2495  TypeError: img should be PIL Image. Got &lt;cl...   \n",
       "2496  Meaning of predValue field in H2O Random fores...   \n",
       "2497   Confusion with tensorflow&#39;s Sequential Dense   \n",
       "2498  How can I have two distinct inputs in keras la...   \n",
       "2499                        PPO Boid agent not learning   \n",
       "\n",
       "                                                   body  \n",
       "0     <p>What is the difference between Sequential a...  \n",
       "1     <p>Here is my custom transformer, meant to tra...  \n",
       "2     <p>I am running quantized llama-2 model from <...  \n",
       "3     <p>I am trying to implement linear regression ...  \n",
       "4     <p>I am trying to perform transfer learning on...  \n",
       "...                                                 ...  \n",
       "2495  <p>I am trying to train my model on my local G...  \n",
       "2496  <p>I built an <code>H2ORandomForestEstimator</...  \n",
       "2497  <p>I'm working on a regression probem using Te...  \n",
       "2498  <p>I am trying to replicate the neural networt...  \n",
       "2499  <p>I have  a custom Boid flocking environment ...  \n",
       "\n",
       "[2500 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data[['tags','title','body']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3dfa7bf-3179-4cd7-9381-4a6edcfdfd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[git, jupyter-notebook, gitattributes]</td>\n",
       "      <td>Is there a way to override a gitattributes fil...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;The context:&lt;/strong&gt;&lt;br /&gt;\\nMy tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[git]</td>\n",
       "      <td>how to interpret the decoration from git log -...</td>\n",
       "      <td>&lt;p&gt;The output of git log --decorate is shown a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[git, azure-devops, tfvc]</td>\n",
       "      <td>How to find code churn information for a certa...</td>\n",
       "      <td>&lt;p&gt;In &lt;strong&gt;Azure DevOps Server 2020&lt;/strong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[python, git, gitlab, commit]</td>\n",
       "      <td>Python Gitlab API get users by whether or not ...</td>\n",
       "      <td>&lt;p&gt;At the company I work at we are trying to g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[git, visual-studio-code]</td>\n",
       "      <td>Multiple branches in one window in multiple fo...</td>\n",
       "      <td>&lt;p&gt;In vscode I want to open the main branch ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>[python, git, pip, python-import, pypi]</td>\n",
       "      <td>Import packages cloned locally instead of PyPI</td>\n",
       "      <td>&lt;p&gt;I had previously installed a package on PyP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>[git, github, github-actions]</td>\n",
       "      <td>git status --porcelain return file difference ...</td>\n",
       "      <td>&lt;p&gt;I want to create a workflow that would ensu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>[git, visual-studio-code]</td>\n",
       "      <td>No GIT Icon in VSCode (where I need to see it)</td>\n",
       "      <td>&lt;p&gt;In Explorer there will be Git icons - U, A,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>[git, repository]</td>\n",
       "      <td>Can I add all my projects having sub git repos...</td>\n",
       "      <td>&lt;p&gt;I have all my code in a folder in which I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>[git, makefile]</td>\n",
       "      <td>How can I write a sh if statement that invokes...</td>\n",
       "      <td>&lt;p&gt;I have the following in a Makefile and ever...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         tags  \\\n",
       "0      [git, jupyter-notebook, gitattributes]   \n",
       "1                                       [git]   \n",
       "2                   [git, azure-devops, tfvc]   \n",
       "3               [python, git, gitlab, commit]   \n",
       "4                   [git, visual-studio-code]   \n",
       "...                                       ...   \n",
       "2495  [python, git, pip, python-import, pypi]   \n",
       "2496            [git, github, github-actions]   \n",
       "2497                [git, visual-studio-code]   \n",
       "2498                        [git, repository]   \n",
       "2499                          [git, makefile]   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Is there a way to override a gitattributes fil...   \n",
       "1     how to interpret the decoration from git log -...   \n",
       "2     How to find code churn information for a certa...   \n",
       "3     Python Gitlab API get users by whether or not ...   \n",
       "4     Multiple branches in one window in multiple fo...   \n",
       "...                                                 ...   \n",
       "2495     Import packages cloned locally instead of PyPI   \n",
       "2496  git status --porcelain return file difference ...   \n",
       "2497     No GIT Icon in VSCode (where I need to see it)   \n",
       "2498  Can I add all my projects having sub git repos...   \n",
       "2499  How can I write a sh if statement that invokes...   \n",
       "\n",
       "                                                   body  \n",
       "0     <p><strong>The context:</strong><br />\\nMy tea...  \n",
       "1     <p>The output of git log --decorate is shown a...  \n",
       "2     <p>In <strong>Azure DevOps Server 2020</strong...  \n",
       "3     <p>At the company I work at we are trying to g...  \n",
       "4     <p>In vscode I want to open the main branch ne...  \n",
       "...                                                 ...  \n",
       "2495  <p>I had previously installed a package on PyP...  \n",
       "2496  <p>I want to create a workflow that would ensu...  \n",
       "2497  <p>In Explorer there will be Git icons - U, A,...  \n",
       "2498  <p>I have all my code in a folder in which I w...  \n",
       "2499  <p>I have the following in a Makefile and ever...  \n",
       "\n",
       "[2500 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1=data_1[['tags','title','body']]\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "939b6333-07c5-494f-94e7-dcccda0ab29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_5556\\3529964616.py:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  data.to_csv(\"D:\\Machine learning .csv\",index=False)\n"
     ]
    }
   ],
   "source": [
    "data.to_csv(\"D:\\Machine learning .csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39f363e0-0c75-4460-890a-71a35a06b9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\G'\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_5556\\1423489094.py:1: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  data_1.to_csv(\"D:\\Git.csv\",index=False)\n"
     ]
    }
   ],
   "source": [
    "data_1.to_csv(\"D:\\Git.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f6991b9-6b33-4c3e-a86a-c060b58396a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tags</th>\n",
       "      <th>is_answered</th>\n",
       "      <th>view_count</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>score</th>\n",
       "      <th>last_activity_date</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>question_id</th>\n",
       "      <th>content_license</th>\n",
       "      <th>...</th>\n",
       "      <th>owner.user_type</th>\n",
       "      <th>owner.profile_image</th>\n",
       "      <th>owner.display_name</th>\n",
       "      <th>owner.link</th>\n",
       "      <th>closed_date</th>\n",
       "      <th>closed_reason</th>\n",
       "      <th>last_edit_date</th>\n",
       "      <th>accepted_answer_id</th>\n",
       "      <th>owner.accept_rate</th>\n",
       "      <th>protected_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['python', 'tensorflow', 'keras', 'deep-learni...</td>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1745492847</td>\n",
       "      <td>1745492847</td>\n",
       "      <td>79590419</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://www.gravatar.com/avatar/5b2b30e8b2e84d...</td>\n",
       "      <td>Michelle Latham</td>\n",
       "      <td>https://stackoverflow.com/users/30360806/miche...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['machine-learning', 'deep-learning', 'nlp', '...</td>\n",
       "      <td>False</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1745477978</td>\n",
       "      <td>1745477978</td>\n",
       "      <td>79589996</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://www.gravatar.com/avatar/8b779a817460bc...</td>\n",
       "      <td>Arpit shourya</td>\n",
       "      <td>https://stackoverflow.com/users/29963476/arpit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>['deep-learning', 'object-detection', 'multila...</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>1745475767</td>\n",
       "      <td>1745475767</td>\n",
       "      <td>79589950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://i.sstatic.net/XLPK1.png?s=256</td>\n",
       "      <td>hdcoe</td>\n",
       "      <td>https://stackoverflow.com/users/17000986/hdcoe</td>\n",
       "      <td>1.745477e+09</td>\n",
       "      <td>Not suitable for this site</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>['deep-learning', 'attention-model', 'image-en...</td>\n",
       "      <td>False</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>1745256683</td>\n",
       "      <td>1745256683</td>\n",
       "      <td>79585045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://www.gravatar.com/avatar/de3d6d2ddd16ea...</td>\n",
       "      <td>StarkReborn</td>\n",
       "      <td>https://stackoverflow.com/users/14478100/stark...</td>\n",
       "      <td>1.745286e+09</td>\n",
       "      <td>Not suitable for this site</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>['python', 'deep-learning', 'overfitting-under...</td>\n",
       "      <td>False</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>1745256151</td>\n",
       "      <td>1745256151</td>\n",
       "      <td>79585032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://www.gravatar.com/avatar/ff46ff4f08256d...</td>\n",
       "      <td>David Jorge</td>\n",
       "      <td>https://stackoverflow.com/users/30247326/david...</td>\n",
       "      <td>1.745302e+09</td>\n",
       "      <td>Not suitable for this site</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>2495</td>\n",
       "      <td>['python', 'machine-learning', 'deep-learning'...</td>\n",
       "      <td>False</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1680774029</td>\n",
       "      <td>1680774029</td>\n",
       "      <td>75948040</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://i.sstatic.net/b6x24.png?s=256</td>\n",
       "      <td>Long</td>\n",
       "      <td>https://stackoverflow.com/users/1511274/long</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>2496</td>\n",
       "      <td>['python', 'machine-learning', 'deep-learning']</td>\n",
       "      <td>False</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1681721334</td>\n",
       "      <td>1680759910</td>\n",
       "      <td>75946226</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://lh3.googleusercontent.com/a-/AOh14Gjrh...</td>\n",
       "      <td>Avijit Chowdhury</td>\n",
       "      <td>https://stackoverflow.com/users/17068501/aviji...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.680769e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>2497</td>\n",
       "      <td>['machine-learning', 'deep-learning', 'pytorch...</td>\n",
       "      <td>True</td>\n",
       "      <td>1046</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1680802986</td>\n",
       "      <td>1680738440</td>\n",
       "      <td>75944777</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://lh6.googleusercontent.com/-XP0RPy-AWFc...</td>\n",
       "      <td>vasu sharma</td>\n",
       "      <td>https://stackoverflow.com/users/10250933/vasu-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.680748e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>2498</td>\n",
       "      <td>['python', 'deep-learning', 'pytorch']</td>\n",
       "      <td>False</td>\n",
       "      <td>658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1680725594</td>\n",
       "      <td>1680725179</td>\n",
       "      <td>75943577</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://www.gravatar.com/avatar/f201a37a063396...</td>\n",
       "      <td>programmer_04_03</td>\n",
       "      <td>https://stackoverflow.com/users/20439163/progr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.680726e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>2499</td>\n",
       "      <td>['machine-learning', 'deep-learning', 'pre-tra...</td>\n",
       "      <td>True</td>\n",
       "      <td>1035</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1680701694</td>\n",
       "      <td>1680699620</td>\n",
       "      <td>75939762</td>\n",
       "      <td>CC BY-SA 4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>registered</td>\n",
       "      <td>https://lh3.googleusercontent.com/a/AItbvmkvBP...</td>\n",
       "      <td>Prajakta Rathod</td>\n",
       "      <td>https://stackoverflow.com/users/19594502/praja...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75940110.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows  26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               tags  \\\n",
       "0              0  ['python', 'tensorflow', 'keras', 'deep-learni...   \n",
       "1              1  ['machine-learning', 'deep-learning', 'nlp', '...   \n",
       "2              2  ['deep-learning', 'object-detection', 'multila...   \n",
       "3              3  ['deep-learning', 'attention-model', 'image-en...   \n",
       "4              4  ['python', 'deep-learning', 'overfitting-under...   \n",
       "...          ...                                                ...   \n",
       "2495        2495  ['python', 'machine-learning', 'deep-learning'...   \n",
       "2496        2496    ['python', 'machine-learning', 'deep-learning']   \n",
       "2497        2497  ['machine-learning', 'deep-learning', 'pytorch...   \n",
       "2498        2498             ['python', 'deep-learning', 'pytorch']   \n",
       "2499        2499  ['machine-learning', 'deep-learning', 'pre-tra...   \n",
       "\n",
       "      is_answered  view_count  answer_count  score  last_activity_date  \\\n",
       "0           False          17             0      0          1745492847   \n",
       "1           False          23             0     -1          1745477978   \n",
       "2           False          18             0     -2          1745475767   \n",
       "3           False          21             0     -2          1745256683   \n",
       "4           False          33             0     -2          1745256151   \n",
       "...           ...         ...           ...    ...                 ...   \n",
       "2495        False          59             0      0          1680774029   \n",
       "2496        False          83             1      0          1681721334   \n",
       "2497         True        1046             1     -1          1680802986   \n",
       "2498        False         658             0      0          1680725594   \n",
       "2499         True        1035             1      0          1680701694   \n",
       "\n",
       "      creation_date  question_id content_license  ... owner.user_type  \\\n",
       "0        1745492847     79590419    CC BY-SA 4.0  ...      registered   \n",
       "1        1745477978     79589996    CC BY-SA 4.0  ...      registered   \n",
       "2        1745475767     79589950             NaN  ...      registered   \n",
       "3        1745256683     79585045             NaN  ...      registered   \n",
       "4        1745256151     79585032             NaN  ...      registered   \n",
       "...             ...          ...             ...  ...             ...   \n",
       "2495     1680774029     75948040    CC BY-SA 4.0  ...      registered   \n",
       "2496     1680759910     75946226    CC BY-SA 4.0  ...      registered   \n",
       "2497     1680738440     75944777    CC BY-SA 4.0  ...      registered   \n",
       "2498     1680725179     75943577    CC BY-SA 4.0  ...      registered   \n",
       "2499     1680699620     75939762    CC BY-SA 4.0  ...      registered   \n",
       "\n",
       "                                    owner.profile_image owner.display_name  \\\n",
       "0     https://www.gravatar.com/avatar/5b2b30e8b2e84d...    Michelle Latham   \n",
       "1     https://www.gravatar.com/avatar/8b779a817460bc...      Arpit shourya   \n",
       "2                 https://i.sstatic.net/XLPK1.png?s=256              hdcoe   \n",
       "3     https://www.gravatar.com/avatar/de3d6d2ddd16ea...        StarkReborn   \n",
       "4     https://www.gravatar.com/avatar/ff46ff4f08256d...        David Jorge   \n",
       "...                                                 ...                ...   \n",
       "2495              https://i.sstatic.net/b6x24.png?s=256               Long   \n",
       "2496  https://lh3.googleusercontent.com/a-/AOh14Gjrh...   Avijit Chowdhury   \n",
       "2497  https://lh6.googleusercontent.com/-XP0RPy-AWFc...        vasu sharma   \n",
       "2498  https://www.gravatar.com/avatar/f201a37a063396...   programmer_04_03   \n",
       "2499  https://lh3.googleusercontent.com/a/AItbvmkvBP...    Prajakta Rathod   \n",
       "\n",
       "                                             owner.link   closed_date  \\\n",
       "0     https://stackoverflow.com/users/30360806/miche...           NaN   \n",
       "1     https://stackoverflow.com/users/29963476/arpit...           NaN   \n",
       "2        https://stackoverflow.com/users/17000986/hdcoe  1.745477e+09   \n",
       "3     https://stackoverflow.com/users/14478100/stark...  1.745286e+09   \n",
       "4     https://stackoverflow.com/users/30247326/david...  1.745302e+09   \n",
       "...                                                 ...           ...   \n",
       "2495       https://stackoverflow.com/users/1511274/long           NaN   \n",
       "2496  https://stackoverflow.com/users/17068501/aviji...           NaN   \n",
       "2497  https://stackoverflow.com/users/10250933/vasu-...           NaN   \n",
       "2498  https://stackoverflow.com/users/20439163/progr...           NaN   \n",
       "2499  https://stackoverflow.com/users/19594502/praja...           NaN   \n",
       "\n",
       "                   closed_reason last_edit_date accepted_answer_id  \\\n",
       "0                            NaN            NaN                NaN   \n",
       "1                            NaN            NaN                NaN   \n",
       "2     Not suitable for this site            NaN                NaN   \n",
       "3     Not suitable for this site            NaN                NaN   \n",
       "4     Not suitable for this site            NaN                NaN   \n",
       "...                          ...            ...                ...   \n",
       "2495                         NaN            NaN                NaN   \n",
       "2496                         NaN   1.680769e+09                NaN   \n",
       "2497                         NaN   1.680748e+09                NaN   \n",
       "2498                         NaN   1.680726e+09                NaN   \n",
       "2499                         NaN            NaN         75940110.0   \n",
       "\n",
       "     owner.accept_rate protected_date  \n",
       "0                  NaN            NaN  \n",
       "1                  NaN            NaN  \n",
       "2                  NaN            NaN  \n",
       "3                  NaN            NaN  \n",
       "4                  NaN            NaN  \n",
       "...                ...            ...  \n",
       "2495               NaN            NaN  \n",
       "2496               NaN            NaN  \n",
       "2497               NaN            NaN  \n",
       "2498               NaN            NaN  \n",
       "2499               NaN            NaN  \n",
       "\n",
       "[2500 rows x 26 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_csv(r\"D:\\deep_ldata.csv\")\n",
    "df1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b312123-6c0b-4fba-8db8-a03550b52a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['python', 'tensorflow', 'keras', 'deep-learni...</td>\n",
       "      <td>Trouble loading tensorflow when running cloned...</td>\n",
       "      <td>&lt;p&gt;So, I'm trying to get the code from this re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'nlp', '...</td>\n",
       "      <td>For training a neural network, if i have label...</td>\n",
       "      <td>&lt;p&gt;I am working on a project which predict cus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['deep-learning', 'object-detection', 'multila...</td>\n",
       "      <td>Polygon vs Bounding box</td>\n",
       "      <td>&lt;p&gt;I want to implement the leaf disease detect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['deep-learning', 'attention-model', 'image-en...</td>\n",
       "      <td>The training loss curve is just a straight lin...</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"https://i.sstatic.net/oTR24bcA.png...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['python', 'deep-learning', 'overfitting-under...</td>\n",
       "      <td>My CNN model is overfitted, and I don&amp;#39;t kn...</td>\n",
       "      <td>&lt;p&gt;I am making a CNN model to recognize skin d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>['python', 'machine-learning', 'deep-learning'...</td>\n",
       "      <td>Identify the timestamps of the merged audio si...</td>\n",
       "      <td>\\n&lt;p&gt;I have an audio file which was created by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>['python', 'machine-learning', 'deep-learning']</td>\n",
       "      <td>InvalidArgumentError: Graph execution error: i...</td>\n",
       "      <td>&lt;pre&gt;&lt;code&gt;classifier = tf.keras.models.Sequen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'pytorch...</td>\n",
       "      <td>My validation accuracy is higher than my train...</td>\n",
       "      <td>&lt;p&gt;I am training an CNN model (AlexNet- with r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>['python', 'deep-learning', 'pytorch']</td>\n",
       "      <td>CUDA error: an illegal memory access while tra...</td>\n",
       "      <td>&lt;p&gt;I am trying to train a deep learning model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'pre-tra...</td>\n",
       "      <td>How to normalize image with its own mean and s...</td>\n",
       "      <td>&lt;p&gt;I am training VGG16 network with medical im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tags  \\\n",
       "0     ['python', 'tensorflow', 'keras', 'deep-learni...   \n",
       "1     ['machine-learning', 'deep-learning', 'nlp', '...   \n",
       "2     ['deep-learning', 'object-detection', 'multila...   \n",
       "3     ['deep-learning', 'attention-model', 'image-en...   \n",
       "4     ['python', 'deep-learning', 'overfitting-under...   \n",
       "...                                                 ...   \n",
       "2495  ['python', 'machine-learning', 'deep-learning'...   \n",
       "2496    ['python', 'machine-learning', 'deep-learning']   \n",
       "2497  ['machine-learning', 'deep-learning', 'pytorch...   \n",
       "2498             ['python', 'deep-learning', 'pytorch']   \n",
       "2499  ['machine-learning', 'deep-learning', 'pre-tra...   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Trouble loading tensorflow when running cloned...   \n",
       "1     For training a neural network, if i have label...   \n",
       "2                               Polygon vs Bounding box   \n",
       "3     The training loss curve is just a straight lin...   \n",
       "4     My CNN model is overfitted, and I don&#39;t kn...   \n",
       "...                                                 ...   \n",
       "2495  Identify the timestamps of the merged audio si...   \n",
       "2496  InvalidArgumentError: Graph execution error: i...   \n",
       "2497  My validation accuracy is higher than my train...   \n",
       "2498  CUDA error: an illegal memory access while tra...   \n",
       "2499  How to normalize image with its own mean and s...   \n",
       "\n",
       "                                                   body  \n",
       "0     <p>So, I'm trying to get the code from this re...  \n",
       "1     <p>I am working on a project which predict cus...  \n",
       "2     <p>I want to implement the leaf disease detect...  \n",
       "3     <p><a href=\"https://i.sstatic.net/oTR24bcA.png...  \n",
       "4     <p>I am making a CNN model to recognize skin d...  \n",
       "...                                                 ...  \n",
       "2495  \\n<p>I have an audio file which was created by...  \n",
       "2496  <pre><code>classifier = tf.keras.models.Sequen...  \n",
       "2497  <p>I am training an CNN model (AlexNet- with r...  \n",
       "2498  <p>I am trying to train a deep learning model ...  \n",
       "2499  <p>I am training VGG16 network with medical im...  \n",
       "\n",
       "[2500 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df1[['tags','title','body']]\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "906dbb18-088c-4a0c-bc36-a81807ca3744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tags</th>\n",
       "      <th>is_answered</th>\n",
       "      <th>view_count</th>\n",
       "      <th>closed_date</th>\n",
       "      <th>answer_count</th>\n",
       "      <th>score</th>\n",
       "      <th>last_activity_date</th>\n",
       "      <th>creation_date</th>\n",
       "      <th>last_edit_date</th>\n",
       "      <th>...</th>\n",
       "      <th>migrated_from.other_site.icon_url</th>\n",
       "      <th>migrated_from.other_site.audience</th>\n",
       "      <th>migrated_from.other_site.site_url</th>\n",
       "      <th>migrated_from.other_site.api_site_parameter</th>\n",
       "      <th>migrated_from.other_site.logo_url</th>\n",
       "      <th>migrated_from.other_site.name</th>\n",
       "      <th>migrated_from.other_site.site_type</th>\n",
       "      <th>migrated_from.on_date</th>\n",
       "      <th>migrated_from.question_id</th>\n",
       "      <th>posted_by_collectives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['python', 'machine-learning', 'time-series', ...</td>\n",
       "      <td>False</td>\n",
       "      <td>26</td>\n",
       "      <td>1.744177e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>-5</td>\n",
       "      <td>1744162234</td>\n",
       "      <td>1744147930</td>\n",
       "      <td>1.744162e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['machine-learning', 'xgboost']</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>1.744146e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>1744132141</td>\n",
       "      <td>1744132141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>['machine-learning', 'proxy', 'dataset', 'hugg...</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "      <td>1.744132e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1744121117</td>\n",
       "      <td>1744120348</td>\n",
       "      <td>1.744121e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>['python', 'machine-learning', 'deep-learning'...</td>\n",
       "      <td>True</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1744143458</td>\n",
       "      <td>1744111216</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>['machine-learning', 'k-means', 'unsupervised-...</td>\n",
       "      <td>False</td>\n",
       "      <td>19</td>\n",
       "      <td>1.744111e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>1744109554</td>\n",
       "      <td>1744109554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>2495</td>\n",
       "      <td>['python', 'amazon-web-services', 'machine-lea...</td>\n",
       "      <td>False</td>\n",
       "      <td>118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1701641010</td>\n",
       "      <td>1701641010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>2496</td>\n",
       "      <td>['python', 'numpy', 'machine-learning', 'neura...</td>\n",
       "      <td>True</td>\n",
       "      <td>409</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1702397036</td>\n",
       "      <td>1701612229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>2497</td>\n",
       "      <td>['python', 'machine-learning', 'pytorch']</td>\n",
       "      <td>True</td>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1701539887</td>\n",
       "      <td>1701539626</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>2498</td>\n",
       "      <td>['machine-learning', 'svm', 'hyperparameters',...</td>\n",
       "      <td>False</td>\n",
       "      <td>63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1701516407</td>\n",
       "      <td>1701491248</td>\n",
       "      <td>1.701501e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>2499</td>\n",
       "      <td>['python', 'machine-learning', 'scikit-learn']</td>\n",
       "      <td>False</td>\n",
       "      <td>52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1701533253</td>\n",
       "      <td>1701452675</td>\n",
       "      <td>1.701533e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows  70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               tags  \\\n",
       "0              0  ['python', 'machine-learning', 'time-series', ...   \n",
       "1              1                    ['machine-learning', 'xgboost']   \n",
       "2              2  ['machine-learning', 'proxy', 'dataset', 'hugg...   \n",
       "3              3  ['python', 'machine-learning', 'deep-learning'...   \n",
       "4              4  ['machine-learning', 'k-means', 'unsupervised-...   \n",
       "...          ...                                                ...   \n",
       "2495        2495  ['python', 'amazon-web-services', 'machine-lea...   \n",
       "2496        2496  ['python', 'numpy', 'machine-learning', 'neura...   \n",
       "2497        2497          ['python', 'machine-learning', 'pytorch']   \n",
       "2498        2498  ['machine-learning', 'svm', 'hyperparameters',...   \n",
       "2499        2499     ['python', 'machine-learning', 'scikit-learn']   \n",
       "\n",
       "      is_answered  view_count   closed_date  answer_count  score  \\\n",
       "0           False          26  1.744177e+09             0     -5   \n",
       "1           False          10  1.744146e+09             0     -2   \n",
       "2           False          25  1.744132e+09             0     -1   \n",
       "3            True          30           NaN             1      1   \n",
       "4           False          19  1.744111e+09             0     -2   \n",
       "...           ...         ...           ...           ...    ...   \n",
       "2495        False         118           NaN             0      0   \n",
       "2496         True         409           NaN             3      2   \n",
       "2497         True          74           NaN             1      0   \n",
       "2498        False          63           NaN             1      0   \n",
       "2499        False          52           NaN             0      2   \n",
       "\n",
       "      last_activity_date  creation_date  last_edit_date  ...  \\\n",
       "0             1744162234     1744147930    1.744162e+09  ...   \n",
       "1             1744132141     1744132141             NaN  ...   \n",
       "2             1744121117     1744120348    1.744121e+09  ...   \n",
       "3             1744143458     1744111216             NaN  ...   \n",
       "4             1744109554     1744109554             NaN  ...   \n",
       "...                  ...            ...             ...  ...   \n",
       "2495          1701641010     1701641010             NaN  ...   \n",
       "2496          1702397036     1701612229             NaN  ...   \n",
       "2497          1701539887     1701539626             NaN  ...   \n",
       "2498          1701516407     1701491248    1.701501e+09  ...   \n",
       "2499          1701533253     1701452675    1.701533e+09  ...   \n",
       "\n",
       "      migrated_from.other_site.icon_url migrated_from.other_site.audience  \\\n",
       "0                                   NaN                               NaN   \n",
       "1                                   NaN                               NaN   \n",
       "2                                   NaN                               NaN   \n",
       "3                                   NaN                               NaN   \n",
       "4                                   NaN                               NaN   \n",
       "...                                 ...                               ...   \n",
       "2495                                NaN                               NaN   \n",
       "2496                                NaN                               NaN   \n",
       "2497                                NaN                               NaN   \n",
       "2498                                NaN                               NaN   \n",
       "2499                                NaN                               NaN   \n",
       "\n",
       "     migrated_from.other_site.site_url  \\\n",
       "0                                  NaN   \n",
       "1                                  NaN   \n",
       "2                                  NaN   \n",
       "3                                  NaN   \n",
       "4                                  NaN   \n",
       "...                                ...   \n",
       "2495                               NaN   \n",
       "2496                               NaN   \n",
       "2497                               NaN   \n",
       "2498                               NaN   \n",
       "2499                               NaN   \n",
       "\n",
       "     migrated_from.other_site.api_site_parameter  \\\n",
       "0                                            NaN   \n",
       "1                                            NaN   \n",
       "2                                            NaN   \n",
       "3                                            NaN   \n",
       "4                                            NaN   \n",
       "...                                          ...   \n",
       "2495                                         NaN   \n",
       "2496                                         NaN   \n",
       "2497                                         NaN   \n",
       "2498                                         NaN   \n",
       "2499                                         NaN   \n",
       "\n",
       "     migrated_from.other_site.logo_url  migrated_from.other_site.name  \\\n",
       "0                                  NaN                            NaN   \n",
       "1                                  NaN                            NaN   \n",
       "2                                  NaN                            NaN   \n",
       "3                                  NaN                            NaN   \n",
       "4                                  NaN                            NaN   \n",
       "...                                ...                            ...   \n",
       "2495                               NaN                            NaN   \n",
       "2496                               NaN                            NaN   \n",
       "2497                               NaN                            NaN   \n",
       "2498                               NaN                            NaN   \n",
       "2499                               NaN                            NaN   \n",
       "\n",
       "      migrated_from.other_site.site_type  migrated_from.on_date  \\\n",
       "0                                    NaN                    NaN   \n",
       "1                                    NaN                    NaN   \n",
       "2                                    NaN                    NaN   \n",
       "3                                    NaN                    NaN   \n",
       "4                                    NaN                    NaN   \n",
       "...                                  ...                    ...   \n",
       "2495                                 NaN                    NaN   \n",
       "2496                                 NaN                    NaN   \n",
       "2497                                 NaN                    NaN   \n",
       "2498                                 NaN                    NaN   \n",
       "2499                                 NaN                    NaN   \n",
       "\n",
       "     migrated_from.question_id posted_by_collectives  \n",
       "0                          NaN                   NaN  \n",
       "1                          NaN                   NaN  \n",
       "2                          NaN                   NaN  \n",
       "3                          NaN                   NaN  \n",
       "4                          NaN                   NaN  \n",
       "...                        ...                   ...  \n",
       "2495                       NaN                   NaN  \n",
       "2496                       NaN                   NaN  \n",
       "2497                       NaN                   NaN  \n",
       "2498                       NaN                   NaN  \n",
       "2499                       NaN                   NaN  \n",
       "\n",
       "[2500 rows x 70 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.read_csv(r\"D:\\stack_data.csv\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2eb6bd8c-f427-42ba-8491-4b4aa62772ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['python', 'tensorflow', 'keras', 'deep-learni...</td>\n",
       "      <td>Trouble loading tensorflow when running cloned...</td>\n",
       "      <td>&lt;p&gt;So, I'm trying to get the code from this re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'nlp', '...</td>\n",
       "      <td>For training a neural network, if i have label...</td>\n",
       "      <td>&lt;p&gt;I am working on a project which predict cus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['deep-learning', 'object-detection', 'multila...</td>\n",
       "      <td>Polygon vs Bounding box</td>\n",
       "      <td>&lt;p&gt;I want to implement the leaf disease detect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['deep-learning', 'attention-model', 'image-en...</td>\n",
       "      <td>The training loss curve is just a straight lin...</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"https://i.sstatic.net/oTR24bcA.png...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['python', 'deep-learning', 'overfitting-under...</td>\n",
       "      <td>My CNN model is overfitted, and I don&amp;#39;t kn...</td>\n",
       "      <td>&lt;p&gt;I am making a CNN model to recognize skin d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2495</th>\n",
       "      <td>['python', 'machine-learning', 'deep-learning'...</td>\n",
       "      <td>Identify the timestamps of the merged audio si...</td>\n",
       "      <td>\\n&lt;p&gt;I have an audio file which was created by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2496</th>\n",
       "      <td>['python', 'machine-learning', 'deep-learning']</td>\n",
       "      <td>InvalidArgumentError: Graph execution error: i...</td>\n",
       "      <td>&lt;pre&gt;&lt;code&gt;classifier = tf.keras.models.Sequen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'pytorch...</td>\n",
       "      <td>My validation accuracy is higher than my train...</td>\n",
       "      <td>&lt;p&gt;I am training an CNN model (AlexNet- with r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>['python', 'deep-learning', 'pytorch']</td>\n",
       "      <td>CUDA error: an illegal memory access while tra...</td>\n",
       "      <td>&lt;p&gt;I am trying to train a deep learning model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'pre-tra...</td>\n",
       "      <td>How to normalize image with its own mean and s...</td>\n",
       "      <td>&lt;p&gt;I am training VGG16 network with medical im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tags  \\\n",
       "0     ['python', 'tensorflow', 'keras', 'deep-learni...   \n",
       "1     ['machine-learning', 'deep-learning', 'nlp', '...   \n",
       "2     ['deep-learning', 'object-detection', 'multila...   \n",
       "3     ['deep-learning', 'attention-model', 'image-en...   \n",
       "4     ['python', 'deep-learning', 'overfitting-under...   \n",
       "...                                                 ...   \n",
       "2495  ['python', 'machine-learning', 'deep-learning'...   \n",
       "2496    ['python', 'machine-learning', 'deep-learning']   \n",
       "2497  ['machine-learning', 'deep-learning', 'pytorch...   \n",
       "2498             ['python', 'deep-learning', 'pytorch']   \n",
       "2499  ['machine-learning', 'deep-learning', 'pre-tra...   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Trouble loading tensorflow when running cloned...   \n",
       "1     For training a neural network, if i have label...   \n",
       "2                               Polygon vs Bounding box   \n",
       "3     The training loss curve is just a straight lin...   \n",
       "4     My CNN model is overfitted, and I don&#39;t kn...   \n",
       "...                                                 ...   \n",
       "2495  Identify the timestamps of the merged audio si...   \n",
       "2496  InvalidArgumentError: Graph execution error: i...   \n",
       "2497  My validation accuracy is higher than my train...   \n",
       "2498  CUDA error: an illegal memory access while tra...   \n",
       "2499  How to normalize image with its own mean and s...   \n",
       "\n",
       "                                                   body  \n",
       "0     <p>So, I'm trying to get the code from this re...  \n",
       "1     <p>I am working on a project which predict cus...  \n",
       "2     <p>I want to implement the leaf disease detect...  \n",
       "3     <p><a href=\"https://i.sstatic.net/oTR24bcA.png...  \n",
       "4     <p>I am making a CNN model to recognize skin d...  \n",
       "...                                                 ...  \n",
       "2495  \\n<p>I have an audio file which was created by...  \n",
       "2496  <pre><code>classifier = tf.keras.models.Sequen...  \n",
       "2497  <p>I am training an CNN model (AlexNet- with r...  \n",
       "2498  <p>I am trying to train a deep learning model ...  \n",
       "2499  <p>I am training VGG16 network with medical im...  \n",
       "\n",
       "[2500 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=df1[['tags','title','body']]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56d924ca-bf57-442c-931b-f75aa6c391c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['python', 'tensorflow', 'keras', 'deep-learni...</td>\n",
       "      <td>Trouble loading tensorflow when running cloned...</td>\n",
       "      <td>&lt;p&gt;So, I'm trying to get the code from this re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'nlp', '...</td>\n",
       "      <td>For training a neural network, if i have label...</td>\n",
       "      <td>&lt;p&gt;I am working on a project which predict cus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['deep-learning', 'object-detection', 'multila...</td>\n",
       "      <td>Polygon vs Bounding box</td>\n",
       "      <td>&lt;p&gt;I want to implement the leaf disease detect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['deep-learning', 'attention-model', 'image-en...</td>\n",
       "      <td>The training loss curve is just a straight lin...</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"https://i.sstatic.net/oTR24bcA.png...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['python', 'deep-learning', 'overfitting-under...</td>\n",
       "      <td>My CNN model is overfitted, and I don&amp;#39;t kn...</td>\n",
       "      <td>&lt;p&gt;I am making a CNN model to recognize skin d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>['python', 'machine-learning', 'deep-learning'...</td>\n",
       "      <td>Identify the timestamps of the merged audio si...</td>\n",
       "      <td>\\n&lt;p&gt;I have an audio file which was created by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>['python', 'machine-learning', 'deep-learning']</td>\n",
       "      <td>InvalidArgumentError: Graph execution error: i...</td>\n",
       "      <td>&lt;pre&gt;&lt;code&gt;classifier = tf.keras.models.Sequen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'pytorch...</td>\n",
       "      <td>My validation accuracy is higher than my train...</td>\n",
       "      <td>&lt;p&gt;I am training an CNN model (AlexNet- with r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>['python', 'deep-learning', 'pytorch']</td>\n",
       "      <td>CUDA error: an illegal memory access while tra...</td>\n",
       "      <td>&lt;p&gt;I am trying to train a deep learning model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'pre-tra...</td>\n",
       "      <td>How to normalize image with its own mean and s...</td>\n",
       "      <td>&lt;p&gt;I am training VGG16 network with medical im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tags  \\\n",
       "0     ['python', 'tensorflow', 'keras', 'deep-learni...   \n",
       "1     ['machine-learning', 'deep-learning', 'nlp', '...   \n",
       "2     ['deep-learning', 'object-detection', 'multila...   \n",
       "3     ['deep-learning', 'attention-model', 'image-en...   \n",
       "4     ['python', 'deep-learning', 'overfitting-under...   \n",
       "...                                                 ...   \n",
       "4995  ['python', 'machine-learning', 'deep-learning'...   \n",
       "4996    ['python', 'machine-learning', 'deep-learning']   \n",
       "4997  ['machine-learning', 'deep-learning', 'pytorch...   \n",
       "4998             ['python', 'deep-learning', 'pytorch']   \n",
       "4999  ['machine-learning', 'deep-learning', 'pre-tra...   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Trouble loading tensorflow when running cloned...   \n",
       "1     For training a neural network, if i have label...   \n",
       "2                               Polygon vs Bounding box   \n",
       "3     The training loss curve is just a straight lin...   \n",
       "4     My CNN model is overfitted, and I don&#39;t kn...   \n",
       "...                                                 ...   \n",
       "4995  Identify the timestamps of the merged audio si...   \n",
       "4996  InvalidArgumentError: Graph execution error: i...   \n",
       "4997  My validation accuracy is higher than my train...   \n",
       "4998  CUDA error: an illegal memory access while tra...   \n",
       "4999  How to normalize image with its own mean and s...   \n",
       "\n",
       "                                                   body  \n",
       "0     <p>So, I'm trying to get the code from this re...  \n",
       "1     <p>I am working on a project which predict cus...  \n",
       "2     <p>I want to implement the leaf disease detect...  \n",
       "3     <p><a href=\"https://i.sstatic.net/oTR24bcA.png...  \n",
       "4     <p>I am making a CNN model to recognize skin d...  \n",
       "...                                                 ...  \n",
       "4995  \\n<p>I have an audio file which was created by...  \n",
       "4996  <pre><code>classifier = tf.keras.models.Sequen...  \n",
       "4997  <p>I am training an CNN model (AlexNet- with r...  \n",
       "4998  <p>I am trying to train a deep learning model ...  \n",
       "4999  <p>I am training VGG16 network with medical im...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08c1a097-2066-4aa7-8535-78c350e9d96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>['python', 'tensorflow', 'keras', 'deep-learni...</td>\n",
       "      <td>Trouble loading tensorflow when running cloned...</td>\n",
       "      <td>&lt;p&gt;So, I'm trying to get the code from this re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2501</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'nlp', '...</td>\n",
       "      <td>For training a neural network, if i have label...</td>\n",
       "      <td>&lt;p&gt;I am working on a project which predict cus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>['deep-learning', 'object-detection', 'multila...</td>\n",
       "      <td>Polygon vs Bounding box</td>\n",
       "      <td>&lt;p&gt;I want to implement the leaf disease detect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503</th>\n",
       "      <td>['deep-learning', 'attention-model', 'image-en...</td>\n",
       "      <td>The training loss curve is just a straight lin...</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"https://i.sstatic.net/oTR24bcA.png...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2504</th>\n",
       "      <td>['python', 'deep-learning', 'overfitting-under...</td>\n",
       "      <td>My CNN model is overfitted, and I don&amp;#39;t kn...</td>\n",
       "      <td>&lt;p&gt;I am making a CNN model to recognize skin d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>['python', 'machine-learning', 'deep-learning'...</td>\n",
       "      <td>Identify the timestamps of the merged audio si...</td>\n",
       "      <td>\\n&lt;p&gt;I have an audio file which was created by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>['python', 'machine-learning', 'deep-learning']</td>\n",
       "      <td>InvalidArgumentError: Graph execution error: i...</td>\n",
       "      <td>&lt;pre&gt;&lt;code&gt;classifier = tf.keras.models.Sequen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'pytorch...</td>\n",
       "      <td>My validation accuracy is higher than my train...</td>\n",
       "      <td>&lt;p&gt;I am training an CNN model (AlexNet- with r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>['python', 'deep-learning', 'pytorch']</td>\n",
       "      <td>CUDA error: an illegal memory access while tra...</td>\n",
       "      <td>&lt;p&gt;I am trying to train a deep learning model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'pre-tra...</td>\n",
       "      <td>How to normalize image with its own mean and s...</td>\n",
       "      <td>&lt;p&gt;I am training VGG16 network with medical im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tags  \\\n",
       "2500  ['python', 'tensorflow', 'keras', 'deep-learni...   \n",
       "2501  ['machine-learning', 'deep-learning', 'nlp', '...   \n",
       "2502  ['deep-learning', 'object-detection', 'multila...   \n",
       "2503  ['deep-learning', 'attention-model', 'image-en...   \n",
       "2504  ['python', 'deep-learning', 'overfitting-under...   \n",
       "...                                                 ...   \n",
       "4995  ['python', 'machine-learning', 'deep-learning'...   \n",
       "4996    ['python', 'machine-learning', 'deep-learning']   \n",
       "4997  ['machine-learning', 'deep-learning', 'pytorch...   \n",
       "4998             ['python', 'deep-learning', 'pytorch']   \n",
       "4999  ['machine-learning', 'deep-learning', 'pre-tra...   \n",
       "\n",
       "                                                  title  \\\n",
       "2500  Trouble loading tensorflow when running cloned...   \n",
       "2501  For training a neural network, if i have label...   \n",
       "2502                            Polygon vs Bounding box   \n",
       "2503  The training loss curve is just a straight lin...   \n",
       "2504  My CNN model is overfitted, and I don&#39;t kn...   \n",
       "...                                                 ...   \n",
       "4995  Identify the timestamps of the merged audio si...   \n",
       "4996  InvalidArgumentError: Graph execution error: i...   \n",
       "4997  My validation accuracy is higher than my train...   \n",
       "4998  CUDA error: an illegal memory access while tra...   \n",
       "4999  How to normalize image with its own mean and s...   \n",
       "\n",
       "                                                   body  \n",
       "2500  <p>So, I'm trying to get the code from this re...  \n",
       "2501  <p>I am working on a project which predict cus...  \n",
       "2502  <p>I want to implement the leaf disease detect...  \n",
       "2503  <p><a href=\"https://i.sstatic.net/oTR24bcA.png...  \n",
       "2504  <p>I am making a CNN model to recognize skin d...  \n",
       "...                                                 ...  \n",
       "4995  \\n<p>I have an audio file which was created by...  \n",
       "4996  <pre><code>classifier = tf.keras.models.Sequen...  \n",
       "4997  <p>I am training an CNN model (AlexNet- with r...  \n",
       "4998  <p>I am trying to train a deep learning model ...  \n",
       "4999  <p>I am training VGG16 network with medical im...  \n",
       "\n",
       "[2500 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data[final_data.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebc20422-2d4c-463b-868b-ead85a64577f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title\n",
       "Trouble loading tensorflow when running cloned repo                                                          1\n",
       "Is there a way to change Adam to legacy when using Mac M1/M2 in TensorFlow?                                  1\n",
       "How to convert Keras Conv3D output to a flatten layer for classification task?                               1\n",
       "Performance results gets printed after every epoch                                                           1\n",
       "Highly imbalanced Alzheimer&#39;s Disease MRI image dataset                                                  1\n",
       "                                                                                                            ..\n",
       "Transferring Discriminator weights to CNN for image classficiation (Microsoft BIG 2015 challenge)            1\n",
       "An error due to unwanted layer in image captioning model                                                     1\n",
       "How do I save a keras model without its training hyperparameters and other training data?                    1\n",
       "Is there a way to get the images where the Ultralytics YOLOv5 model missed the object?                       1\n",
       "How to normalize image with its own mean and standard deviation during training in deep learning networks    1\n",
       "Name: count, Length: 2499, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.title=final_data.title.drop_duplicates(keep='first') # drops the duplicates\n",
    "final_data.title.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e286a50-ea7f-4c3f-a68f-591adab31805",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fv=final_data[[\"title\",\"body\"]]\n",
    "cv=final_data[[\"tags\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54489083-f287-4543-bbf8-eb41a7c5c7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv[\"tags\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86277e3d-8e98-4125-8d4a-9b74b3c519c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['python', 'tensorflow', 'keras', 'deep-learni...</td>\n",
       "      <td>Trouble loading tensorflow when running cloned...</td>\n",
       "      <td>&lt;p&gt;So, I'm trying to get the code from this re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'nlp', '...</td>\n",
       "      <td>For training a neural network, if i have label...</td>\n",
       "      <td>&lt;p&gt;I am working on a project which predict cus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['deep-learning', 'object-detection', 'multila...</td>\n",
       "      <td>Polygon vs Bounding box</td>\n",
       "      <td>&lt;p&gt;I want to implement the leaf disease detect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['deep-learning', 'attention-model', 'image-en...</td>\n",
       "      <td>The training loss curve is just a straight lin...</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"https://i.sstatic.net/oTR24bcA.png...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['python', 'deep-learning', 'overfitting-under...</td>\n",
       "      <td>My CNN model is overfitted, and I don&amp;#39;t kn...</td>\n",
       "      <td>&lt;p&gt;I am making a CNN model to recognize skin d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>['python', 'machine-learning', 'deep-learning'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n&lt;p&gt;I have an audio file which was created by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>['python', 'machine-learning', 'deep-learning']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;pre&gt;&lt;code&gt;classifier = tf.keras.models.Sequen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'pytorch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;I am training an CNN model (AlexNet- with r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>['python', 'deep-learning', 'pytorch']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;I am trying to train a deep learning model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'pre-tra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;I am training VGG16 network with medical im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tags  \\\n",
       "0     ['python', 'tensorflow', 'keras', 'deep-learni...   \n",
       "1     ['machine-learning', 'deep-learning', 'nlp', '...   \n",
       "2     ['deep-learning', 'object-detection', 'multila...   \n",
       "3     ['deep-learning', 'attention-model', 'image-en...   \n",
       "4     ['python', 'deep-learning', 'overfitting-under...   \n",
       "...                                                 ...   \n",
       "4995  ['python', 'machine-learning', 'deep-learning'...   \n",
       "4996    ['python', 'machine-learning', 'deep-learning']   \n",
       "4997  ['machine-learning', 'deep-learning', 'pytorch...   \n",
       "4998             ['python', 'deep-learning', 'pytorch']   \n",
       "4999  ['machine-learning', 'deep-learning', 'pre-tra...   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Trouble loading tensorflow when running cloned...   \n",
       "1     For training a neural network, if i have label...   \n",
       "2                               Polygon vs Bounding box   \n",
       "3     The training loss curve is just a straight lin...   \n",
       "4     My CNN model is overfitted, and I don&#39;t kn...   \n",
       "...                                                 ...   \n",
       "4995                                                NaN   \n",
       "4996                                                NaN   \n",
       "4997                                                NaN   \n",
       "4998                                                NaN   \n",
       "4999                                                NaN   \n",
       "\n",
       "                                                   body  \n",
       "0     <p>So, I'm trying to get the code from this re...  \n",
       "1     <p>I am working on a project which predict cus...  \n",
       "2     <p>I want to implement the leaf disease detect...  \n",
       "3     <p><a href=\"https://i.sstatic.net/oTR24bcA.png...  \n",
       "4     <p>I am making a CNN model to recognize skin d...  \n",
       "...                                                 ...  \n",
       "4995  \\n<p>I have an audio file which was created by...  \n",
       "4996  <pre><code>classifier = tf.keras.models.Sequen...  \n",
       "4997  <p>I am training an CNN model (AlexNet- with r...  \n",
       "4998  <p>I am trying to train a deep learning model ...  \n",
       "4999  <p>I am training VGG16 network with medical im...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c6a7313-b233-45e1-99e6-b93fd00671a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer,SnowballStemmer\n",
    "import nltk\n",
    "\n",
    "# Download nltk resources (run once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'<.*?>', ' ', text)  # remove HTML tags\n",
    "\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token == 'c++':  # keep 'c++' intact\n",
    "            tokens.append(token)\n",
    "            continue\n",
    "\n",
    "        token = re.sub(r'[^a-z]', '', token)  # keep only letters\n",
    "        if not token or token in stop:\n",
    "            continue\n",
    "\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "        tokens.append(token)\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "99aebe64-aba1-4eb2-9fca-0bcdb09db487",
   "metadata": {},
   "outputs": [],
   "source": [
    "sno = SnowballStemmer('english') \n",
    "def stem(tokens):\n",
    "    fil = []\n",
    "    for token in tokens:\n",
    "        if token not in stop:\n",
    "            s = sno.stem(token).encode('utf8')  # stem and encode as bytes\n",
    "            fil.append(s)\n",
    "    # join bytes with space, then decode back to string\n",
    "    s = b' '.join(fil)\n",
    "    return s.decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b893ebad-bbab-4668-b5e9-030be34417f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_19820\\3807138661.py:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  cleaned = re.sub('\\s+', ' ', cleaned).strip()     # remove extra spaces\n"
     ]
    }
   ],
   "source": [
    "# creating the new columns \n",
    "# cleaned questions and cleaned tags\n",
    "# after cleaning the data\n",
    "def clean_tags(tags_str):\n",
    "    cleaned = re.sub('[^A-Za-z#+-]', ' ', tags_str)  # keep letters and #, +, -\n",
    "    cleaned = re.sub('\\s+', ' ', cleaned).strip()     # remove extra spaces\n",
    "    return cleaned\n",
    "\n",
    "# Apply tag cleaning\n",
    "final_data['cleanTags'] = final_data.tags.apply(clean_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "362dc657-3ab6-4791-9eb6-936cc897866b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>cleanQues</th>\n",
       "      <th>cleanTags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['python', 'tensorflow', 'keras', 'deep-learni...</td>\n",
       "      <td>Trouble loading tensorflow when running cloned...</td>\n",
       "      <td>&lt;p&gt;So, I'm trying to get the code from this re...</td>\n",
       "      <td>b'troubl load tensorflow run clone repo'</td>\n",
       "      <td>python    tensorflow    keras    deep-learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'nlp', '...</td>\n",
       "      <td>For training a neural network, if i have label...</td>\n",
       "      <td>&lt;p&gt;I am working on a project which predict cus...</td>\n",
       "      <td>b'train neural network label encod featur need...</td>\n",
       "      <td>machine-learning    deep-learning    nlp    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['deep-learning', 'object-detection', 'multila...</td>\n",
       "      <td>Polygon vs Bounding box</td>\n",
       "      <td>&lt;p&gt;I want to implement the leaf disease detect...</td>\n",
       "      <td>b'polygon vs bound box'</td>\n",
       "      <td>deep-learning    object-detection    multila...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['deep-learning', 'attention-model', 'image-en...</td>\n",
       "      <td>The training loss curve is just a straight lin...</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"https://i.sstatic.net/oTR24bcA.png...</td>\n",
       "      <td>b'train loss curv straight line random deviat ...</td>\n",
       "      <td>deep-learning    attention-model    image-en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['python', 'deep-learning', 'overfitting-under...</td>\n",
       "      <td>My CNN model is overfitted, and I don&amp;#39;t kn...</td>\n",
       "      <td>&lt;p&gt;I am making a CNN model to recognize skin d...</td>\n",
       "      <td>b'cnn model overfit dont know fix'</td>\n",
       "      <td>python    deep-learning    overfitting-under...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>['deep-learning', 'jax', 'flax']</td>\n",
       "      <td>Is there a way to update weights of an nnx.Mod...</td>\n",
       "      <td>&lt;p&gt;I have a neural network (nnx.Module) writte...</td>\n",
       "      <td>b'way updat weight nnxmodul flax nnx use laxsc...</td>\n",
       "      <td>deep-learning    jax    flax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>['python', 'deep-learning', 'pytorch', 'yolo',...</td>\n",
       "      <td>The issue of mask fragmentation during SAM2 tr...</td>\n",
       "      <td>&lt;p&gt;I am currently working on object tracking. ...</td>\n",
       "      <td>b'issu mask fragment sam track'</td>\n",
       "      <td>python    deep-learning    pytorch    yolo  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'compute...</td>\n",
       "      <td>How to use Stable Diffusion WebUI with Control...</td>\n",
       "      <td>&lt;p&gt;I'm trying to generate images using Stable ...</td>\n",
       "      <td>b'use stabl diffus webui controlnet posetoimag...</td>\n",
       "      <td>machine-learning    deep-learning    compute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>['python', 'deep-learning', 'pytorch', 'artifi...</td>\n",
       "      <td>Logits Don&amp;#39;t Change in a Custom Reimplemen...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;&lt;/p&gt;\\n&lt;p&gt;The si...</td>\n",
       "      <td>b'logit dont chang custom reimplement clip mod...</td>\n",
       "      <td>python    deep-learning    pytorch    artifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>['machine-learning', 'deep-learning', 'pytorch...</td>\n",
       "      <td>Common practices to mitigate accuracy plateaui...</td>\n",
       "      <td>&lt;p&gt;I'm training a Deep neural network to detec...</td>\n",
       "      <td>b'common practic mitig accuraci plateau baselin'</td>\n",
       "      <td>machine-learning    deep-learning    pytorch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tags  \\\n",
       "0  ['python', 'tensorflow', 'keras', 'deep-learni...   \n",
       "1  ['machine-learning', 'deep-learning', 'nlp', '...   \n",
       "2  ['deep-learning', 'object-detection', 'multila...   \n",
       "3  ['deep-learning', 'attention-model', 'image-en...   \n",
       "4  ['python', 'deep-learning', 'overfitting-under...   \n",
       "5                   ['deep-learning', 'jax', 'flax']   \n",
       "6  ['python', 'deep-learning', 'pytorch', 'yolo',...   \n",
       "7  ['machine-learning', 'deep-learning', 'compute...   \n",
       "8  ['python', 'deep-learning', 'pytorch', 'artifi...   \n",
       "9  ['machine-learning', 'deep-learning', 'pytorch...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Trouble loading tensorflow when running cloned...   \n",
       "1  For training a neural network, if i have label...   \n",
       "2                            Polygon vs Bounding box   \n",
       "3  The training loss curve is just a straight lin...   \n",
       "4  My CNN model is overfitted, and I don&#39;t kn...   \n",
       "5  Is there a way to update weights of an nnx.Mod...   \n",
       "6  The issue of mask fragmentation during SAM2 tr...   \n",
       "7  How to use Stable Diffusion WebUI with Control...   \n",
       "8  Logits Don&#39;t Change in a Custom Reimplemen...   \n",
       "9  Common practices to mitigate accuracy plateaui...   \n",
       "\n",
       "                                                body  \\\n",
       "0  <p>So, I'm trying to get the code from this re...   \n",
       "1  <p>I am working on a project which predict cus...   \n",
       "2  <p>I want to implement the leaf disease detect...   \n",
       "3  <p><a href=\"https://i.sstatic.net/oTR24bcA.png...   \n",
       "4  <p>I am making a CNN model to recognize skin d...   \n",
       "5  <p>I have a neural network (nnx.Module) writte...   \n",
       "6  <p>I am currently working on object tracking. ...   \n",
       "7  <p>I'm trying to generate images using Stable ...   \n",
       "8  <p><strong>The problem</strong></p>\\n<p>The si...   \n",
       "9  <p>I'm training a Deep neural network to detec...   \n",
       "\n",
       "                                           cleanQues  \\\n",
       "0           b'troubl load tensorflow run clone repo'   \n",
       "1  b'train neural network label encod featur need...   \n",
       "2                            b'polygon vs bound box'   \n",
       "3  b'train loss curv straight line random deviat ...   \n",
       "4                 b'cnn model overfit dont know fix'   \n",
       "5  b'way updat weight nnxmodul flax nnx use laxsc...   \n",
       "6                    b'issu mask fragment sam track'   \n",
       "7  b'use stabl diffus webui controlnet posetoimag...   \n",
       "8  b'logit dont chang custom reimplement clip mod...   \n",
       "9   b'common practic mitig accuraci plateau baselin'   \n",
       "\n",
       "                                           cleanTags  \n",
       "0    python    tensorflow    keras    deep-learni...  \n",
       "1    machine-learning    deep-learning    nlp    ...  \n",
       "2    deep-learning    object-detection    multila...  \n",
       "3    deep-learning    attention-model    image-en...  \n",
       "4    python    deep-learning    overfitting-under...  \n",
       "5                     deep-learning    jax    flax    \n",
       "6    python    deep-learning    pytorch    yolo  ...  \n",
       "7    machine-learning    deep-learning    compute...  \n",
       "8    python    deep-learning    pytorch    artifi...  \n",
       "9    machine-learning    deep-learning    pytorch...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1671e141-d1c3-4efe-bd89-2b4bedebee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = []\n",
    "for j in final_data.title:\n",
    "    ques.append(stem(clean(j)))\n",
    "\n",
    "final_data['cleanQues'] = ques\n",
    "import re\n",
    "\n",
    "ctags = []\n",
    "for i in final_data.tags:\n",
    "    ctags.append(re.sub('[^A-Za-z#+-]', ' ', i)) \n",
    "\n",
    "final_data['cleanTags'] = ctags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9690c1cc-d530-4b1d-81c3-a4092f63771a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>troubl load tensorflow run clone repo</td>\n",
       "      <td>python    tensorflow    keras    deep-learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train neural network label encod featur need s...</td>\n",
       "      <td>machine-learning    deep-learning    nlp    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>polygon v bound box</td>\n",
       "      <td>deep-learning    object-detection    multila...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train loss curv straight line random deviat mean</td>\n",
       "      <td>deep-learning    attention-model    image-en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cnn model overfit dont know fix</td>\n",
       "      <td>python    deep-learning    overfitting-under...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0              troubl load tensorflow run clone repo   \n",
       "1  train neural network label encod featur need s...   \n",
       "2                                polygon v bound box   \n",
       "3   train loss curv straight line random deviat mean   \n",
       "4                    cnn model overfit dont know fix   \n",
       "\n",
       "                                                tags  \n",
       "0    python    tensorflow    keras    deep-learni...  \n",
       "1    machine-learning    deep-learning    nlp    ...  \n",
       "2    deep-learning    object-detection    multila...  \n",
       "3    deep-learning    attention-model    image-en...  \n",
       "4    python    deep-learning    overfitting-under...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.DataFrame()\n",
    "d['text'] = final_data.cleanQues\n",
    "d['tags'] = final_data.cleanTags\n",
    "\n",
    "# Save with .csv extension\n",
    "d.to_csv('ML_DL_Final.csv', index=False)\n",
    "\n",
    "# Load the saved CSV file\n",
    "df = pd.read_csv('ML_DL_Final.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "62a70702-216f-4343-bdf8-94c9430af1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'troubl load tensorflow run clone repo'</td>\n",
       "      <td>python    tensorflow    keras    deep-learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'train neural network label encod featur need...</td>\n",
       "      <td>machine-learning    deep-learning    nlp    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'polygon vs bound box'</td>\n",
       "      <td>deep-learning    object-detection    multila...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'train loss curv straight line random deviat ...</td>\n",
       "      <td>deep-learning    attention-model    image-en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'cnn model overfit dont know fix'</td>\n",
       "      <td>python    deep-learning    overfitting-under...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0           b'troubl load tensorflow run clone repo'   \n",
       "1  b'train neural network label encod featur need...   \n",
       "2                            b'polygon vs bound box'   \n",
       "3  b'train loss curv straight line random deviat ...   \n",
       "4                 b'cnn model overfit dont know fix'   \n",
       "\n",
       "                                                tags  \n",
       "0    python    tensorflow    keras    deep-learni...  \n",
       "1    machine-learning    deep-learning    nlp    ...  \n",
       "2    deep-learning    object-detection    multila...  \n",
       "3    deep-learning    attention-model    image-en...  \n",
       "4    python    deep-learning    overfitting-under...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import pandas as pd\n",
    "df = pd.read_csv('ML_DL_Final')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6cc0ab23-c31c-452b-86fe-20aa2f8a412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(df.text, df.tags, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea38a827-1715-4617-a7ca-b15dd0fc94ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "tfvectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, ngram_range=(1,3), lowercase=False)\n",
    "\n",
    "x_train_multilabel = tfvectorizer.fit_transform(x_train)\n",
    "x_test_multilabel = tfvectorizer.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43456363-830c-48d3-8997-a647a87edfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [tags.split() for tags in y_train]\n",
    "y_test = [tags.split() for tags in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eca5935c-7ed5-469f-8017-cdee9bf66c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:900: UserWarning: unknown class(es) ['aggregation', 'amazon-s', 'amd-processor', 'apple-silicon', 'arkit', 'augmented-reality', 'auto-keras', 'azure-databricks', 'bias-neuron', 'bin', 'ckpt', 'concatenation', 'coordinate-systems', 'correlation', 'cudnn', 'cupy', 'data-analysis', 'data-fitting', 'data-generation', 'discriminator', 'evolutionary-algorithm', 'execution', 'fb-hydra', 'few-shot-learning', 'file-type', 'frame-rate', 'functional-programming', 'gabor-filter', 'gaussianblur', 'generative-programming', 'gesture-recognition', 'gis', 'google-cloud-vertex-ai', 'hdf', 'hdfs', 'hierarchical-clustering', 'html', 'huggingface-hub', 'in-place', 'initialization', 'invalid-argument', 'itk', 'lapack', 'layout-parser', 'libraries', 'mask', 'memory-leaks', 'metal', 'miniconda', 'movenet', 'multiple-gpu', 'multithreading', 'multivariate-time-series', 'networkx', 'newtons-method', 'nlp-question-answering', 'omegaconf', 'paddleocr', 'part-of-speech', 'post-processing', 'privacy', 'profiler', 'progress-bar', 'python-import', 'python-multithreading', 'qimage', 'qt', 'quantities', 'ram', 'ray', 'ray-tune', 'reverse-engineering', 'sample', 'scalability', 'segment', 'self-attention', 'server-side-rendering', 'simpleitk', 'smote', 'soundfile', 'standardized', 'theory', 'three', 'torchscript', 'tracking', 'transformation', 'trend', 'vector', 'vega-lite', 'virtual-machine', 'vscode-debugger', 'winapi'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train_multilabel = mlb.fit_transform(y_train)\n",
    "y_test_multilabel = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3d085eff-4c04-4073-9abd-d83c443fa86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23224\n"
     ]
    }
   ],
   "source": [
    "print(len(tfvectorizer.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5c947a4-88e3-4dd1-8e4a-1174c987981d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['image-processing', 'deep-learning', 'stable-diffusion', 'generative-art'], ['python', 'machine-learning', 'deep-learning', 'pytorch', 'perceptron'], ['python', 'tensorflow', 'keras', 'deep-learning', 'neural-network']]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4baa4639-2252-4c54-baa3-86c864b06be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\sklearn\\multiclass.py:90: UserWarning: Label 142 is present in all training examples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.018\n",
      "Macro F1 Score : 0.007899896295251322\n",
      "Micro F1 Score : 0.5442264393695722\n",
      "Hamming Loss : 0.0038194070080862534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score,hamming_loss\n",
    "\n",
    "\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='log_loss', max_iter=1000, tol=1e-3, penalty='l2'))\n",
    "classifier.fit(x_train_multilabel, y_train_multilabel)\n",
    "\n",
    "predictions = classifier.predict(x_test_multilabel)\n",
    "\n",
    "print(\"Accuracy :\", accuracy_score(y_test_multilabel, predictions))\n",
    "print(\"Macro F1 Score :\", f1_score(y_test_multilabel, predictions, average='macro', zero_division=0))\n",
    "print(\"Micro F1 Score :\", f1_score(y_test_multilabel, predictions, average='micro', zero_division=0))\n",
    "print(\"Hamming Loss :\", hamming_loss(y_test_multilabel, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8fb4acba-02d3-49b7-a239-c637343fce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('deep-learning',), ('deep-learning', 'python', 'pytorch'), ('deep-learning', 'pytorch'), ('deep-learning', 'huggingface-transformers'), ('deep-learning', 'python'), ('deep-learning', 'python', 'tensorflow'), ('deep-learning',), ('deep-learning', 'python'), ('deep-learning', 'tensorflow'), ('deep-learning',), ('deep-learning',), ('deep-learning', 'python'), ('deep-learning', 'neural-network'), ('deep-learning', 'python'), ('deep-learning', 'python'), ('deep-learning',), ('deep-learning', 'yolov'), ('deep-learning', 'lstm'), ('deep-learning', 'python'), ('deep-learning', 'pytorch'), ('deep-learning', 'python'), ('deep-learning', 'pytorch'), ('deep-learning',), ('deep-learning',), ('deep-learning', 'tensorflow'), ('deep-learning',), ('deep-learning', 'python'), ('deep-learning', 'python'), ('deep-learning',), ('deep-learning', 'python'), ('deep-learning', 'keras', 'python', 'tensorflow'), ('deep-learning', 'keras', 'python', 'tensorflow'), ('deep-learning', 'python'), ('deep-learning', 'neural-network', 'python'), ('deep-learning',), ('deep-learning', 'python'), ('deep-learning', 'python'), ('deep-learning', 'pytorch'), ('deep-learning', 'neural-network', 'tensorflow'), ('deep-learning', 'python'), ('deep-learning', 'python', 'tensorflow'), ('deep-learning',), ('deep-learning', 'pytorch'), ('deep-learning', 'python'), ('deep-learning',), ('deep-learning', 'python', 'pytorch'), ('deep-learning', 'python', 'pytorch'), ('deep-learning', 'python'), ('deep-learning', 'python'), ('deep-learning', 'python', 'pytorch'), ('deep-learning', 'pytorch'), ('deep-learning', 'python', 'pytorch'), ('deep-learning',), ('deep-learning', 'neural-network'), ('deep-learning', 'python', 'pytorch'), ('deep-learning', 'python'), ('deep-learning', 'machine-learning', 'python'), ('deep-learning',), ('deep-learning', 'yolov'), ('deep-learning',), ('deep-learning', 'python'), ('deep-learning', 'tensorflow'), ('deep-learning', 'neural-network', 'python'), ('deep-learning',), ('deep-learning', 'machine-learning', 'python'), ('deep-learning', 'machine-learning', 'tensorflow'), ('deep-learning',), ('deep-learning', 'python'), ('deep-learning', 'python'), ('deep-learning', 'python'), ('deep-learning', 'pytorch'), ('deep-learning', 'python', 'tensorflow'), ('deep-learning', 'python', 'pytorch'), ('deep-learning',), ('deep-learning', 'machine-learning', 'pytorch'), ('deep-learning', 'keras', 'python', 'tensorflow'), ('deep-learning', 'keras', 'python'), ('deep-learning', 'python'), ('deep-learning', 'python'), ('deep-learning', 'keras', 'lstm', 'python'), ('deep-learning', 'keras', 'python', 'tensorflow'), ('deep-learning',), ('deep-learning', 'keras', 'python', 'tensorflow'), ('deep-learning',), ('deep-learning', 'python'), ('deep-learning', 'keras', 'neural-network', 'python', 'tensorflow'), ('deep-learning',), ('deep-learning', 'python'), ('deep-learning', 'python'), ('deep-learning',), ('deep-learning', 'python'), ('deep-learning', 'pytorch'), ('deep-learning', 'neural-network', 'pytorch'), ('conv-neural-network', 'deep-learning', 'python'), ('deep-learning', 'keras', 'python', 'tensorflow'), ('deep-learning', 'neural-network', 'python', 'pytorch'), ('deep-learning', 'keras', 'python', 'tensorflow'), ('deep-learning', 'neural-network', 'pytorch'), ('deep-learning', 'loss-function', 'python', 'tensorflow'), ('deep-learning',)]\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(x_test_multilabel)\n",
    "\n",
    "predicted_tags = mlb.inverse_transform(predictions)\n",
    "\n",
    "print(predicted_tags[:100])  # print first 5 predicted tag lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5a17fd53-e9c7-40e8-b970-18dd6a47b50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Precision: 0.8072519083969466\n",
      "Micro Recall   : 0.4104803493449782\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Micro Precision:\", precision_score(y_test_multilabel, predictions, average='micro', zero_division=0))\n",
    "print(\"Micro Recall   :\", recall_score(y_test_multilabel, predictions, average='micro', zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0ff7692b-9260-42b3-86fc-14bbf07a3dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_19820\\3049308817.py:3: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  with open(\"D:\\model.pkl\", \"wb\") as f:\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"D:\\model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7361240d-49b3-44c7-b9fb-945b5c7b2eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:/tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfvectorizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3976ef73-0bd3-4f24-b273-925940a7d535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_19820\\1540465224.py:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  with open(\"D:\\mlb.pkl\", \"wb\") as f:\n"
     ]
    }
   ],
   "source": [
    "with open(\"D:\\mlb.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mlb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63579d-f0e7-4c0c-a07b-554dc5b08445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3459edea-6d14-4510-b2cb-77147288dfd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b3a48-bee7-4268-912e-b804e2551fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbba059-1a17-49cb-b354-46fd911d4876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1eb28e70-cc81-4332-89d4-70fa08294d9b",
   "metadata": {},
   "source": [
    "# SIMPLE EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c92607d-255f-43b7-b99c-f133519db30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad9833a9-0baf-4a64-861e-440381185ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simple_EDA(final_data, column):\n",
    "    final_data[column] = final_data[column].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "    results = {\n",
    "        \"lower & upper mix\": final_data[column].apply(lambda x: not (x.islower() or x.isupper())).sum(),\n",
    "        \"HTML/XML tags\":     final_data[column].apply(lambda x: bool(re.search(r\"<.*?>\", x))).sum(),\n",
    "        \"URLs\":              final_data[column].apply(lambda x: bool(re.search(r\"https?://\\S+\", x))).sum(),\n",
    "        \"Emails\":            final_data[column].apply(lambda x: bool(re.search(r\"\\S+@\\S+\", x))).sum(),\n",
    "        \"Mentions (@/#)\":    final_data[column].apply(lambda x: bool(re.search(r\"\\B[@#]\\S+\", x))).sum(),\n",
    "        \"Emojis\":            final_data[column].apply(lambda x: bool(emoji.emoji_count(x))).sum(),\n",
    "        \"Digits\":            final_data[column].apply(lambda x: bool(re.search(r\"\\d\", x))).sum(),\n",
    "        \"Punctuation\":       final_data[column].apply(lambda x: bool(re.search(r'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', x))).sum(),\n",
    "        \"Dates\":             final_data[column].apply(\n",
    "                                lambda x: bool(\n",
    "                                    re.search(r\"\\b[0-9]{4}/[0-9]{1,2}/[0-9]{1,2}\\b\", x) or \n",
    "                                    re.search(r\"\\b[0-9]{1,2}/[0-9]{1,2}/[0-9]{4}\\b\", x)\n",
    "                                )\n",
    "                             ).sum()\n",
    "    }\n",
    "\n",
    "    for key, value in results.items():\n",
    "        print(f\" {value} rows contain {key}\")\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b703015-32d9-4061-80aa-279b6a9c69aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_10944\\3580528400.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_data[column] = final_data[column].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 rows contain lower & upper mix\n",
      " 0 rows contain HTML/XML tags\n",
      " 0 rows contain URLs\n",
      " 0 rows contain Emails\n",
      " 0 rows contain Mentions (@/#)\n",
      " 0 rows contain Emojis\n",
      " 246 rows contain Digits\n",
      " 0 rows contain Punctuation\n",
      " 0 rows contain Dates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lower & upper mix': 0,\n",
       " 'HTML/XML tags': 0,\n",
       " 'URLs': 0,\n",
       " 'Emails': 0,\n",
       " 'Mentions (@/#)': 0,\n",
       " 'Emojis': 0,\n",
       " 'Digits': 246,\n",
       " 'Punctuation': 0,\n",
       " 'Dates': 0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Simple_EDA(cv,\"tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c60555a-c339-48ee-aac0-f894ca1acf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                python, tensorflow, keras, deep-learning\n",
      "1       machine-learning, deep-learning, nlp, neural-n...\n",
      "2       deep-learning, object-detection, multilabel-cl...\n",
      "3       deep-learning, attention-model, image-enhancem...\n",
      "4         python, deep-learning, overfitting-underfitting\n",
      "                              ...                        \n",
      "2495    python, machine-learning, deep-learning, signa...\n",
      "2496              python, machine-learning, deep-learning\n",
      "2497    machine-learning, deep-learning, pytorch, conv...\n",
      "2498                       python, deep-learning, pytorch\n",
      "2499    machine-learning, deep-learning, pre-trained-m...\n",
      "Name: tags, Length: 2500, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_10944\\393640643.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cv[\"tags\"] = cv[\"tags\"].apply(lambda x: re.sub(r\"[\\[\\]']\", \"\", str(x))) # removing string#\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "cv[\"tags\"] = cv[\"tags\"].apply(lambda x: re.sub(r\"[\\[\\]']\", \"\", str(x))) # removing string#\n",
    "print(cv[\"tags\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a24282-8db5-451e-bcaf-2b62ce02d14e",
   "metadata": {},
   "source": [
    "# Pre_Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f385221-a6ec-4b39-9b70-f6cf90f4ecf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:36: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_10944\\1667345713.py:36: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  data[col] = data[col].apply(lambda x: re.sub(\"https?://\\S+\", \" \", x))\n"
     ]
    }
   ],
   "source": [
    "def pre_proce(data, coln, case=\"lower\", tags=True, urls=True, emails=True,\n",
    "              mentions=True, digits=True, dates=True, emojii=True,\n",
    "              contractionsss=True, stopwordss=True, punc=True):\n",
    "    import re\n",
    "    import emoji\n",
    "    import contractions\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    # Stopwords\n",
    "    stp = stopwords.words(\"english\")\n",
    "    if \"not\" in stp:\n",
    "        stp.remove(\"not\")\n",
    "\n",
    "    # Emoji\n",
    "    if emojii:\n",
    "        for col in coln:\n",
    "            data[col] = data[col].apply(lambda x: emoji.demojize(x, delimiters=(\"\", \"\")))\n",
    "\n",
    "    # Case conversion\n",
    "    if case == \"lower\":\n",
    "        for col in coln:\n",
    "            data[col] = data[col].str.lower()\n",
    "    elif case == \"upper\":\n",
    "        for col in coln:\n",
    "            data[col] = data[col].str.upper()\n",
    "\n",
    "    # Remove HTML tags\n",
    "    if tags:\n",
    "        for col in coln:\n",
    "            data[col] = data[col].apply(lambda x: re.sub(\"<.*?>\", \" \", x))\n",
    "\n",
    "    # Remove URLs\n",
    "    if urls:\n",
    "        for col in coln:\n",
    "            data[col] = data[col].apply(lambda x: re.sub(\"https?://\\S+\", \" \", x))\n",
    "\n",
    "    # Remove Emails\n",
    "    if emails:\n",
    "        for col in coln:\n",
    "            data[col] = data[col].apply(lambda x: re.sub(r\"\\S+@\\S+\", \" \", x))\n",
    "\n",
    "    # Remove Mentions (@ and #)\n",
    "    if mentions:\n",
    "        for col in coln:\n",
    "            data[col] = data[col].apply(lambda x: re.sub(r\"\\B[@#]\\S+\", \" \", x))\n",
    "\n",
    "    # Remove Digits\n",
    "    if digits:\n",
    "        for col in coln:\n",
    "            data[col] = data[col].apply(lambda x: re.sub(r\"\\d+\", \" \", x))\n",
    "\n",
    "    # Remove Dates\n",
    "    if dates:\n",
    "        for col in coln:\n",
    "            data[col] = data[col].apply(lambda x: re.sub(r\"\\b[0-9]{1,2}/[0-9]{1,2}/[0-9]{4}\\b\", \" \", x))\n",
    "            data[col] = data[col].apply(lambda x: re.sub(r\"\\b[0-9]{4}/[0-9]{1,2}/[0-9]{1,2}\\b\", \" \", x))\n",
    "\n",
    "    # Expand Contractions\n",
    "    if contractionsss:\n",
    "        for col in coln:\n",
    "            data[col] = data[col].apply(lambda x: contractions.fix(x))\n",
    "\n",
    "    # Remove Punctuations\n",
    "    if punc:\n",
    "        for col in coln:\n",
    "            data[col] = data[col].apply(lambda x: re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]', \" \", x))\n",
    "\n",
    "    # Remove Stopwords\n",
    "    if stopwordss:\n",
    "        for col in coln:\n",
    "            data[col] = data[col].apply(lambda x: \" \".join([word for word in word_tokenize(x) if word not in stp]))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ade22f85-fc93-4566-b5bf-0eca7ce514f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_10944\\1667345713.py:36: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  data[col] = data[col].apply(lambda x: re.sub(\"https?://\\S+\", \" \", x))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preprocessed_data2\u001b[38;5;241m=\u001b[39mpre_proce(fv,[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[26], line 18\u001b[0m, in \u001b[0;36mpre_proce\u001b[1;34m(data, coln, case, tags, urls, emails, mentions, digits, dates, emojii, contractionsss, stopwordss, punc)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m emojii:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m coln:\n\u001b[1;32m---> 18\u001b[0m         data[col] \u001b[38;5;241m=\u001b[39m data[col]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: emoji\u001b[38;5;241m.\u001b[39mdemojize(x, delimiters\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Case conversion\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m case \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[26], line 18\u001b[0m, in \u001b[0;36mpre_proce.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m emojii:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m coln:\n\u001b[1;32m---> 18\u001b[0m         data[col] \u001b[38;5;241m=\u001b[39m data[col]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: emoji\u001b[38;5;241m.\u001b[39mdemojize(x, delimiters\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Case conversion\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m case \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\emoji\\core.py:313\u001b[0m, in \u001b[0;36mdemojize\u001b[1;34m(string, delimiters, language, version, handle_version)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m emoji_match\u001b[38;5;241m.\u001b[39memoji\n\u001b[0;32m    312\u001b[0m matches \u001b[38;5;241m=\u001b[39m tokenize(string, keep_zwj\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdemojize_keep_zwj)\n\u001b[1;32m--> 313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28mstr\u001b[39m(handle(token\u001b[38;5;241m.\u001b[39mvalue)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token\u001b[38;5;241m.\u001b[39mvalue, EmojiMatch) \u001b[38;5;28;01melse\u001b[39;00m token\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m matches\n\u001b[0;32m    316\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\emoji\\core.py:313\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m emoji_match\u001b[38;5;241m.\u001b[39memoji\n\u001b[0;32m    312\u001b[0m matches \u001b[38;5;241m=\u001b[39m tokenize(string, keep_zwj\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdemojize_keep_zwj)\n\u001b[1;32m--> 313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28mstr\u001b[39m(handle(token\u001b[38;5;241m.\u001b[39mvalue)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token\u001b[38;5;241m.\u001b[39mvalue, EmojiMatch) \u001b[38;5;28;01melse\u001b[39;00m token\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m matches\n\u001b[0;32m    316\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\emoji\\tokenizer.py:174\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(string, keep_zwj)\u001b[0m\n\u001b[0;32m    172\u001b[0m result: List[Token] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    173\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 174\u001b[0m length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(string)\n\u001b[0;32m    175\u001b[0m ignore: List[\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28mint\u001b[39m\n\u001b[0;32m    177\u001b[0m ] \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# index of chars in string that are skipped, i.e. the ZWJ-char in non-RGI-ZWJ-sequences\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m length:\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "preprocessed_data2=pre_proce(fv,[\"title\",'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "837df27d-a0fe-401f-9d05-f50416337dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>including multiple dataset transformers custom...</td>\n",
       "      <td>custom transformer , meant transform subject d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama- , q -quantized model &amp; response time di...</td>\n",
       "      <td>running quantized llama- model . using differe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>implementation linear regression not working ?</td>\n",
       "      <td>trying implement linear regression scratch pyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transfer learning using vgg mnist digits</td>\n",
       "      <td>trying perform transfer learning mnist digits ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpe using spacy identifies locations not geogr...</td>\n",
       "      <td>using spacy library nlp code identify prominen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>use project custom template gitlab</td>\n",
       "      <td>new gitlab , need use project template creatin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>restore permissions files directories within g...</td>\n",
       "      <td>new coding trying figure commit terminal . exe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>recovering much history possible corrupt git r...</td>\n",
       "      <td>used photorec recover zlib files disk image ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>git clone clone folder</td>\n",
       "      <td>create file hello.txt hello folder inside repo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>fix gitlab merge requests merging sqashed commits</td>\n",
       "      <td>branches : develop master . made several merge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "0     including multiple dataset transformers custom...   \n",
       "1     llama- , q -quantized model & response time di...   \n",
       "2        implementation linear regression not working ?   \n",
       "3              transfer learning using vgg mnist digits   \n",
       "4     gpe using spacy identifies locations not geogr...   \n",
       "...                                                 ...   \n",
       "8995                 use project custom template gitlab   \n",
       "8996  restore permissions files directories within g...   \n",
       "8997  recovering much history possible corrupt git r...   \n",
       "8998                             git clone clone folder   \n",
       "8999  fix gitlab merge requests merging sqashed commits   \n",
       "\n",
       "                                                   body  \n",
       "0     custom transformer , meant transform subject d...  \n",
       "1     running quantized llama- model . using differe...  \n",
       "2     trying implement linear regression scratch pyt...  \n",
       "3     trying perform transfer learning mnist digits ...  \n",
       "4     using spacy library nlp code identify prominen...  \n",
       "...                                                 ...  \n",
       "8995  new gitlab , need use project template creatin...  \n",
       "8996  new coding trying figure commit terminal . exe...  \n",
       "8997  used photorec recover zlib files disk image ta...  \n",
       "8998  create file hello.txt hello folder inside repo...  \n",
       "8999  branches : develop master . made several merge...  \n",
       "\n",
       "[9000 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3531a-5552-490e-91af-bedfae7e5d84",
   "metadata": {},
   "source": [
    "* Combining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ba403da-09f8-4064-8f9a-82a319029c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.DataFrame({\n",
    "    'tags': cv[\"tags\"].fillna(''),\n",
    "    'title': preprocessed_data2['title'].fillna(''),\n",
    "    'body': preprocessed_data2['body'].fillna('') \n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84f7564e-6a74-4451-b555-8ca5fcc199e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>python, pandas, machine-learning, scikit-learn...</td>\n",
       "      <td>including multiple dataset transformers custom...</td>\n",
       "      <td>custom transformer , meant transform subject d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>machine-learning, large-language-model, llama-...</td>\n",
       "      <td>llama- , q -quantized model &amp; response time di...</td>\n",
       "      <td>running quantized llama- model . using differe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>python, machine-learning, linear-regression, g...</td>\n",
       "      <td>implementation linear regression not working ?</td>\n",
       "      <td>trying implement linear regression scratch pyt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>python, tensorflow, machine-learning, keras, d...</td>\n",
       "      <td>transfer learning using vgg mnist digits</td>\n",
       "      <td>trying perform transfer learning mnist digits ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>machine-learning, nlp, spacy</td>\n",
       "      <td>gpe using spacy identifies locations not geogr...</td>\n",
       "      <td>using spacy library nlp code identify prominen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8995</th>\n",
       "      <td>git, templates, gitlab</td>\n",
       "      <td>use project custom template gitlab</td>\n",
       "      <td>new gitlab , need use project template creatin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>git, permissions</td>\n",
       "      <td>restore permissions files directories within g...</td>\n",
       "      <td>new coding trying figure commit terminal . exe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8997</th>\n",
       "      <td>git, git-filter-branch, git-filter-repo, git-fsck</td>\n",
       "      <td>recovering much history possible corrupt git r...</td>\n",
       "      <td>used photorec recover zlib files disk image ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8998</th>\n",
       "      <td>git, github</td>\n",
       "      <td>git clone clone folder</td>\n",
       "      <td>create file hello.txt hello folder inside repo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8999</th>\n",
       "      <td>git, gitlab</td>\n",
       "      <td>fix gitlab merge requests merging sqashed commits</td>\n",
       "      <td>branches : develop master . made several merge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tags  \\\n",
       "0     python, pandas, machine-learning, scikit-learn...   \n",
       "1     machine-learning, large-language-model, llama-...   \n",
       "2     python, machine-learning, linear-regression, g...   \n",
       "3     python, tensorflow, machine-learning, keras, d...   \n",
       "4                          machine-learning, nlp, spacy   \n",
       "...                                                 ...   \n",
       "8995                             git, templates, gitlab   \n",
       "8996                                   git, permissions   \n",
       "8997  git, git-filter-branch, git-filter-repo, git-fsck   \n",
       "8998                                        git, github   \n",
       "8999                                        git, gitlab   \n",
       "\n",
       "                                                  title  \\\n",
       "0     including multiple dataset transformers custom...   \n",
       "1     llama- , q -quantized model & response time di...   \n",
       "2        implementation linear regression not working ?   \n",
       "3              transfer learning using vgg mnist digits   \n",
       "4     gpe using spacy identifies locations not geogr...   \n",
       "...                                                 ...   \n",
       "8995                 use project custom template gitlab   \n",
       "8996  restore permissions files directories within g...   \n",
       "8997  recovering much history possible corrupt git r...   \n",
       "8998                             git clone clone folder   \n",
       "8999  fix gitlab merge requests merging sqashed commits   \n",
       "\n",
       "                                                   body  \n",
       "0     custom transformer , meant transform subject d...  \n",
       "1     running quantized llama- model . using differe...  \n",
       "2     trying implement linear regression scratch pyt...  \n",
       "3     trying perform transfer learning mnist digits ...  \n",
       "4     using spacy library nlp code identify prominen...  \n",
       "...                                                 ...  \n",
       "8995  new gitlab , need use project template creatin...  \n",
       "8996  new coding trying figure commit terminal . exe...  \n",
       "8997  used photorec recover zlib files disk image ta...  \n",
       "8998  create file hello.txt hello folder inside repo...  \n",
       "8999  branches : develop master . made several merge...  \n",
       "\n",
       "[9000 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea9836d4-a883-42ea-80af-7dcbe5aeb38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fv=combined_df[[\"title\",\"body\"]]\n",
    "cv=combined_df[[\"tags\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "665a9bf0-0105-47b0-aa76-f3f890cd822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3ef6af5-e384-45f5-9d3a-5c1dd0cdd3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(fv,cv,test_size=0.2,stratify=cv,random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25bcc8a9-e3a8-4d08-b54a-991faf030cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d233627-cc78-4e85-a584-0ed94f631845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'body'], dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5767d439-125d-4146-a7bd-9602c0f3bc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tags'], dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b72cb17c-3131-4762-80dc-b13e5e8924d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7200, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81bc8c8-ff14-4fff-bf4d-bcc34d95969c",
   "metadata": {},
   "source": [
    "# Functional Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "647d7acc-2a97-47eb-96b3-8a61b2d6504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing=FunctionTransformer(pre_proce,kw_args={'coln':[\"title\",\"body\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed648ce4-94c6-4b93-b438-7547021a270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_transformer=pre_processing.fit_transform(x_train)\n",
    "x_test_transformer=pre_processing.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "722a524f-5bbe-4a92-9439-ef1df762518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ee61fd-7162-4b34-bd9e-33052b0bc723",
   "metadata": {},
   "source": [
    "# Column Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa38e420-6d00-400e-8c98-762bdd18a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('title_tfidf', TfidfVectorizer(), 'title'),\n",
    "        ('body_tfidf', TfidfVectorizer(), 'body'),\n",
    "\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    sparse_threshold=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4f44c2e-9c84-4978-9c95-cd2415b4d654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6352a038-3541-4d6c-8882-5457d311b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train['tags'].tolist()  # list of lists\n",
    "y_test = y_test['tags'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "62fcd7df-382a-4a23-ac05-bfc327ac9c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb=MultiLabelBinarizer()# Converting tags into numerical format\n",
    "y_train_mlb=mlb.fit_transform(y_train)\n",
    "y_test_mlb=mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8661ebc6-da38-48f1-bec4-92a3e9a30f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7200, 35)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_mlb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d010847-2c49-46c3-8cff-48ac4586f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3f5ef35-20db-49e4-86c6-0f6441b9ae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\New folder\\mlb_model.pkl\",\"wb\") as f:\n",
    "    pickle.dump(mlb,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "593531e5-88dc-4332-b061-d693149b7859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7200x6236 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 601367 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34fa41f-3451-4b96-9557-45e4d8f2ce58",
   "metadata": {},
   "source": [
    "# Creating Pipe_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8770f7de-4675-4b7c-bd1b-cf03fe14385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4cc299ce-f8c4-4c90-ab6b-9341d26a59b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;functiontransformer&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function pre_proce at 0x0000016875C3C2C0&gt;,\n",
       "                                     kw_args={&#x27;coln&#x27;: [&#x27;title&#x27;, &#x27;body&#x27;]})),\n",
       "                (&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;title_tfidf&#x27;,\n",
       "                                                  TfidfVectorizer(), &#x27;title&#x27;),\n",
       "                                                 (&#x27;body_tfidf&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;body&#x27;)]))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;functiontransformer&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function pre_proce at 0x0000016875C3C2C0&gt;,\n",
       "                                     kw_args={&#x27;coln&#x27;: [&#x27;title&#x27;, &#x27;body&#x27;]})),\n",
       "                (&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;title_tfidf&#x27;,\n",
       "                                                  TfidfVectorizer(), &#x27;title&#x27;),\n",
       "                                                 (&#x27;body_tfidf&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;body&#x27;)]))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;FunctionTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.FunctionTransformer.html\">?<span>Documentation for FunctionTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>FunctionTransformer(func=&lt;function pre_proce at 0x0000016875C3C2C0&gt;,\n",
       "                    kw_args={&#x27;coln&#x27;: [&#x27;title&#x27;, &#x27;body&#x27;]})</pre></div> </div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;columntransformer: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for columntransformer: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(transformers=[(&#x27;title_tfidf&#x27;, TfidfVectorizer(), &#x27;title&#x27;),\n",
       "                                (&#x27;body_tfidf&#x27;, TfidfVectorizer(), &#x27;body&#x27;)])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">title_tfidf</label><div class=\"sk-toggleable__content fitted\"><pre>title</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">body_tfidf</label><div class=\"sk-toggleable__content fitted\"><pre>body</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('functiontransformer',\n",
       "                 FunctionTransformer(func=<function pre_proce at 0x0000016875C3C2C0>,\n",
       "                                     kw_args={'coln': ['title', 'body']})),\n",
       "                ('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('title_tfidf',\n",
       "                                                  TfidfVectorizer(), 'title'),\n",
       "                                                 ('body_tfidf',\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  'body')]))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = make_pipeline(pre_processing,tf_idf)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb4312a8-d73a-412f-9ab7-dd8b8e31a89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7200x6236 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 601367 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_x_train = pipeline.fit_transform(x_train)\n",
    "final_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "910f86c5-1b2f-47bd-b5f2-2b56c986fd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1800x6236 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 151078 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_x_test=pipeline.transform(x_test)\n",
    "final_x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b42ff864-0c93-4953-acd4-962dd2acca50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7200, 6236), (1800, 6236))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_x_train.shape,final_x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e550180-fc95-44c5-ab3b-6265f8a59104",
   "metadata": {},
   "source": [
    "# Model_Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ad1cda98-a013-4ce0-85ad-48a48099a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c8a456d-5317-45ea-ad10-1ca53ac8d89e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2370392594.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[55], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    model=trial.suggest_categorical(\"model\":[\"RFC\",\"LR\"])\u001b[0m\n\u001b[1;37m                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    model=trial.suggest_categorical(\"model\",[\"RFC\",\"LR\"])\n",
    "    if model == \"RFC\":\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\",10,100)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 1, 10)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        model=RandomForestClassifier(n_estimators=n_estimators,max_depth=max_depth,min_samples_split=min_samples_split,random_state=42)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf91ea0f-f814-4edf-b590-d398b5d2f8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
